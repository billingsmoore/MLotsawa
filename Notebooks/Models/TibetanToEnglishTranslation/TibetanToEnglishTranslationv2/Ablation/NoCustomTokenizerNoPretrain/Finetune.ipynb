{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_dataset = load_from_disk('Data/tokenized-finetuning-ds') # Change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Unfinetuned Tokenizer, Model, and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\", device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-t5/t5-small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of all Tibetan Unicode characters (U+0F00 to U+0FFF)\n",
    "tibetan_chars = [chr(codepoint) for codepoint in range(0x0F00, 0x0FFF)]\n",
    "\n",
    "# Add the Tibetan characters to the tokenizer's vocabulary\n",
    "new_tokens = [char for char in tibetan_chars if char not in tokenizer.get_vocab()]\n",
    "\n",
    "# Add new tokens to the tokenizer\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Resize model embeddings to accommodate the new vocabulary size\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "enc = tokenizer.encode(dataset['train'][0]['tibetan'])\n",
    "dec = tokenizer.decode(enc)\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, Adafactor, EarlyStoppingCallback\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    scale_parameter=True, \n",
    "    relative_step=False, \n",
    "    warmup_init=False, \n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "model, optimizer = accelerator.prepare(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU and CHRF metrics\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "ter_metric = evaluate.load(\"ter\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Postprocess text\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_score = bleu_result[\"score\"]\n",
    "\n",
    "    # Compute CHRF score\n",
    "    chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    chrf_score = chrf_result[\"score\"]\n",
    "\n",
    "    # Compute TER score\n",
    "    ter_result = ter_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    ter_score = ter_result[\"score\"]\n",
    "\n",
    "    # Return rounded results\n",
    "    metrics = {\n",
    "        \"bleu\": round(bleu_score, 4),\n",
    "        \"chrf\": round(chrf_score, 4),\n",
    "        \"ter\": round(ter_score, 4)\n",
    "    }\n",
    "\n",
    "    #print(\"Computed Metrics:\", metrics)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=english-v2\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=english-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"../Models/small-checkpoints\",\n",
    "    run_name='ft-s-bud-full',\n",
    "    auto_find_batch_size=True,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    push_to_hub=False,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=100,\n",
    "    #label_smoothing_factor=.01,    # no help\n",
    "    #max_grad_norm=.6, # default is 1 # no help\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['dev'],\n",
    "    processing_class=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback()]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbillingsmoore\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/j/Desktop/MLotsawa/Notebooks/Models/TibetanToEnglishTranslation/TibetanToEnglishTranslationv2/BuddhistOnly/Finetuning/wandb/run-20250410_101713-7sqaocvm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/billingsmoore/english-v2/runs/7sqaocvm' target=\"_blank\">ft-s-bud-full</a></strong> to <a href='https://wandb.ai/billingsmoore/english-v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/billingsmoore/english-v2' target=\"_blank\">https://wandb.ai/billingsmoore/english-v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/billingsmoore/english-v2/runs/7sqaocvm' target=\"_blank\">https://wandb.ai/billingsmoore/english-v2/runs/7sqaocvm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3984087' max='10767800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3984086/10767800 209:30:39 < 356:44:06, 5.28 it/s, Epoch 37.00/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Chrf</th>\n",
       "      <th>Ter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.970800</td>\n",
       "      <td>0.957164</td>\n",
       "      <td>1.417000</td>\n",
       "      <td>10.260100</td>\n",
       "      <td>97.804100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.883300</td>\n",
       "      <td>0.848822</td>\n",
       "      <td>1.916800</td>\n",
       "      <td>12.640400</td>\n",
       "      <td>94.860800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.869400</td>\n",
       "      <td>0.776180</td>\n",
       "      <td>2.126200</td>\n",
       "      <td>14.256900</td>\n",
       "      <td>92.802800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.804300</td>\n",
       "      <td>0.724258</td>\n",
       "      <td>2.358800</td>\n",
       "      <td>15.288500</td>\n",
       "      <td>92.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.760800</td>\n",
       "      <td>0.685457</td>\n",
       "      <td>2.496000</td>\n",
       "      <td>16.058900</td>\n",
       "      <td>91.306200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.727000</td>\n",
       "      <td>0.655009</td>\n",
       "      <td>2.563500</td>\n",
       "      <td>16.451100</td>\n",
       "      <td>90.713700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.718300</td>\n",
       "      <td>0.630627</td>\n",
       "      <td>2.644000</td>\n",
       "      <td>16.926400</td>\n",
       "      <td>90.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.698400</td>\n",
       "      <td>0.610325</td>\n",
       "      <td>2.717100</td>\n",
       "      <td>17.183700</td>\n",
       "      <td>90.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.641700</td>\n",
       "      <td>0.593309</td>\n",
       "      <td>2.798200</td>\n",
       "      <td>17.471600</td>\n",
       "      <td>89.766200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.629700</td>\n",
       "      <td>0.579236</td>\n",
       "      <td>2.821400</td>\n",
       "      <td>17.710000</td>\n",
       "      <td>89.497400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.632200</td>\n",
       "      <td>0.567541</td>\n",
       "      <td>2.859000</td>\n",
       "      <td>17.795600</td>\n",
       "      <td>89.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.607800</td>\n",
       "      <td>0.556145</td>\n",
       "      <td>2.943000</td>\n",
       "      <td>18.025600</td>\n",
       "      <td>89.310200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.604400</td>\n",
       "      <td>0.547532</td>\n",
       "      <td>2.952200</td>\n",
       "      <td>18.158600</td>\n",
       "      <td>88.993000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.584200</td>\n",
       "      <td>0.539380</td>\n",
       "      <td>3.002200</td>\n",
       "      <td>18.370200</td>\n",
       "      <td>88.899500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.602100</td>\n",
       "      <td>0.531884</td>\n",
       "      <td>3.072600</td>\n",
       "      <td>18.469800</td>\n",
       "      <td>88.897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.592900</td>\n",
       "      <td>0.523986</td>\n",
       "      <td>3.079700</td>\n",
       "      <td>18.492200</td>\n",
       "      <td>88.843400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.571800</td>\n",
       "      <td>0.519109</td>\n",
       "      <td>3.088000</td>\n",
       "      <td>18.607500</td>\n",
       "      <td>88.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.575400</td>\n",
       "      <td>0.512884</td>\n",
       "      <td>3.106800</td>\n",
       "      <td>18.690300</td>\n",
       "      <td>88.554700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.571300</td>\n",
       "      <td>0.507520</td>\n",
       "      <td>3.144000</td>\n",
       "      <td>18.778000</td>\n",
       "      <td>88.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.552800</td>\n",
       "      <td>0.503022</td>\n",
       "      <td>3.181900</td>\n",
       "      <td>18.897100</td>\n",
       "      <td>88.446700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.561700</td>\n",
       "      <td>0.498948</td>\n",
       "      <td>3.203400</td>\n",
       "      <td>18.971900</td>\n",
       "      <td>88.370800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.494037</td>\n",
       "      <td>3.190700</td>\n",
       "      <td>19.006000</td>\n",
       "      <td>88.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.540400</td>\n",
       "      <td>0.488862</td>\n",
       "      <td>3.214700</td>\n",
       "      <td>18.992400</td>\n",
       "      <td>88.375600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.528800</td>\n",
       "      <td>0.486700</td>\n",
       "      <td>3.208200</td>\n",
       "      <td>19.071400</td>\n",
       "      <td>88.245400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.482113</td>\n",
       "      <td>3.245600</td>\n",
       "      <td>19.090900</td>\n",
       "      <td>88.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>0.479569</td>\n",
       "      <td>3.292400</td>\n",
       "      <td>19.242000</td>\n",
       "      <td>88.152600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.505600</td>\n",
       "      <td>0.475903</td>\n",
       "      <td>3.252400</td>\n",
       "      <td>19.127100</td>\n",
       "      <td>88.133900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.503200</td>\n",
       "      <td>0.473652</td>\n",
       "      <td>3.291500</td>\n",
       "      <td>19.309300</td>\n",
       "      <td>88.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.527700</td>\n",
       "      <td>0.471206</td>\n",
       "      <td>3.312300</td>\n",
       "      <td>19.330600</td>\n",
       "      <td>88.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.524700</td>\n",
       "      <td>0.468726</td>\n",
       "      <td>3.334600</td>\n",
       "      <td>19.319300</td>\n",
       "      <td>88.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.505100</td>\n",
       "      <td>0.465443</td>\n",
       "      <td>3.330800</td>\n",
       "      <td>19.348700</td>\n",
       "      <td>88.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.476500</td>\n",
       "      <td>0.463538</td>\n",
       "      <td>3.329200</td>\n",
       "      <td>19.385400</td>\n",
       "      <td>87.904200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>0.461346</td>\n",
       "      <td>3.370900</td>\n",
       "      <td>19.433800</td>\n",
       "      <td>87.965700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>0.459995</td>\n",
       "      <td>3.374500</td>\n",
       "      <td>19.505700</td>\n",
       "      <td>87.931800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.474800</td>\n",
       "      <td>0.458081</td>\n",
       "      <td>3.376400</td>\n",
       "      <td>19.518700</td>\n",
       "      <td>87.852900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.491700</td>\n",
       "      <td>0.455560</td>\n",
       "      <td>3.366000</td>\n",
       "      <td>19.458900</td>\n",
       "      <td>87.903000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 30:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'finetuned-small-model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
