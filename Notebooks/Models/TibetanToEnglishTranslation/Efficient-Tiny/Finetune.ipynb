{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Unfinetuned Tokenizer, Model, and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Models/pretrained-tiny-model\", device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('billingsmoore/getok-v0')\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, Adafactor, EarlyStoppingCallback\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    scale_parameter=True, \n",
    "    relative_step=False, \n",
    "    warmup_init=False, \n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "model, optimizer = accelerator.prepare(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU and CHRF metrics\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "ter_metric = evaluate.load(\"ter\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Postprocess text\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_score = bleu_result[\"score\"]\n",
    "\n",
    "    # Compute CHRF score\n",
    "    chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    chrf_score = chrf_result[\"score\"]\n",
    "\n",
    "    # Compute TER score\n",
    "    ter_result = ter_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    ter_score = ter_result[\"score\"]\n",
    "\n",
    "    # Return rounded results\n",
    "    metrics = {\n",
    "        \"bleu\": round(bleu_score, 4),\n",
    "        \"chrf\": round(chrf_score, 4),\n",
    "        \"ter\": round(ter_score, 4)\n",
    "    }\n",
    "\n",
    "    #print(\"Computed Metrics:\", metrics)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=english-v2\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=english-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_dataset = load_from_disk('/home/j/Desktop/MLotsawa/Notebooks/Models/TibetanToEnglishTranslation/TibetanToEnglishTranslationv2/Ground/BuddhistOnly/Finetuning/Data/bo-en/tokenized-finetuning-ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"Models/ft-checkpoints\",\n",
    "    run_name='ft-tiny-bud-full',\n",
    "    auto_find_batch_size=True,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    push_to_hub=False,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=100,\n",
    "    #label_smoothing_factor=.01,    # no help\n",
    "    #max_grad_norm=.6, # default is 1 # no help\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['dev'],\n",
    "    processing_class=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback()]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2799629' max='10767800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2799628/10767800 90:56:18 < 258:49:30, 8.55 it/s, Epoch 26.00/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Chrf</th>\n",
       "      <th>Ter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.973100</td>\n",
       "      <td>0.968893</td>\n",
       "      <td>0.779600</td>\n",
       "      <td>8.853300</td>\n",
       "      <td>98.802200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.896800</td>\n",
       "      <td>0.872205</td>\n",
       "      <td>1.353600</td>\n",
       "      <td>10.786300</td>\n",
       "      <td>96.314300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.897200</td>\n",
       "      <td>0.811253</td>\n",
       "      <td>1.725000</td>\n",
       "      <td>12.611600</td>\n",
       "      <td>93.868800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.838600</td>\n",
       "      <td>0.766009</td>\n",
       "      <td>1.965600</td>\n",
       "      <td>13.666000</td>\n",
       "      <td>93.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.804900</td>\n",
       "      <td>0.731470</td>\n",
       "      <td>2.088200</td>\n",
       "      <td>14.371500</td>\n",
       "      <td>92.483000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.778200</td>\n",
       "      <td>0.705041</td>\n",
       "      <td>2.177800</td>\n",
       "      <td>14.951300</td>\n",
       "      <td>91.904800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.775400</td>\n",
       "      <td>0.682949</td>\n",
       "      <td>2.215300</td>\n",
       "      <td>15.491200</td>\n",
       "      <td>91.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.759800</td>\n",
       "      <td>0.664945</td>\n",
       "      <td>2.279400</td>\n",
       "      <td>15.720700</td>\n",
       "      <td>90.880500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.707300</td>\n",
       "      <td>0.650083</td>\n",
       "      <td>2.357100</td>\n",
       "      <td>16.059600</td>\n",
       "      <td>90.727300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.699400</td>\n",
       "      <td>0.638205</td>\n",
       "      <td>2.389800</td>\n",
       "      <td>16.350600</td>\n",
       "      <td>90.425600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.627859</td>\n",
       "      <td>2.424800</td>\n",
       "      <td>16.436200</td>\n",
       "      <td>90.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.683700</td>\n",
       "      <td>0.618178</td>\n",
       "      <td>2.463900</td>\n",
       "      <td>16.632300</td>\n",
       "      <td>90.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.684300</td>\n",
       "      <td>0.610827</td>\n",
       "      <td>2.526200</td>\n",
       "      <td>16.802100</td>\n",
       "      <td>90.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.665100</td>\n",
       "      <td>0.603783</td>\n",
       "      <td>2.548100</td>\n",
       "      <td>16.996500</td>\n",
       "      <td>89.852600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.598077</td>\n",
       "      <td>2.560900</td>\n",
       "      <td>16.950800</td>\n",
       "      <td>89.828700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.685100</td>\n",
       "      <td>0.591828</td>\n",
       "      <td>2.619300</td>\n",
       "      <td>17.135700</td>\n",
       "      <td>89.850100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.662100</td>\n",
       "      <td>0.586654</td>\n",
       "      <td>2.608100</td>\n",
       "      <td>17.183000</td>\n",
       "      <td>89.606700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.671800</td>\n",
       "      <td>0.581848</td>\n",
       "      <td>2.628900</td>\n",
       "      <td>17.302800</td>\n",
       "      <td>89.569800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.669800</td>\n",
       "      <td>0.578570</td>\n",
       "      <td>2.630300</td>\n",
       "      <td>17.353600</td>\n",
       "      <td>89.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.650900</td>\n",
       "      <td>0.575165</td>\n",
       "      <td>2.702900</td>\n",
       "      <td>17.521400</td>\n",
       "      <td>89.448800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.663500</td>\n",
       "      <td>0.571175</td>\n",
       "      <td>2.679200</td>\n",
       "      <td>17.549900</td>\n",
       "      <td>89.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.624400</td>\n",
       "      <td>0.567664</td>\n",
       "      <td>2.707000</td>\n",
       "      <td>17.662400</td>\n",
       "      <td>89.364800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.644600</td>\n",
       "      <td>0.564428</td>\n",
       "      <td>2.741700</td>\n",
       "      <td>17.618900</td>\n",
       "      <td>89.411200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.633500</td>\n",
       "      <td>0.562001</td>\n",
       "      <td>2.680500</td>\n",
       "      <td>17.601600</td>\n",
       "      <td>89.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.615600</td>\n",
       "      <td>0.558964</td>\n",
       "      <td>2.733000</td>\n",
       "      <td>17.758900</td>\n",
       "      <td>89.253400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 20:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'finetuned-tiny-model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
