{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Unfinetuned Tokenizer, Model, and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Models/ft-checkpoints/checkpoint-2799628\", device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('billingsmoore/getok-v0')\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, Adafactor, EarlyStoppingCallback\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    scale_parameter=True, \n",
    "    relative_step=False, \n",
    "    warmup_init=False, \n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "model, optimizer = accelerator.prepare(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU and CHRF metrics\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "ter_metric = evaluate.load(\"ter\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Postprocess text\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_score = bleu_result[\"score\"]\n",
    "\n",
    "    # Compute CHRF score\n",
    "    chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    chrf_score = chrf_result[\"score\"]\n",
    "\n",
    "    # Compute TER score\n",
    "    ter_result = ter_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    ter_score = ter_result[\"score\"]\n",
    "\n",
    "    # Return rounded results\n",
    "    metrics = {\n",
    "        \"bleu\": round(bleu_score, 4),\n",
    "        \"chrf\": round(chrf_score, 4),\n",
    "        \"ter\": round(ter_score, 4)\n",
    "    }\n",
    "\n",
    "    #print(\"Computed Metrics:\", metrics)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=english-v2\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=english-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_dataset = load_from_disk('/home/j/Desktop/MLotsawa/Notebooks/Models/TibetanToEnglishTranslation/TibetanToEnglishTranslationv2/Ground/BuddhistOnly/Finetuning/Data/bo-en/tokenized-finetuning-ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"Models/ft-checkpoints\",\n",
    "    run_name='ft-tiny-bud-full',\n",
    "    auto_find_batch_size=True,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    push_to_hub=False,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=100,\n",
    "    #label_smoothing_factor=.01,    # no help\n",
    "    #max_grad_norm=.6, # default is 1 # no help\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['dev'],\n",
    "    processing_class=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback()]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2799628' max='10767800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2799628/10767800 91:36:27 < 260:43:45, 8.49 it/s, Epoch 26/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Chrf</th>\n",
       "      <th>Ter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.591700</td>\n",
       "      <td>0.555712</td>\n",
       "      <td>2.741400</td>\n",
       "      <td>17.758500</td>\n",
       "      <td>89.198500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.600700</td>\n",
       "      <td>0.553241</td>\n",
       "      <td>2.801200</td>\n",
       "      <td>17.881000</td>\n",
       "      <td>89.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.631900</td>\n",
       "      <td>0.551505</td>\n",
       "      <td>2.772700</td>\n",
       "      <td>17.890900</td>\n",
       "      <td>89.111300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.621000</td>\n",
       "      <td>0.547805</td>\n",
       "      <td>2.802000</td>\n",
       "      <td>17.890700</td>\n",
       "      <td>89.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.617700</td>\n",
       "      <td>0.546469</td>\n",
       "      <td>2.821800</td>\n",
       "      <td>18.035300</td>\n",
       "      <td>89.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.614500</td>\n",
       "      <td>0.543705</td>\n",
       "      <td>2.832500</td>\n",
       "      <td>18.001500</td>\n",
       "      <td>89.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.627000</td>\n",
       "      <td>0.543576</td>\n",
       "      <td>2.861700</td>\n",
       "      <td>18.109600</td>\n",
       "      <td>89.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.628500</td>\n",
       "      <td>0.540482</td>\n",
       "      <td>2.828800</td>\n",
       "      <td>18.046800</td>\n",
       "      <td>88.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.592300</td>\n",
       "      <td>0.539095</td>\n",
       "      <td>2.872700</td>\n",
       "      <td>18.139900</td>\n",
       "      <td>88.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.593500</td>\n",
       "      <td>0.537371</td>\n",
       "      <td>2.864500</td>\n",
       "      <td>18.155500</td>\n",
       "      <td>88.921800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.608000</td>\n",
       "      <td>0.536112</td>\n",
       "      <td>2.873500</td>\n",
       "      <td>18.140600</td>\n",
       "      <td>88.976500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.594000</td>\n",
       "      <td>0.534341</td>\n",
       "      <td>2.844200</td>\n",
       "      <td>18.186700</td>\n",
       "      <td>88.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.600500</td>\n",
       "      <td>0.533107</td>\n",
       "      <td>2.903000</td>\n",
       "      <td>18.244700</td>\n",
       "      <td>88.939500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.588900</td>\n",
       "      <td>0.532157</td>\n",
       "      <td>2.917300</td>\n",
       "      <td>18.297700</td>\n",
       "      <td>88.777800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.616000</td>\n",
       "      <td>0.531315</td>\n",
       "      <td>2.914700</td>\n",
       "      <td>18.240100</td>\n",
       "      <td>88.815200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.614000</td>\n",
       "      <td>0.528862</td>\n",
       "      <td>2.916400</td>\n",
       "      <td>18.244100</td>\n",
       "      <td>88.886400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.598200</td>\n",
       "      <td>0.528437</td>\n",
       "      <td>2.891100</td>\n",
       "      <td>18.273600</td>\n",
       "      <td>88.681300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.609700</td>\n",
       "      <td>0.527055</td>\n",
       "      <td>2.943300</td>\n",
       "      <td>18.364300</td>\n",
       "      <td>88.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.611300</td>\n",
       "      <td>0.526429</td>\n",
       "      <td>2.912800</td>\n",
       "      <td>18.332400</td>\n",
       "      <td>88.780500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.596100</td>\n",
       "      <td>0.525578</td>\n",
       "      <td>2.994500</td>\n",
       "      <td>18.479300</td>\n",
       "      <td>88.781900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.610900</td>\n",
       "      <td>0.524443</td>\n",
       "      <td>2.946100</td>\n",
       "      <td>18.426000</td>\n",
       "      <td>88.659700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.576500</td>\n",
       "      <td>0.523473</td>\n",
       "      <td>2.960700</td>\n",
       "      <td>18.474000</td>\n",
       "      <td>88.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.597500</td>\n",
       "      <td>0.522415</td>\n",
       "      <td>2.972300</td>\n",
       "      <td>18.422300</td>\n",
       "      <td>88.724300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.588500</td>\n",
       "      <td>0.521979</td>\n",
       "      <td>2.930000</td>\n",
       "      <td>18.410600</td>\n",
       "      <td>88.604600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.572500</td>\n",
       "      <td>0.520115</td>\n",
       "      <td>2.960700</td>\n",
       "      <td>18.419500</td>\n",
       "      <td>88.732700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.569000</td>\n",
       "      <td>0.520789</td>\n",
       "      <td>2.992200</td>\n",
       "      <td>18.557300</td>\n",
       "      <td>88.705200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2799628, training_loss=0.6047512223549558, metrics={'train_runtime': 329791.1146, 'train_samples_per_second': 261.201, 'train_steps_per_second': 32.65, 'total_flos': 2.5270222232538317e+17, 'train_loss': 0.6047512223549558, 'epoch': 26.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'finetuned-tiny-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbillingsmoore/mlotsawa-ground-tiny\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLotsawa/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:4346\u001b[39m, in \u001b[36mPreTrainedModel.push_to_hub\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4344\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tags:\n\u001b[32m   4345\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] = tags\n\u001b[32m-> \u001b[39m\u001b[32m4346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLotsawa/.venv/lib/python3.13/site-packages/transformers/utils/hub.py:979\u001b[39m, in \u001b[36mPushToHubMixin.push_to_hub\u001b[39m\u001b[34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[39m\n\u001b[32m    972\u001b[39m     \u001b[38;5;28mself\u001b[39m.save_pretrained(\n\u001b[32m    973\u001b[39m         work_dir,\n\u001b[32m    974\u001b[39m         max_shard_size=max_shard_size,\n\u001b[32m    975\u001b[39m         safe_serialization=safe_serialization,\n\u001b[32m    976\u001b[39m         save_jinja_files=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    977\u001b[39m     )\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwork_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[38;5;66;03m# Update model card if needed:\u001b[39;00m\n\u001b[32m    982\u001b[39m model_card.save(os.path.join(work_dir, \u001b[33m\"\u001b[39m\u001b[33mREADME.md\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLotsawa/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:4292\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   4287\u001b[39m     gc.collect()\n\u001b[32m   4289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   4290\u001b[39m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   4291\u001b[39m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4292\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4293\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4294\u001b[39m     save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLotsawa/.venv/lib/python3.13/site-packages/safetensors/torch.py:352\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    322\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    323\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    324\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    325\u001b[39m ):\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    328\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata=metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLotsawa/.venv/lib/python3.13/site-packages/safetensors/torch.py:589\u001b[39m, in \u001b[36m_flatten\u001b[39m\u001b[34m(tensors)\u001b[39m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[32m    577\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    578\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    579\u001b[39m \u001b[33m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m \u001b[33m        \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    586\u001b[39m     k: {\n\u001b[32m    587\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v.dtype).split(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m],\n\u001b[32m    588\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m: v.shape,\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43m_tobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    590\u001b[39m     }\n\u001b[32m    591\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors.items()\n\u001b[32m    592\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/MLotsawa/.venv/lib/python3.13/site-packages/safetensors/torch.py:507\u001b[39m, in \u001b[36m_tobytes\u001b[39m\u001b[34m(tensor, name)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    500\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mYou are trying to save a non contiguous tensor: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` which is not allowed. It either means you\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m are trying to save tensors which are reference of each other in which case it\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms recommended to save\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m only the full tensors, and reslice at load time, or simply call `.contiguous()` on your tensor to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m pack it before saving.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     )\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensor.device.type != \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    506\u001b[39m     \u001b[38;5;66;03m# Moving tensor to cpu before saving\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     tensor = \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mctypes\u001b[39;00m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub('billingsmoore/mlotsawa-ground-tiny', private=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
