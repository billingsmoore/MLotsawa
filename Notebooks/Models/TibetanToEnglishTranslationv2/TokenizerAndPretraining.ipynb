{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue Pretraining on T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "agg = load_dataset(\"billingsmoore/Aggregated-bo-en\", split='train')\n",
    "op = load_dataset(\"openpecha/cleaned_MT_v1.0.3\", split='train')\n",
    "\n",
    "dataset = concatenate_datasets([agg, op])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrupt Training Text\n",
    "\n",
    "T5 is trained by learning to correct missing spans in text. Thus, the training data must have spans masked for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def bo_corrupt_text(example):\n",
    "    text = example[\"bo\"]\n",
    "    words = text.split()\n",
    "    num_masks = max(1, len(words) // 6)  # Mask ~15-20% of the words\n",
    "    masked_indices = sorted(random.sample(range(len(words)), num_masks))\n",
    "\n",
    "    new_text = []\n",
    "    labels = []\n",
    "    current_mask = 0\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if i in masked_indices:\n",
    "            if not new_text or new_text[-1] != f\"<extra_id_{current_mask}>\":\n",
    "                new_text.append(f\"<extra_id_{current_mask}>\")\n",
    "                labels.append(f\"<extra_id_{current_mask}> {word}\")\n",
    "                current_mask += 1\n",
    "            else:\n",
    "                labels[-1] += f\" {word}\"\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "\n",
    "    return {\"input_text\": \" \".join(new_text), \"target_text\": \" \".join(labels)}\n",
    "\n",
    "bo_train_dataset = dataset.map(bo_corrupt_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_corrupt_text(example):\n",
    "    text = example[\"en\"]\n",
    "    words = text.split()\n",
    "    num_masks = max(1, len(words) // 6)  # Mask ~15-20% of the words\n",
    "    masked_indices = sorted(random.sample(range(len(words)), num_masks))\n",
    "\n",
    "    new_text = []\n",
    "    labels = []\n",
    "    current_mask = 0\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if i in masked_indices:\n",
    "            if not new_text or new_text[-1] != f\"<extra_id_{current_mask}>\":\n",
    "                new_text.append(f\"<extra_id_{current_mask}>\")\n",
    "                labels.append(f\"<extra_id_{current_mask}> {word}\")\n",
    "                current_mask += 1\n",
    "            else:\n",
    "                labels[-1] += f\" {word}\"\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "\n",
    "    return {\"input_text\": \" \".join(new_text), \"target_text\": \" \".join(labels)}\n",
    "\n",
    "en_train_dataset = dataset.map(en_corrupt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "ds = concatenate_datasets([en_train_dataset, bo_train_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Tokenizer\n",
    "\n",
    "T5 does not accomodate Tibetan in its tokenizer by default. To fix this, I've trained a custom tokenizer on the uncorrupted training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Initialize and train the tokenizer\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer.train_from_iterator(\n",
    "    (dataset['bo'] + dataset['en']),\n",
    "    vocab_size=32_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\", \"<unk>\"]\n",
    ")\n",
    "\n",
    "# Wrap the tokenizer with PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"[PAD]\",  # Set padding token\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Tokenizer to AutoTokenizer\n",
    "\n",
    "The tokenizer needs to be converted to an AutoTokenizer to function correctly. This is done by just saving the pre-trained tokenizer and re-loading it as an AutoTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer.save_pretrained('./my_tokenizer')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('my_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(example):\n",
    "    inputs = tokenizer(example[\"input_text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(example[\"target_text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    return {\n",
    "        \"input_ids\": inputs.input_ids,\n",
    "        \"attention_mask\": inputs.attention_mask,\n",
    "        \"labels\": targets.input_ids\n",
    "    }\n",
    "\n",
    "tokenized_dataset = ds.map(tokenize_data, batched=True, remove_columns=[\"bo\", 'en', 'topic', \"input_text\", \"target_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env WANDB_PROJECT=english-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/mt5-base\", device_map='cuda:0')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"mt5-base-continue-pretrain\",\n",
    "    save_strategy=\"epoch\",\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=1,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('pretrained-model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
