{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "DO NOT FORGET TO SEPARATE A TEST SET!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files='/home/j/Documents/Projects/MLotsawa/notebooks/data collection and cleaning/train-pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train'].train_test_split(.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 55216,\n",
       " 'tibetan': 'གཉེན་བཤེས་འཁྲུལ་པའི་གྲོགས་ལ་རྟག་པར་འཛིན༔',\n",
       " ' phonetic': 'nyen shé trulpé drok la takpar dzin',\n",
       " ' english': 'How utterly mistaken are the minds of ignorant beings!'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Unfinetuned Tokenizer, Model, and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 20:45:47.721544: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-25 20:45:47.721657: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-25 20:45:47.726912: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-25 20:45:48.945916: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/j/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint = \"billingsmoore/phonetic-tibetan-to-english-translation\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, device_map=\"cuda:0\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Tibetan to Tokenizer\n",
    "\n",
    "The T5 tokenizer does not notably support the Tibetan script. So, we need to add it manually. Once the characters have been added to the tokenizer, the model needs to have its token embeddings resized to accomodate the added tokens. This is all pretty straightforward, as seen in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32245, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tibetan characters to add\n",
    "tibetan_chars = [\n",
    "    # Consonants\n",
    "    \"ཀ\", \"ཁ\", \"ག\", \"ང\", \"ཅ\", \"ཆ\", \"ཇ\", \"ཉ\", \"ཏ\", \"ཐ\", \"ད\", \"ན\", \"པ\", \"པ\", \"ཕ\", \"བ\", \"མ\",\n",
    "    \"ཙ\", \"ཚ\", \"ཛ\", \"ཝ\", \"ཞ\", \"ཟ\", \"འ\", \"ཡ\", \"ར\", \"ལ\", \"ཤ\", \"ཥ\", \"ས\", \"ཧ\", \"ཨ\",\n",
    "\n",
    "    # Subjoined Consonants\n",
    "    \"ྐ\", \"ྑ\", \"ྒ\", \"ྒྷ\", \"ྔ\", \"ྕ\", \"ྖ\", \"ྗ\", \"྘\", \"ྙ\", \"ྚ\", \"ྛ\", \"ྜ\", \"ྜྷ\", \"ྞ\", \"ྟ\",\n",
    "    \"ྠ\", \"ྡ\", \"ྡྷ\", \"ྣ\", \"ྤ\", \"ྥ\", \"ྦ\", \"ྦྷ\", \"ྨ\", \"ྩ\", \"ྪ\", \"ྫ\", \"ྫྷ\", \"ྭ\", \"ྮ\", \"ྯ\",\n",
    "    \"ྰ\", \"ྱ\", \"ྲ\", \"ླ\", \"ྴ\", \"ྵ\", \"ྶ\", \"ྷ\", \"ྸ\", \"ྐྵ\", \"ྺ\", \"ྻ\", \"ྼ\", \"྽\", \"྾\", \"྿\",\n",
    "\n",
    "    # Vowels\n",
    "    \"ི\", \"ཱི\", \"ུ\", \"ཱུ\", \"ྲྀ\", \"ཷ\", \"ླྀ\", \"ཹ\", \"ེ\", \"ཻ\", \"ོ\", \"ཽ\", \"ཾ\", \"ཿ\",\n",
    "\n",
    "    # Other Marks and Symbols\n",
    "    \"འ\", \"ཡ\", \"ར\", \"ལ\", \"ཤ\", \"ཥ\", \"ས\", \"ཧ\", \"ཨ\",\n",
    "\n",
    "    # Additional Tibetan Characters\n",
    "    \"ཀྵ\", \"ཁྵ\", \"གྵ\", \"ངྵ\", \"ཅྵ\", \"ཆྵ\", \"ཇྵ\", \"ཉྵ\", \"ཏྵ\", \"ཐྵ\", \"དྵ\", \"ནྵ\", \"པྵ\", \n",
    "    \"པྵ\", \"ཕྵ\", \"བྵ\", \"མྵ\", \"ཙྵ\", \"ཚྵ\", \"ཛྵ\", \"ཝྵ\", \"ཞྵ\", \"ཟྵ\", \"འྵ\", \"ཡྵ\", \"རྵ\", \n",
    "    \"ལྵ\", \"ཤྵ\", \"ཥྵ\", \"སྵ\", \"ཧྵ\", \"ཨྵ\", \"པྪ\", \"པྫ\", \"པྫྷ\", \"པྭ\", \"པྮ\", \"པྯ\", \"པྰ\", \n",
    "    \"པྱ\", \"པྲ\", \"པླ\", \"པྴ\", \"པྵ\", \"པྶ\", \"པྷ\", \"པྸ\", \"པྐྵ\", \"པྺ\", \"པྻ\", \"པྼ\", \"པ྽\", \n",
    "    \"པ྾\", \"པ྿\"\n",
    "]\n",
    "\n",
    "\n",
    "#'ཀཁགངཅཆཇཉཏཐདནཔཕབམཙཚཛཝཞཟའཡརལཤཥསཧཨ'\n",
    "\n",
    "# Add the Tibetan characters to the tokenizer's vocabulary\n",
    "new_tokens = [char for char in tibetan_chars if char not in tokenizer.get_vocab()]\n",
    "\n",
    "# Add new tokens to the tokenizer\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Resize model embeddings to accommodate the new vocabulary size\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "The dataset can now be tokenized for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = 'tibetan'\n",
    "target_lang = ' english'\n",
    "\n",
    "def preprocess_function(examples):\n",
    "\n",
    "    inputs = [example for example in examples[source_lang]]\n",
    "    targets = [example for example in examples[target_lang]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c1f2ef37a042f8a5549f92075589f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532d830a51a74a9d83b719f7d0478bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12184 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Finally, we can train the model. Note that the optimizer used is Adafactor. This is the optimizer that is preferred for translation tasks and for the T5 model in general. The transformers api includes a built in version of Adafactor, but I define it separately here so that we can optimize it with the 'accelerate' library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, Adafactor\n",
    "\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    scale_parameter=True, \n",
    "    relative_step=False, \n",
    "    warmup_init=False, \n",
    "    lr=2e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    scale_parameter=True, \n",
    "    relative_step=False, \n",
    "    warmup_init=False, \n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "model, optimizer = accelerator.prepare(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbillingsmoore\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/j/Documents/Projects/MLotsawa/notebooks/tib-script-training/wandb/run-20240925_204711-ghwml841</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/billingsmoore/huggingface/runs/ghwml841/workspace' target=\"_blank\">base-t5-large</a></strong> to <a href='https://wandb.ai/billingsmoore/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/billingsmoore/huggingface' target=\"_blank\">https://wandb.ai/billingsmoore/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/billingsmoore/huggingface/runs/ghwml841/workspace' target=\"_blank\">https://wandb.ai/billingsmoore/huggingface/runs/ghwml841/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba38763376bc401581500cb5c78d1cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43155 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d4045ba9e449ffbcb3c4aa2972ae70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86305 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4436, 'grad_norm': 0.3283935785293579, 'learning_rate': 0.00029826197786918485, 'epoch': 0.03}\n",
      "{'loss': 0.2099, 'grad_norm': 0.20505794882774353, 'learning_rate': 0.0002965239557383697, 'epoch': 0.06}\n",
      "{'loss': 0.2003, 'grad_norm': 0.38089054822921753, 'learning_rate': 0.0002947859336075546, 'epoch': 0.09}\n",
      "{'loss': 0.1975, 'grad_norm': 0.11922363191843033, 'learning_rate': 0.00029304791147673946, 'epoch': 0.12}\n",
      "{'loss': 0.1885, 'grad_norm': 1.3892505168914795, 'learning_rate': 0.0002913098893459243, 'epoch': 0.14}\n",
      "{'loss': 0.1848, 'grad_norm': 0.15316489338874817, 'learning_rate': 0.0002895718672151092, 'epoch': 0.17}\n",
      "{'loss': 0.1746, 'grad_norm': 0.2239162027835846, 'learning_rate': 0.00028783384508429403, 'epoch': 0.2}\n",
      "{'loss': 0.1702, 'grad_norm': 0.3501626253128052, 'learning_rate': 0.0002860958229534789, 'epoch': 0.23}\n",
      "{'loss': 0.169, 'grad_norm': 0.21480047702789307, 'learning_rate': 0.00028435780082266377, 'epoch': 0.26}\n",
      "{'loss': 0.168, 'grad_norm': 0.13402053713798523, 'learning_rate': 0.00028261977869184864, 'epoch': 0.29}\n",
      "{'loss': 0.1691, 'grad_norm': 0.22125999629497528, 'learning_rate': 0.0002808817565610335, 'epoch': 0.32}\n",
      "{'loss': 0.1669, 'grad_norm': 0.402912437915802, 'learning_rate': 0.0002791437344302184, 'epoch': 0.35}\n",
      "{'loss': 0.1627, 'grad_norm': 0.16824586689472198, 'learning_rate': 0.00027740571229940326, 'epoch': 0.38}\n",
      "{'loss': 0.1612, 'grad_norm': 0.24989116191864014, 'learning_rate': 0.00027566769016858813, 'epoch': 0.41}\n",
      "{'loss': 0.1631, 'grad_norm': 0.17561110854148865, 'learning_rate': 0.00027392966803777295, 'epoch': 0.43}\n",
      "{'loss': 0.1555, 'grad_norm': 0.15025809407234192, 'learning_rate': 0.0002721916459069579, 'epoch': 0.46}\n",
      "{'loss': 0.155, 'grad_norm': 0.1736900806427002, 'learning_rate': 0.0002704536237761427, 'epoch': 0.49}\n",
      "{'loss': 0.1545, 'grad_norm': 0.17189408838748932, 'learning_rate': 0.00026871560164532757, 'epoch': 0.52}\n",
      "{'loss': 0.1535, 'grad_norm': 0.23158934712409973, 'learning_rate': 0.0002669775795145125, 'epoch': 0.55}\n",
      "{'loss': 0.1498, 'grad_norm': 0.23574082553386688, 'learning_rate': 0.0002652395573836973, 'epoch': 0.58}\n",
      "{'loss': 0.1498, 'grad_norm': 0.23532922565937042, 'learning_rate': 0.0002635015352528822, 'epoch': 0.61}\n",
      "{'loss': 0.1523, 'grad_norm': 0.8979027271270752, 'learning_rate': 0.00026176351312206706, 'epoch': 0.64}\n",
      "{'loss': 0.1443, 'grad_norm': 0.19540482759475708, 'learning_rate': 0.00026002549099125193, 'epoch': 0.67}\n",
      "{'loss': 0.1426, 'grad_norm': 0.1766587495803833, 'learning_rate': 0.0002582874688604368, 'epoch': 0.7}\n",
      "{'loss': 0.1458, 'grad_norm': 0.1669408082962036, 'learning_rate': 0.0002565494467296217, 'epoch': 0.72}\n",
      "{'loss': 0.1421, 'grad_norm': 0.27073556184768677, 'learning_rate': 0.00025481142459880655, 'epoch': 0.75}\n",
      "{'loss': 0.1423, 'grad_norm': 0.17651864886283875, 'learning_rate': 0.0002530734024679914, 'epoch': 0.78}\n",
      "{'loss': 0.1385, 'grad_norm': 0.17656345665454865, 'learning_rate': 0.00025133538033717624, 'epoch': 0.81}\n",
      "{'loss': 0.1383, 'grad_norm': 0.14215151965618134, 'learning_rate': 0.00024959735820636117, 'epoch': 0.84}\n",
      "{'loss': 0.1421, 'grad_norm': 0.6565664410591125, 'learning_rate': 0.000247859336075546, 'epoch': 0.87}\n",
      "{'loss': 0.1417, 'grad_norm': 0.21226920187473297, 'learning_rate': 0.00024612131394473086, 'epoch': 0.9}\n",
      "{'loss': 0.1392, 'grad_norm': 0.16834114491939545, 'learning_rate': 0.00024438329181391573, 'epoch': 0.93}\n",
      "{'loss': 0.1328, 'grad_norm': 0.2627754211425781, 'learning_rate': 0.00024264526968310063, 'epoch': 0.96}\n",
      "{'loss': 0.1356, 'grad_norm': 0.20056188106536865, 'learning_rate': 0.00024090724755228548, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1528a0cfd7e41ffac7c11e435170a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8631 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: context deadline exceeded (<Response [500]>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11411003768444061, 'eval_runtime': 2491.9697, 'eval_samples_per_second': 27.706, 'eval_steps_per_second': 3.464, 'epoch': 1.0}\n",
      "{'loss': 0.1288, 'grad_norm': 0.2582014501094818, 'learning_rate': 0.00023916922542147032, 'epoch': 1.01}\n",
      "{'loss': 0.12, 'grad_norm': 0.21864376962184906, 'learning_rate': 0.00023743120329065522, 'epoch': 1.04}\n",
      "{'loss': 0.1226, 'grad_norm': 0.1880509853363037, 'learning_rate': 0.0002356931811598401, 'epoch': 1.07}\n",
      "{'loss': 0.1257, 'grad_norm': 0.3232046961784363, 'learning_rate': 0.00023395515902902494, 'epoch': 1.1}\n",
      "{'loss': 0.1205, 'grad_norm': 0.1799364686012268, 'learning_rate': 0.00023221713689820984, 'epoch': 1.13}\n",
      "{'loss': 0.1202, 'grad_norm': 0.20776763558387756, 'learning_rate': 0.00023047911476739468, 'epoch': 1.16}\n",
      "{'loss': 0.1185, 'grad_norm': 0.2594086527824402, 'learning_rate': 0.00022874109263657956, 'epoch': 1.19}\n",
      "{'loss': 0.1223, 'grad_norm': 0.3204634189605713, 'learning_rate': 0.0002270030705057644, 'epoch': 1.22}\n",
      "{'loss': 0.1231, 'grad_norm': 0.22221463918685913, 'learning_rate': 0.0002252650483749493, 'epoch': 1.25}\n",
      "{'loss': 0.1171, 'grad_norm': 0.2823980152606964, 'learning_rate': 0.00022352702624413415, 'epoch': 1.27}\n",
      "{'loss': 0.1154, 'grad_norm': 0.19718949496746063, 'learning_rate': 0.00022178900411331902, 'epoch': 1.3}\n",
      "{'loss': 0.1177, 'grad_norm': 0.1693287193775177, 'learning_rate': 0.0002200509819825039, 'epoch': 1.33}\n",
      "{'loss': 0.1208, 'grad_norm': 0.14935119450092316, 'learning_rate': 0.00021831295985168877, 'epoch': 1.36}\n",
      "{'loss': 0.1171, 'grad_norm': 0.25971677899360657, 'learning_rate': 0.0002165749377208736, 'epoch': 1.39}\n",
      "{'loss': 0.1161, 'grad_norm': 0.1843590885400772, 'learning_rate': 0.00021483691559005848, 'epoch': 1.42}\n",
      "{'loss': 0.1172, 'grad_norm': 0.42665502429008484, 'learning_rate': 0.00021309889345924338, 'epoch': 1.45}\n",
      "{'loss': 0.1125, 'grad_norm': 0.2985341548919678, 'learning_rate': 0.00021136087132842823, 'epoch': 1.48}\n",
      "{'loss': 0.1157, 'grad_norm': 0.2202461063861847, 'learning_rate': 0.00020962284919761307, 'epoch': 1.51}\n",
      "{'loss': 0.1147, 'grad_norm': 0.33788251876831055, 'learning_rate': 0.00020788482706679797, 'epoch': 1.54}\n",
      "{'loss': 0.1124, 'grad_norm': 0.26198142766952515, 'learning_rate': 0.00020614680493598285, 'epoch': 1.56}\n",
      "{'loss': 0.1136, 'grad_norm': 0.24749311804771423, 'learning_rate': 0.0002044087828051677, 'epoch': 1.59}\n",
      "{'loss': 0.1101, 'grad_norm': 0.18942445516586304, 'learning_rate': 0.0002026707606743526, 'epoch': 1.62}\n",
      "{'loss': 0.1105, 'grad_norm': 0.20394916832447052, 'learning_rate': 0.00020093273854353744, 'epoch': 1.65}\n",
      "{'loss': 0.1153, 'grad_norm': 0.2116648554801941, 'learning_rate': 0.0001991947164127223, 'epoch': 1.68}\n",
      "{'loss': 0.1096, 'grad_norm': 0.26704028248786926, 'learning_rate': 0.00019745669428190715, 'epoch': 1.71}\n",
      "{'loss': 0.1121, 'grad_norm': 0.1991230547428131, 'learning_rate': 0.00019571867215109205, 'epoch': 1.74}\n",
      "{'loss': 0.1115, 'grad_norm': 0.18938514590263367, 'learning_rate': 0.0001939806500202769, 'epoch': 1.77}\n",
      "{'loss': 0.1115, 'grad_norm': 0.15869906544685364, 'learning_rate': 0.00019224262788946177, 'epoch': 1.8}\n",
      "{'loss': 0.1075, 'grad_norm': 0.2173566222190857, 'learning_rate': 0.00019050460575864664, 'epoch': 1.82}\n",
      "{'loss': 0.1094, 'grad_norm': 0.3176783323287964, 'learning_rate': 0.00018876658362783152, 'epoch': 1.85}\n",
      "{'loss': 0.107, 'grad_norm': 0.35041701793670654, 'learning_rate': 0.00018702856149701636, 'epoch': 1.88}\n",
      "{'loss': 0.1068, 'grad_norm': 0.19470953941345215, 'learning_rate': 0.00018529053936620126, 'epoch': 1.91}\n",
      "{'loss': 0.1028, 'grad_norm': 0.18252266943454742, 'learning_rate': 0.00018355251723538613, 'epoch': 1.94}\n",
      "{'loss': 0.1079, 'grad_norm': 0.2811696529388428, 'learning_rate': 0.00018181449510457098, 'epoch': 1.97}\n",
      "{'loss': 0.1062, 'grad_norm': 0.1755194067955017, 'learning_rate': 0.00018007647297375582, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652309c6b02141cdb02b1523a182dd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8631 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08213633298873901, 'eval_runtime': 2495.8243, 'eval_samples_per_second': 27.663, 'eval_steps_per_second': 3.458, 'epoch': 2.0}\n",
      "{'loss': 0.094, 'grad_norm': 0.33351725339889526, 'learning_rate': 0.00017833845084294072, 'epoch': 2.03}\n",
      "{'loss': 0.0954, 'grad_norm': 0.20615004003047943, 'learning_rate': 0.0001766004287121256, 'epoch': 2.06}\n",
      "{'loss': 0.0938, 'grad_norm': 0.40948981046676636, 'learning_rate': 0.00017486240658131044, 'epoch': 2.09}\n",
      "{'loss': 0.0934, 'grad_norm': 0.3150709867477417, 'learning_rate': 0.00017312438445049534, 'epoch': 2.11}\n",
      "{'loss': 0.0924, 'grad_norm': 0.17849668860435486, 'learning_rate': 0.0001713863623196802, 'epoch': 2.14}\n",
      "{'loss': 0.0947, 'grad_norm': 0.23893845081329346, 'learning_rate': 0.00016964834018886506, 'epoch': 2.17}\n",
      "{'loss': 0.0952, 'grad_norm': 0.25892844796180725, 'learning_rate': 0.0001679103180580499, 'epoch': 2.2}\n",
      "{'loss': 0.094, 'grad_norm': 0.26203739643096924, 'learning_rate': 0.0001661722959272348, 'epoch': 2.23}\n",
      "{'loss': 0.0933, 'grad_norm': 0.21653632819652557, 'learning_rate': 0.00016443427379641965, 'epoch': 2.26}\n",
      "{'loss': 0.093, 'grad_norm': 0.24919863045215607, 'learning_rate': 0.00016269625166560452, 'epoch': 2.29}\n",
      "{'loss': 0.0937, 'grad_norm': 0.2523762881755829, 'learning_rate': 0.0001609582295347894, 'epoch': 2.32}\n",
      "{'loss': 0.0917, 'grad_norm': 0.2647170126438141, 'learning_rate': 0.00015922020740397427, 'epoch': 2.35}\n",
      "{'loss': 0.0911, 'grad_norm': 0.2683556377887726, 'learning_rate': 0.0001574821852731591, 'epoch': 2.38}\n",
      "{'loss': 0.0898, 'grad_norm': 0.328858345746994, 'learning_rate': 0.000155744163142344, 'epoch': 2.4}\n",
      "{'loss': 0.0931, 'grad_norm': 0.3326776921749115, 'learning_rate': 0.00015400614101152886, 'epoch': 2.43}\n",
      "{'loss': 0.0908, 'grad_norm': 0.2583887279033661, 'learning_rate': 0.00015226811888071373, 'epoch': 2.46}\n",
      "{'loss': 0.0931, 'grad_norm': 0.30299103260040283, 'learning_rate': 0.00015053009674989858, 'epoch': 2.49}\n",
      "{'loss': 0.0906, 'grad_norm': 0.25654709339141846, 'learning_rate': 0.00014879207461908345, 'epoch': 2.52}\n",
      "{'loss': 0.0905, 'grad_norm': 0.21983809769153595, 'learning_rate': 0.00014705405248826835, 'epoch': 2.55}\n",
      "{'loss': 0.0935, 'grad_norm': 0.24596314132213593, 'learning_rate': 0.00014531603035745322, 'epoch': 2.58}\n",
      "{'loss': 0.0905, 'grad_norm': 0.26467785239219666, 'learning_rate': 0.00014357800822663807, 'epoch': 2.61}\n",
      "{'loss': 0.0901, 'grad_norm': 0.2495078444480896, 'learning_rate': 0.00014183998609582294, 'epoch': 2.64}\n",
      "{'loss': 0.0888, 'grad_norm': 0.5387611389160156, 'learning_rate': 0.0001401019639650078, 'epoch': 2.66}\n",
      "{'loss': 0.0913, 'grad_norm': 0.2764776051044464, 'learning_rate': 0.00013836394183419268, 'epoch': 2.69}\n",
      "{'loss': 0.0903, 'grad_norm': 0.2646688222885132, 'learning_rate': 0.00013662591970337756, 'epoch': 2.72}\n",
      "{'loss': 0.0888, 'grad_norm': 0.19841721653938293, 'learning_rate': 0.0001348878975725624, 'epoch': 2.75}\n",
      "{'loss': 0.0887, 'grad_norm': 0.18761304020881653, 'learning_rate': 0.00013314987544174727, 'epoch': 2.78}\n",
      "{'loss': 0.0875, 'grad_norm': 0.18194156885147095, 'learning_rate': 0.00013141185331093215, 'epoch': 2.81}\n",
      "{'loss': 0.0882, 'grad_norm': 0.2337685525417328, 'learning_rate': 0.00012967383118011702, 'epoch': 2.84}\n",
      "{'loss': 0.0883, 'grad_norm': 0.26062047481536865, 'learning_rate': 0.0001279358090493019, 'epoch': 2.87}\n",
      "{'loss': 0.0882, 'grad_norm': 0.29124972224235535, 'learning_rate': 0.00012619778691848674, 'epoch': 2.9}\n",
      "{'loss': 0.0872, 'grad_norm': 0.3234640657901764, 'learning_rate': 0.0001244597647876716, 'epoch': 2.93}\n",
      "{'loss': 0.086, 'grad_norm': 0.4077025353908539, 'learning_rate': 0.00012272174265685648, 'epoch': 2.95}\n",
      "{'loss': 0.0882, 'grad_norm': 0.23077638447284698, 'learning_rate': 0.00012098372052604135, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a5753a381242ceaa22f9dfc28f1278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8631 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06309106945991516, 'eval_runtime': 2493.2051, 'eval_samples_per_second': 27.692, 'eval_steps_per_second': 3.462, 'epoch': 3.0}\n",
      "{'loss': 0.0832, 'grad_norm': 0.2174602746963501, 'learning_rate': 0.00011924569839522623, 'epoch': 3.01}\n",
      "{'loss': 0.0774, 'grad_norm': 0.2557228207588196, 'learning_rate': 0.00011750767626441109, 'epoch': 3.04}\n",
      "{'loss': 0.0774, 'grad_norm': 0.30044952034950256, 'learning_rate': 0.00011576965413359596, 'epoch': 3.07}\n",
      "{'loss': 0.0817, 'grad_norm': 0.32031986117362976, 'learning_rate': 0.00011403163200278082, 'epoch': 3.1}\n",
      "{'loss': 0.0788, 'grad_norm': 0.3648945391178131, 'learning_rate': 0.00011229360987196569, 'epoch': 3.13}\n",
      "{'loss': 0.0791, 'grad_norm': 0.2257809340953827, 'learning_rate': 0.00011055558774115055, 'epoch': 3.16}\n",
      "{'loss': 0.0789, 'grad_norm': 0.18788762390613556, 'learning_rate': 0.00010881756561033542, 'epoch': 3.19}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase-t5-large\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     auto_find_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     15\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/memory.py:146\u001b[0m, in \u001b[0;36mfind_executable_batch_size.<locals>.decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo executable batch size found, reached zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2341\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2338\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2339\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[0;32m-> 2341\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2345\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/optimizer.py:170\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/optimization.py:905\u001b[0m, in \u001b[0;36mAdafactor.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    903\u001b[0m     p_data_fp32\u001b[38;5;241m.\u001b[39madd_(p_data_fp32, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m lr))\n\u001b[0;32m--> 905\u001b[0m \u001b[43mp_data_fp32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m {torch\u001b[38;5;241m.\u001b[39mfloat16, torch\u001b[38;5;241m.\u001b[39mbfloat16}:\n\u001b[1;32m    908\u001b[0m     p\u001b[38;5;241m.\u001b[39mcopy_(p_data_fp32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"base-t5-large\",\n",
    "    auto_find_batch_size=True,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, #check this\n",
    "    push_to_hub=False,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['train'],\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
