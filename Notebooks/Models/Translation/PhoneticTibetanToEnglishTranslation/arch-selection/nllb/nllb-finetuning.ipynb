{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning NLLB\n",
    "\n",
    "The purpose of this notebook is document the process of fine-tuning an NLLB model for translating from Literary Tibetan to English. \n",
    "\n",
    "Some of the code in this notebook is based on the the tutorial ['How To Fine Tune a NLLB 200 Model for Translating A New Language'](https://cointegrated.medium.com/how-to-fine-tune-a-nllb-200-model-for-translating-a-new-language-a37fc706b865). However, the training loop and preprocessing have been heavily revised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "from transformers.optimization import Adafactor\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import trange\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text Pairs\n",
    "\n",
    "### Loading the Data\n",
    "\n",
    "The code below loads in the text pairs as a list, [Tibetan, English]. Then batches them. This is a helper function for the custom training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_pairs(data, batch_size, num_batches):\n",
    "\n",
    "    print(f'Loading {data}...', end='\\r')\n",
    "    \n",
    "    data_path = '../../data/training-batches/' + data\n",
    "\n",
    "    with open(data_path) as f:\n",
    "        lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "    pairs = []\n",
    "    for line in lines:\n",
    "        try:\n",
    "            tib, eng = line.split(\",\")[:2]\n",
    "            eng = eng.lower()\n",
    "            pairs.append([tib, eng])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(f'Batching {data}... ', end='\\r')\n",
    "    \n",
    "    copy = pairs.copy()\n",
    "    batches = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        xx, yy = [], []\n",
    "        for _ in range(batch_size):\n",
    "            i = random.randint(0, len(copy)-1)\n",
    "            item = copy[i]\n",
    "            xx.append(item[0])\n",
    "            yy.append(item[1])\n",
    "            del copy[i]\n",
    "        batches.append([xx, yy])\n",
    "\n",
    "    print(f'Training on {data}             ')\n",
    "\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "### Pre-Trained Model\n",
    "Here, I've downloaded the pre-trained NLLB model and its associated tokenizer.\n",
    "\n",
    "'nllb-checkpoint-0' is my proof-of-concept NLLB finetuned on the dataset for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\").cuda()\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/home/j/Documents/Projects/MLotsawa/notebooks/nllb/nllb-checkpoint-0\").cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Below, I've selected the Adafactor optimizer for training. The values passed to the optimizer are taken from the tutorial mentioned above and are arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adafactor(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    scale_parameter=False,\n",
    "    relative_step=False,\n",
    "    lr=1e-4,\n",
    "    clip_threshold=1.0,\n",
    "    weight_decay=1e-3,\n",
    ")\n",
    "scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = 'nllb-checkpoint-0.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "Below, I've written a custom training. The first draft was adapted from the previously mentioned tutorial but it has since been substantially re-written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = []\n",
    "\n",
    "epoch_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_dir, optimizer, batch_size=16, epochs=1):\n",
    "    global all_losses, epoch_losses\n",
    "    \n",
    "    x, y, loss = None, None, None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    steps_per_batch = 100\n",
    "\n",
    "    \n",
    "\n",
    "    for _ in range(epochs):\n",
    "\n",
    "            remaining_shards = os.listdir(data_dir)\n",
    "\n",
    "            losses = []  # simple tracking of average loss\n",
    "\n",
    "            for i in range(len(os.listdir(data_dir))):\n",
    "\n",
    "                random.shuffle(remaining_shards)\n",
    "\n",
    "                shard = remaining_shards[0]\n",
    "\n",
    "                del remaining_shards[0]\n",
    "\n",
    "\n",
    "                batches = get_batch_pairs(shard, batch_size, steps_per_batch)\n",
    "\n",
    "                shard_losses = []\n",
    "\n",
    "                desc = ('Epoch '+str(_)+ '.' + str(i) +', Shard: ' + str(shard))\n",
    "                tq = trange(len(shard_losses), steps_per_batch, desc=desc) # take 100 random batches from each data shard\n",
    "                \n",
    "                for i in tq:\n",
    "\n",
    "                    xx, yy = batches[i][0], batches[i][1]\n",
    "\n",
    "                    try:\n",
    "\n",
    "                        tokenizer.src_lang = 'bo'\n",
    "                        x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=128).to(model.device)\n",
    "                        tokenizer.src_lang = 'eng_Latn'\n",
    "                        y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=128).to(model.device)\n",
    "                        # -100 is a magic value ignored in the loss function\n",
    "                        # because we don't want the model to learn to predict padding ids\n",
    "                        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "                        loss = model(**x, labels=y.input_ids).loss\n",
    "                        loss.backward()\n",
    "                        losses.append(loss.item())\n",
    "                        shard_losses.append(loss.item())\n",
    "\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad(set_to_none=True)\n",
    "                        scheduler.step()\n",
    "\n",
    "                        print('loss: ' + str(np.mean(losses)),  end=\"\\r\")\n",
    "\n",
    "                    except RuntimeError as e:  # usually, it is out-of-memory\n",
    "                        optimizer.zero_grad(set_to_none=True)\n",
    "                        x, y, loss = None, None, None\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache()\n",
    "                        continue\n",
    "\n",
    "                print('loss: ' + str(np.mean(losses)))\n",
    "                all_losses+=losses\n",
    "                try:\n",
    "                    plt.close()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                ys1 = all_losses\n",
    "                xs = [x for x in range(len(ys1))]\n",
    "\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(xs, ys1)\n",
    "\n",
    "                ys2 = epoch_losses\n",
    "                xs = [x for x in range(len(ys2))]\n",
    "\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(xs, ys2)\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "            epoch_losses.append(losses[-1])\n",
    "            model.save_pretrained(MODEL_SAVE_PATH+str(_))\n",
    "\n",
    "    return [all_losses, epoch_losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train(data_dir='../../data/training-batches', optimizer=optimizer, batch_size=32, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
