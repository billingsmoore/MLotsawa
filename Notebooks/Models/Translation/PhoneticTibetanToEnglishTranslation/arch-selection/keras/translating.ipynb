{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating Tibetan to English\n",
    "\n",
    "The purpose of this notebook is to present how the trained translation model can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "First, we'll import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load in the trained tokenizers and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/j/Documents/Projects/MLotsawa/models/keras/tokenizers/big-dataset/eng-tokenizer.pickle', 'rb') as handle:\n",
    "    eng_tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('/home/j/Documents/Projects/MLotsawa/models/keras/tokenizers/big-dataset/tib-tokenizer.pickle', 'rb') as handle:\n",
    "    tib_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tib_eng_translator = tf.keras.models.load_model(\"/home/j/Documents/Projects/MLotsawa/models/keras/tib-eng-translator-big-dataset.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to bring our constants back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 15\n",
    "VOCAB_SIZE = 15000\n",
    "\n",
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Translated Sentences\n",
    "\n",
    "Even if the translations are perfect, the outputs of our model are not meaningful sentences. The model only outputs numerical tokens. In order to turn these into something that a human can read they need to be decoded.\n",
    "\n",
    "Below is a function to decode these translated sentences. This function takes in an English sentence, runs it through our translator model then works its way through the output of the model, converting the output into words in Tibetan using our tokenizers.\n",
    "\n",
    "Part of decoding the sequence is sampling the probabilities of tokens that should follow the existing translation. The sampler is the algorithm that is used to select that next work or token. Here I've used the Greedy sampler which simply finds the highest likelihood next word and adds it to the translated sentence. It is computationally inexpensive and because the outputs are pretty short we don't need to worry about the Greedy sampler outputting long, repetitive sentences that don't make much sense, which can be an issue with the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tib_eng_translate(input_sentence):\n",
    "\n",
    "    input_sentences = tf.constant([input_sentence])\n",
    "\n",
    "    batch_size = tf.shape(input_sentences)[0]\n",
    "\n",
    "    encoder_input_tokens = tib_tokenizer(input_sentences).to_tensor(\n",
    "        shape=(None, MAX_SEQUENCE_LENGTH)\n",
    "    )\n",
    "\n",
    "    def next(prompt, cache, index):\n",
    "        logits = tib_eng_translator([encoder_input_tokens, prompt])[:, index - 1, :]\n",
    "        hidden_states = None\n",
    "        return logits, hidden_states, cache\n",
    "    \n",
    "    length = MAX_SEQUENCE_LENGTH\n",
    "    start = tf.fill((batch_size, 1), eng_tokenizer.token_to_id(\"[START]\"))\n",
    "    pad = tf.fill((batch_size, length - 1), eng_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    prompt = tf.concat((start, pad), axis=-1)\n",
    "\n",
    "    generated_tokens = keras_nlp.samplers.GreedySampler()(\n",
    "        next,\n",
    "        prompt,\n",
    "        end_token_id=eng_tokenizer.token_to_id(\"[END]\"),\n",
    "        index=1\n",
    "    )\n",
    "    generated_sentences = eng_tokenizer.detokenize(generated_tokens)\n",
    "    try:\n",
    "        generated_sentences = generated_sentences.numpy()[0].decode(\"utf-8\")\n",
    "\n",
    "        generated_sentences = (\n",
    "            generated_sentences.replace(\"[PAD]\", \"\")\n",
    "            .replace(\"[START]\", \"\")\n",
    "            .replace(\"[END]\", \"\")\n",
    "            .replace(\"[UNK]\", \"\")\n",
    "            .strip()\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "    return generated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Translations\n",
    "\n",
    "Now, let's look at some example translations from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ['sangye chö dang tsok kyi chok nam la', \n",
    "         'changchub bardu dak ni kyab su chi',\n",
    "         'dak gi jin sok gyipé sönam kyi',\n",
    "         'dro la pen chir sangye drubpar shok']\n",
    "\n",
    "translated = [tib_eng_translate(sentence) for sentence in input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the most powerful and abundant    invoking the handsome',\n",
       " 'my own mind now i have arrived in their transferred before us  after',\n",
       " 'the auspicious opportunity recalling the enlightened ladys rage and abundant   and the',\n",
       " 'the outer and inner benefit to all other offerings']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
