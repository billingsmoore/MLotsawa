{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"Data/kham_asr_dataset\")['test']\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer, Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2Config\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=False)\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"་\")\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"Models/baseline-fine-tuned\")\n",
    "\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_predictions(batch):\n",
    "    # Load and resample the audio\n",
    "    audio = batch[\"audio\"]\n",
    "    inputs = processor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"], \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).input_values.to(\"cuda\")\n",
    "\n",
    "    # Generate logits and get argmax predictions\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Decode predictions to text\n",
    "    batch[\"prediction\"] = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "# Apply the function to the test dataset\n",
    "processed_test_dataset = dataset.map(generate_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "from tibetan_wer.metrics import wer, ser\n",
    "\n",
    "# Extract predictions and references\n",
    "predictions = [elt[0].replace(' ', '་') for elt in processed_test_dataset[\"prediction\"]]\n",
    "references = [elt for elt in processed_test_dataset[\"transcription\"]]\n",
    "\n",
    "# Compute metrics\n",
    "cer = jiwer.cer(predictions, references)\n",
    "ser = ser(predictions, references)['micro_ser']\n",
    "wer = wer(predictions, references)['micro_wer']\n",
    "\n",
    "print(f\"Character Error Rate (CER): {cer}\")\n",
    "print(f\"Syllable Error Rate (SER): {ser}\")\n",
    "print(f\"Word Error Rate (WER): {wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('baseline_preds.pickle', 'wb') as f:\n",
    "    pickle.dump(f, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
