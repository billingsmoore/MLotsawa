{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('rat-poc-ds-w-context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': 'སྒོམ། སྤྱོད། འབྲས་ཀྱི་རྣམ་གཞག་ཅུང་ཟད་བརྗོད་ན། འཇུག་སྒོ་ནི། ཕྱི་ནང་སྒྲུབ་གསང་ཆུ་བོ་བཞི་རྫོགས་ཀྱི་དབང་སུམ་ཅུ་རྩ་དྲུག་གིས་རྒྱུད་སྨིན་ཅིང་། དམ་ཚིག་གཞུང་བཞིན་སྲུང་བའོ།། གཞི་ལྟ་བ་གཏན་ལ་འབེབས་ཚུལ་ནི། ཆོས་ཐམས་ཅད་ཀྱི་གཤིས་ལུགས་དཀྱིལ་འཁོར་གསུམ་དུ་གནས་པ་ཉིད་ཀྱི་ཆོས་ཅན་ཤེས་བྱའི་དོན་ལ་ཤེས་བྱེད་རིགས་པའི་གཏན་ཚིགས་རྣམས་ཀྱི་ཇི་ལྟ་བར་གཏན་ལ་ཕབ་སྟེ་རྟོགས་པའོ།། ལམ་སྒོམ་པས་ཉམས་སུ་ལེན་ཚུལ་ནི། ཆོས་ཉིད་སྙིང་པོ་ཇི་བཞིན་པའི་ངང་ལ་མཉམ་པར་འཇོག་པ་མཚན་མེད་རྣམ་པར་མི་རྟོག་པའི་ཏིང་ངེ་འཛིན་དང་། བསྐྱེད་སྔགས་བརྗོད་པ་ཙམ་གྱིས་རྟེན་དང་བརྟེན་པའི་འཁོར་ལོར་བསྒོམ་པ་མཚན་བཅས་ལྷའི་ཏིང་འཛིན་བསྒོམ་པ་གྲོལ་ལམ་དང་། ཐབས་ལམ་སྟེང་འོག་གི་སྒོ་ལ་བརྟེན་ནས་བདེ་སྟོང་གི་ཡེ་ཤེས་བསྐྱེད་པ་སྟེ་ལམ་གཉིས་ཀྱི་རྣལ་འབྱོར་བསྒོམ་པའོ།།',\n",
       " 'Target': 'Once again, let us say a little about its point of entry, view, meditation, conduct and results: i. Entry Point One’s mind is matured through the thirty-six empowerments in which the four rivers—outer, inner, accomplishing and secret—are complete, and one keeps the samayas as described in the texts. ii. View Through logical reasoning one determines that which is to be known, the fact that all phenomena are characterized as being the three mandalas in their fundamental nature, and realizes that this is so. iii. Meditation Meditation practice here consists of two paths. On the path of liberation one practises the non-conceptual samādhi of simply resting in a state that accords with the essence of reality itself, and the conceptual samādhi of deity practice, in which one visualizes the mandala of supporting palace and supported deities simply by reciting the mantra of generation. On the path of skilful means one generates the wisdom of bliss and emptiness through the practices of the upper and lower gateways.',\n",
       " 'File_Name': None,\n",
       " 'Machine Aligned': True,\n",
       " '__index_level_0__': 294714,\n",
       " 'Tag': {'Buddhist': True,\n",
       "  'LH labels': ['Longchen Nyingtik', 'Yumka Dechen Gyalmo'],\n",
       "  'Topic': 'Meditation'},\n",
       " 'context': ['འདི་དག་ལ་བརྟེན་ནས་བསམ་གཏན་གྱི་སེམས་བསྒྲུབ་པར་བྱའོ།། -> It is through these that the mind of meditation is attained.',\n",
       "  '1 ཞེས་སོགས་མང་དུ་བཤད་པ་ལྟར་རོ།། དེ་ལྟར་ན་སྐབས་འདིར་གཞན་དགེ་སྦྱོར་ཅི་རིགས་ཙམ་དང་། ཁྱད་པར་བླ་མའི་རྣལ་འབྱོར་གྱི་སྐབས་སུ་བླ་མ་ལ་ཆོས་སྐུའི་མོས་གུས་བསྐྱེད་ནས་སྣང་སྲིད་ཆོས་སྐུར་རྟོགས་པར་གསོལ་བ་དྲག་ཏུ་འདེབས་པ་སྔོན་དུ་འགྲོ་བས། སྔར་བཤད་པ་ལྟར་ངོ་བོའི་གཞག་ཐབས་ཀྱི་མཉམ་བཞག་རྩེ་གཅིག་ཏུ་བསྐྱངས་ཏེ་རིག་པ་དྭངས་སྙིགས་ཕྱེད་པར་བྱས་ལ། དེ་ནས་རིག་པ་མ་ཡེངས་ཤིང་འདི་སྒོམ་གྱི་གཏད་སོ་མེད་པར་སེམས་གུ་ཡངས་སང་ངེ་བའི་ངང་ནས། ཡིད་ལ་བདེ་དོག་ཆེར་མི་འབྱུང་བའི་རྣམ་རྟོག་ཕྲ་མོ་ཤར་བའམ་ཆེད་དུ་སྤྲོས་ཏེ། དེའི་རྣམ་པ་གསལ་བཞིན་པ་འི་ངོ་བོ་ལ་འཛིན་མེད་ཡེངས་མེད་དུ་ཅེ་རེ་བལྟ། དེ་བཞིན་དུ་ཡིད་ལ་དགའ་མི་དགའི་ཟུག་རྔུ་བསྐྱེད་པའི་རྣམ་རྟོག་རགས་པ་མངོན་ཚན་ཆེ་བ་ཤར་བའམ་ཆེད་དུ་སྤྲོས་ཏེ། ཕྲ་རགས་དེ་གཉིས་ཀྱི་ངོ་བོ་ལ་ཁྱད་པར་ཅི་འདུག་ཡང་ཡང་བལྟ། དེ་ལ་སྒྲོ་འདོགས་ཆོད་ན་དེ་དག་དང་སེམས་གནས་དུས་ཀྱི་ངོ་བོ་ལ་ཁྱད་པར་ཅི་འདུག་བལྟ། -> 1 There are many such statements. At this point, during whatever spiritual practices you do and particularly during guru yoga, begin by arousing the devotion in which you see your gurus as the dharmakāya. Make fervent supplications to realize all possible appearances are the dharmakāya. Next, as described before, cultivate one-pointed equipoise, which is the means for resting in the essence, and differentiate between brilliant states of mind and sullied ones. Then, in a spacious and vibrant state of mind—when your awareness is undistracted and your meditation is without reference point—there may arise a subtle thought that doesn’t create great mental pleasure or pain, or you should cause one to arise. Without clinging or wandering, look fixedly at its essence while its manifestation is vivid. Similarly, there may arise a clearly evident coarse thought, the kind that creates the miseries associated with mental joys and sorrows, or you should cause one to arise. Look again and again to see if there is any difference between the essence of a subtle thought and that of a coarse thought. When you sever misinterpretations regarding that, look to see if there is any difference between the essence of those thoughts and the essence of the still mind.',\n",
       "  'གཞི་ལྟ་བ་གཏན་ལ་འབེབས་ཚུལ་ནི། སེམས་ལས་འདས་པའི་རིག་པ་རྗེན་པ་འདི་ཀའི་ངོ་བོ་སྟོང་པ་ཆོས་ཀྱི་སྐུ། རང་བཞིན་གསལ་བ་ལོངས་སྐུ། ཐུགས་རྗེ་ཀུན་ཁྱབ་སྤྲུལ་སྐུ་སྟེ། སྐུ་གསུམ་དབྱེར་མེད་པའི་རང་བྱུང་གི་ཡེ་ཤེས་ཀྱི་རང་ཞལ་ལྟ་བའོ།། ལེ་ལོ་ཅན་འབད་མེད་དུ་གྲོལ་བ་ཀ་དག་ཁྲེགས་ཆོད་ཀྱི་ལམ་དང་། བརྩོན་འགྲུས་ཅན་འབད་བཅས་སུ་གྲོལ་བ་ལྷུན་གྲུབ་ཐོད་རྒལ་གྱི་ལམ་བསྒོམ་པའོ།། ཅིར་སྣང་ཐམས་ཅད་ཆོས་ཉིད་ཀྱི་རོལ་པར་ཤར་བས་སྤང་བླང་རེ་དོགས་དང་བྲལ་བར་སྤྱོད་པའོ།། འབྲས་བུ་ཇི་ལྟར་ཐོབ་ཚུལ་ནི། ལམ་གྱི་སྣང་བ་བཞི་མཐར་ཕྱིན་ཏེ། འཇའ་ལུས་འཕོ་བ་ཆེན་པོའི་སྐུ་མཆོག་བརྙེས་ནས་དཔལ་ཀུན་ཏུ་བཟང་པོའི་གོ་འཕང་ངམ་བཅུ་གསུམ་ཡེ་ཤེས་བླ་མའི་ས་ཐོབ་པའོ།། -> View The view is definitively established by looking directly into the naturally arising wisdom in which the three kāyas are inseparable: the empty essence of naked awareness beyond the ordinary mind is the dharmakāya, its cognizant nature is the sambhogakāya, and its all-pervasive compassionate energy is the nirmāṇakāya. Meditation The meditation consists of the approach of cutting through resistance to primordial purity (kadak trekchö), through which the lazy can reach liberation without effort, and the approach of the direct realization of spontaneous presence (lhundrup tögal), through which the diligent can reach liberation with exertion. Conduct The conduct is free from hope and fear and adopting and abandoning, because all that appears manifests as the display of reality itself. v. Results Perfecting the four visions of the path, one gains the supreme kāya, the rainbow body of great transference, and attains the level of glorious Samantabhadra, the thirteenth bhūmi known as ‘Unexcelled Wisdom’ (yeshe lama). ']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': 'འཇམ་དབལ་གྱིས་རྗེ་བླ་མར། ཟླ་བ་གྲགས་བ་ནི་སངས་རྒྱས་ཀྱི་ཞིང་ཁམས་ཤིག་ནས་ས་མཐོན་པོའི་བྱང་ཆུབ་སེམས་དཔའ་རྣམ་དཔྱོད་སྙིང་སྟོབས་ཅན་ཞིག་མགོན་བོ་ཀླུ་སྒྲུབ་ཀྱི་ཟབ་མོ་ལྟ་བའི་བསྟན་བ་སྤེལ་ཕྱིར་བསམ་བཞིན་བྱོན་པ་ཡིན་བས་དེའི་གཞུང་ལ་ནོར་འཁྲུལ་མེད་ཚུལ་གསུངས་བ་བཞིན།',\n",
       " 'Target': 'Mañjushrı told him that Chandrakırti was a courageous and discerning bodhisattva of high degree who had come here from a buddhafield with the intention of spreading Protector Nagarjuna’s teachings on the profound view, and that his writings were error free.',\n",
       " 'File_Name': 'TM0718',\n",
       " 'Machine Aligned': False,\n",
       " '__index_level_0__': 1102159,\n",
       " 'Tag': {'Buddhist': True,\n",
       "  'LH labels': ['Nyingma Mönlam'],\n",
       "  'Topic': 'Mahamudra'},\n",
       " 'context': ['འདི་དག་ལ་བརྟེན་ནས་བསམ་གཏན་གྱི་སེམས་བསྒྲུབ་པར་བྱའོ།། -> It is through these that the mind of meditation is attained.',\n",
       "  '1 ཞེས་སོགས་མང་དུ་བཤད་པ་ལྟར་རོ།། དེ་ལྟར་ན་སྐབས་འདིར་གཞན་དགེ་སྦྱོར་ཅི་རིགས་ཙམ་དང་། ཁྱད་པར་བླ་མའི་རྣལ་འབྱོར་གྱི་སྐབས་སུ་བླ་མ་ལ་ཆོས་སྐུའི་མོས་གུས་བསྐྱེད་ནས་སྣང་སྲིད་ཆོས་སྐུར་རྟོགས་པར་གསོལ་བ་དྲག་ཏུ་འདེབས་པ་སྔོན་དུ་འགྲོ་བས། སྔར་བཤད་པ་ལྟར་ངོ་བོའི་གཞག་ཐབས་ཀྱི་མཉམ་བཞག་རྩེ་གཅིག་ཏུ་བསྐྱངས་ཏེ་རིག་པ་དྭངས་སྙིགས་ཕྱེད་པར་བྱས་ལ། དེ་ནས་རིག་པ་མ་ཡེངས་ཤིང་འདི་སྒོམ་གྱི་གཏད་སོ་མེད་པར་སེམས་གུ་ཡངས་སང་ངེ་བའི་ངང་ནས། ཡིད་ལ་བདེ་དོག་ཆེར་མི་འབྱུང་བའི་རྣམ་རྟོག་ཕྲ་མོ་ཤར་བའམ་ཆེད་དུ་སྤྲོས་ཏེ། དེའི་རྣམ་པ་གསལ་བཞིན་པ་འི་ངོ་བོ་ལ་འཛིན་མེད་ཡེངས་མེད་དུ་ཅེ་རེ་བལྟ། དེ་བཞིན་དུ་ཡིད་ལ་དགའ་མི་དགའི་ཟུག་རྔུ་བསྐྱེད་པའི་རྣམ་རྟོག་རགས་པ་མངོན་ཚན་ཆེ་བ་ཤར་བའམ་ཆེད་དུ་སྤྲོས་ཏེ། ཕྲ་རགས་དེ་གཉིས་ཀྱི་ངོ་བོ་ལ་ཁྱད་པར་ཅི་འདུག་ཡང་ཡང་བལྟ། དེ་ལ་སྒྲོ་འདོགས་ཆོད་ན་དེ་དག་དང་སེམས་གནས་དུས་ཀྱི་ངོ་བོ་ལ་ཁྱད་པར་ཅི་འདུག་བལྟ། -> 1 There are many such statements. At this point, during whatever spiritual practices you do and particularly during guru yoga, begin by arousing the devotion in which you see your gurus as the dharmakāya. Make fervent supplications to realize all possible appearances are the dharmakāya. Next, as described before, cultivate one-pointed equipoise, which is the means for resting in the essence, and differentiate between brilliant states of mind and sullied ones. Then, in a spacious and vibrant state of mind—when your awareness is undistracted and your meditation is without reference point—there may arise a subtle thought that doesn’t create great mental pleasure or pain, or you should cause one to arise. Without clinging or wandering, look fixedly at its essence while its manifestation is vivid. Similarly, there may arise a clearly evident coarse thought, the kind that creates the miseries associated with mental joys and sorrows, or you should cause one to arise. Look again and again to see if there is any difference between the essence of a subtle thought and that of a coarse thought. When you sever misinterpretations regarding that, look to see if there is any difference between the essence of those thoughts and the essence of the still mind.',\n",
       "  'གཞི་ལྟ་བ་གཏན་ལ་འབེབས་ཚུལ་ནི། སེམས་ལས་འདས་པའི་རིག་པ་རྗེན་པ་འདི་ཀའི་ངོ་བོ་སྟོང་པ་ཆོས་ཀྱི་སྐུ། རང་བཞིན་གསལ་བ་ལོངས་སྐུ། ཐུགས་རྗེ་ཀུན་ཁྱབ་སྤྲུལ་སྐུ་སྟེ། སྐུ་གསུམ་དབྱེར་མེད་པའི་རང་བྱུང་གི་ཡེ་ཤེས་ཀྱི་རང་ཞལ་ལྟ་བའོ།། ལེ་ལོ་ཅན་འབད་མེད་དུ་གྲོལ་བ་ཀ་དག་ཁྲེགས་ཆོད་ཀྱི་ལམ་དང་། བརྩོན་འགྲུས་ཅན་འབད་བཅས་སུ་གྲོལ་བ་ལྷུན་གྲུབ་ཐོད་རྒལ་གྱི་ལམ་བསྒོམ་པའོ།། ཅིར་སྣང་ཐམས་ཅད་ཆོས་ཉིད་ཀྱི་རོལ་པར་ཤར་བས་སྤང་བླང་རེ་དོགས་དང་བྲལ་བར་སྤྱོད་པའོ།། འབྲས་བུ་ཇི་ལྟར་ཐོབ་ཚུལ་ནི། ལམ་གྱི་སྣང་བ་བཞི་མཐར་ཕྱིན་ཏེ། འཇའ་ལུས་འཕོ་བ་ཆེན་པོའི་སྐུ་མཆོག་བརྙེས་ནས་དཔལ་ཀུན་ཏུ་བཟང་པོའི་གོ་འཕང་ངམ་བཅུ་གསུམ་ཡེ་ཤེས་བླ་མའི་ས་ཐོབ་པའོ།། -> View The view is definitively established by looking directly into the naturally arising wisdom in which the three kāyas are inseparable: the empty essence of naked awareness beyond the ordinary mind is the dharmakāya, its cognizant nature is the sambhogakāya, and its all-pervasive compassionate energy is the nirmāṇakāya. Meditation The meditation consists of the approach of cutting through resistance to primordial purity (kadak trekchö), through which the lazy can reach liberation without effort, and the approach of the direct realization of spontaneous presence (lhundrup tögal), through which the diligent can reach liberation with exertion. Conduct The conduct is free from hope and fear and adopting and abandoning, because all that appears manifests as the display of reality itself. v. Results Perfecting the four visions of the path, one gains the supreme kāya, the rainbow body of great transference, and attains the level of glorious Samantabhadra, the thirteenth bhūmi known as ‘Unexcelled Wisdom’ (yeshe lama). ']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer, Model, and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint = \"google/switch-base-8\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, device_map=\"cuda:0\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32355, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a list of all Tibetan Unicode characters (U+0F00 to U+0FFF)\n",
    "tibetan_chars = [chr(codepoint) for codepoint in range(0x0F00, 0x0FFF)]\n",
    "\n",
    "# Add the Tibetan characters to the tokenizer's vocabulary\n",
    "new_tokens = [char for char in tibetan_chars if char not in tokenizer.get_vocab()]\n",
    "\n",
    "# Add new tokens to the tokenizer\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Resize model embeddings to accommodate the new vocabulary size\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (659 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'སྒོམ།སྤྱོད།འབྲས་ཀྱི་རྣམ་གཞག་ཅུང་ཟད་བརྗོད་ན།འཇུག་སྒོ་ནི།ཕྱི་ནང་སྒྲུབ་གསང་ཆུ་བོ་བཞི་རྫོགས་ཀྱི་དབང་སུམ་ཅུ་རྩ་དྲུག་གིས་རྒྱུད་སྨིན་ཅིང་།དམ་ཚིག་གཞུང་བཞིན་སྲུང་བའོ།།གཞི་ལྟ་བ་གཏན་ལ་འབེབས་ཚུལ་ནི།ཆོས་ཐམས་ཅད་ཀྱི་གཤིས་ལུགས་དཀྱིལ་འཁོར་གསུམ་དུ་གནས་པ་ཉིད་ཀྱི་ཆོས་ཅན་ཤེས་བྱའི་དོན་ལ་ཤེས་བྱེད་རིགས་པའི་གཏན་ཚིགས་རྣམས་ཀྱི་ཇི་ལྟ་བར་གཏན་ལ་ཕབ་སྟེ་རྟོགས་པའོ།།ལམ་སྒོམ་པས་ཉམས་སུ་ལེན་ཚུལ་ནི།ཆོས་ཉིད་སྙིང་པོ་ཇི་བཞིན་པའི་ངང་ལ་མཉམ་པར་འཇོག་པ་མཚན་མེད་རྣམ་པར་མི་རྟོག་པའི་ཏིང་ངེ་འཛིན་དང་།བསྐྱེད་སྔགས་བརྗོད་པ་ཙམ་གྱིས་རྟེན་དང་བརྟེན་པའི་འཁོར་ལོར་བསྒོམ་པ་མཚན་བཅས་ལྷའི་ཏིང་འཛིན་བསྒོམ་པ་གྲོལ་ལམ་དང་།ཐབས་ལམ་སྟེང་འོག་གི་སྒོ་ལ་བརྟེན་ནས་བདེ་སྟོང་གི་ཡེ་ཤེས་བསྐྱེད་པ་སྟེ་ལམ་གཉིས་ཀྱི་རྣལ་འབྱོར་བསྒོམ་པའོ།།</s>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = tokenizer.encode(dataset['train'][0]['Source'])\n",
    "dec = tokenizer.decode(enc)\n",
    "dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "The dataset can now be tokenized for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9441a83689c44754a53c7044ff7da490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c048320f6d2c4407a7f46dbe9058ef15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_with_multiple_contexts(example):\n",
    "    input_text = (\n",
    "        f\"translate Tibetan to English: {example['Source']} Context: \"\n",
    "        + \" \".join([f\"{i+1}. {context}\" for i, context in enumerate(example['context'])])\n",
    "    )\n",
    "    target_text = example['Target']\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=512).input_ids,\n",
    "        \"labels\": tokenizer(target_text, padding=\"max_length\", truncation=True, max_length=512).input_ids,\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "dataset_with_contexts = dataset.map(\n",
    "    preprocess_with_multiple_contexts,\n",
    "    batched=False,  # Process one example at a time\n",
    "    #remove_columns=dataset.column_names,  # Remove old columns to update the schema\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU and CHRF metrics\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Postprocess text\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_score = bleu_result[\"score\"]\n",
    "\n",
    "    # Compute CHRF score\n",
    "    chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    chrf_score = chrf_result[\"score\"]\n",
    "\n",
    "    # Compute generation length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    avg_gen_len = np.mean(prediction_lens)\n",
    "\n",
    "    # Return rounded results\n",
    "    return {\n",
    "        \"bleu\": round(bleu_score, 4),\n",
    "        \"chrf\": round(chrf_score, 4),\n",
    "        \"gen_len\": round(avg_gen_len, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, Adafactor\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    scale_parameter=True, \n",
    "    relative_step=False, \n",
    "    warmup_init=False, \n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "model, optimizer = accelerator.prepare(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT='moe-translation-experiments'\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT='moe-translation-experiments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbillingsmoore\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/j/Documents/MLotsawa/Notebooks/Models/MixtureOfExperts/RetrievalAugmentedTranslation/wandb/run-20250206_203431-w50lrpu2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/billingsmoore/%27moe-translation-experiments%27/runs/w50lrpu2' target=\"_blank\">rat-single-context</a></strong> to <a href='https://wandb.ai/billingsmoore/%27moe-translation-experiments%27' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/billingsmoore/%27moe-translation-experiments%27' target=\"_blank\">https://wandb.ai/billingsmoore/%27moe-translation-experiments%27</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/billingsmoore/%27moe-translation-experiments%27/runs/w50lrpu2' target=\"_blank\">https://wandb.ai/billingsmoore/%27moe-translation-experiments%27/runs/w50lrpu2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b31a649d2e14085ae35b7c614fbdbce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06540db4074942678fd58cebaae1376b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb35d3b1a5074517b4f19e1c4c6a5948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfd3e748483491498dfa5f30ba20ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5419, 'grad_norm': 116.56964111328125, 'learning_rate': 0.0002988888888888889, 'epoch': 0.01}\n",
      "{'loss': 0.6846, 'grad_norm': 2.418769598007202, 'learning_rate': 0.0002977777777777777, 'epoch': 0.02}\n",
      "{'loss': 0.5799, 'grad_norm': 1.2639744281768799, 'learning_rate': 0.00029666666666666665, 'epoch': 0.03}\n",
      "{'loss': 0.6246, 'grad_norm': 3.5089917182922363, 'learning_rate': 0.0002955555555555555, 'epoch': 0.04}\n",
      "{'loss': 0.568, 'grad_norm': 3.307935953140259, 'learning_rate': 0.00029444444444444445, 'epoch': 0.06}\n",
      "{'loss': 0.5424, 'grad_norm': 0.7533357739448547, 'learning_rate': 0.00029333333333333327, 'epoch': 0.07}\n",
      "{'loss': 0.5749, 'grad_norm': 1.0895600318908691, 'learning_rate': 0.0002922222222222222, 'epoch': 0.08}\n",
      "{'loss': 0.5006, 'grad_norm': 0.8133211731910706, 'learning_rate': 0.0002911111111111111, 'epoch': 0.09}\n",
      "{'loss': 0.5114, 'grad_norm': 1.6593739986419678, 'learning_rate': 0.00029, 'epoch': 0.1}\n",
      "{'loss': 0.5092, 'grad_norm': 1.5580596923828125, 'learning_rate': 0.0002888888888888888, 'epoch': 0.11}\n",
      "{'loss': 0.4853, 'grad_norm': 6.625301361083984, 'learning_rate': 0.00028777777777777775, 'epoch': 0.12}\n",
      "{'loss': 0.5021, 'grad_norm': 3.123877763748169, 'learning_rate': 0.0002866666666666667, 'epoch': 0.13}\n",
      "{'loss': 0.4684, 'grad_norm': 2.66495943069458, 'learning_rate': 0.00028555555555555555, 'epoch': 0.14}\n",
      "{'loss': 0.4822, 'grad_norm': 0.7895844578742981, 'learning_rate': 0.0002844444444444444, 'epoch': 0.16}\n",
      "{'loss': 0.518, 'grad_norm': 1.8481143712997437, 'learning_rate': 0.0002833333333333333, 'epoch': 0.17}\n",
      "{'loss': 0.5399, 'grad_norm': 0.618417501449585, 'learning_rate': 0.00028222222222222223, 'epoch': 0.18}\n",
      "{'loss': 0.4825, 'grad_norm': 1.3753138780593872, 'learning_rate': 0.0002811111111111111, 'epoch': 0.19}\n",
      "{'loss': 0.4827, 'grad_norm': 3.3150813579559326, 'learning_rate': 0.00028, 'epoch': 0.2}\n",
      "{'loss': 0.4476, 'grad_norm': 0.9815902709960938, 'learning_rate': 0.00027888888888888885, 'epoch': 0.21}\n",
      "{'loss': 0.513, 'grad_norm': 1.1695479154586792, 'learning_rate': 0.0002777777777777778, 'epoch': 0.22}\n",
      "{'loss': 0.578, 'grad_norm': 0.695031464099884, 'learning_rate': 0.00027666666666666665, 'epoch': 0.23}\n",
      "{'loss': 0.5427, 'grad_norm': 3.2191052436828613, 'learning_rate': 0.0002755555555555555, 'epoch': 0.24}\n",
      "{'loss': 0.4668, 'grad_norm': 1.9677486419677734, 'learning_rate': 0.00027444444444444445, 'epoch': 0.26}\n",
      "{'loss': 0.4984, 'grad_norm': 1.0899893045425415, 'learning_rate': 0.00027333333333333333, 'epoch': 0.27}\n",
      "{'loss': 0.4926, 'grad_norm': 0.7736023664474487, 'learning_rate': 0.0002722222222222222, 'epoch': 0.28}\n",
      "{'loss': 0.5337, 'grad_norm': 0.9072696566581726, 'learning_rate': 0.0002711111111111111, 'epoch': 0.29}\n",
      "{'loss': 0.4777, 'grad_norm': 1.0902032852172852, 'learning_rate': 0.00027, 'epoch': 0.3}\n",
      "{'loss': 0.4736, 'grad_norm': 3.477689266204834, 'learning_rate': 0.0002688888888888889, 'epoch': 0.31}\n",
      "{'loss': 0.4574, 'grad_norm': 3.858564615249634, 'learning_rate': 0.00026777777777777775, 'epoch': 0.32}\n",
      "{'loss': 0.4917, 'grad_norm': 0.7296169996261597, 'learning_rate': 0.0002666666666666666, 'epoch': 0.33}\n",
      "{'loss': 0.5286, 'grad_norm': 0.8095192313194275, 'learning_rate': 0.00026555555555555555, 'epoch': 0.34}\n",
      "{'loss': 0.4902, 'grad_norm': 1.1127039194107056, 'learning_rate': 0.00026444444444444443, 'epoch': 0.36}\n",
      "{'loss': 0.5226, 'grad_norm': 0.9472877979278564, 'learning_rate': 0.0002633333333333333, 'epoch': 0.37}\n",
      "{'loss': 0.4713, 'grad_norm': 2.753770589828491, 'learning_rate': 0.00026222222222222223, 'epoch': 0.38}\n",
      "{'loss': 0.4234, 'grad_norm': 3.282848834991455, 'learning_rate': 0.0002611111111111111, 'epoch': 0.39}\n",
      "{'loss': 0.4903, 'grad_norm': 1.092785120010376, 'learning_rate': 0.00026, 'epoch': 0.4}\n",
      "{'loss': 0.4969, 'grad_norm': 1.3328864574432373, 'learning_rate': 0.00025888888888888885, 'epoch': 0.41}\n",
      "{'loss': 0.4941, 'grad_norm': 2.109659194946289, 'learning_rate': 0.0002577777777777778, 'epoch': 0.42}\n",
      "{'loss': 0.4637, 'grad_norm': 0.8949025869369507, 'learning_rate': 0.00025666666666666665, 'epoch': 0.43}\n",
      "{'loss': 0.4962, 'grad_norm': 0.9585047960281372, 'learning_rate': 0.00025555555555555553, 'epoch': 0.44}\n",
      "{'loss': 0.4686, 'grad_norm': 0.3972627818584442, 'learning_rate': 0.0002544444444444444, 'epoch': 0.46}\n",
      "{'loss': 0.5235, 'grad_norm': 0.6142979860305786, 'learning_rate': 0.00025333333333333333, 'epoch': 0.47}\n",
      "{'loss': 0.4727, 'grad_norm': 0.4614129662513733, 'learning_rate': 0.0002522222222222222, 'epoch': 0.48}\n",
      "{'loss': 0.4431, 'grad_norm': 0.7769684195518494, 'learning_rate': 0.0002511111111111111, 'epoch': 0.49}\n",
      "{'loss': 0.4513, 'grad_norm': 0.4019991457462311, 'learning_rate': 0.00025, 'epoch': 0.5}\n",
      "{'loss': 0.4181, 'grad_norm': 0.9032948613166809, 'learning_rate': 0.0002488888888888889, 'epoch': 0.51}\n",
      "{'loss': 0.4425, 'grad_norm': 1.016701340675354, 'learning_rate': 0.00024777777777777775, 'epoch': 0.52}\n",
      "{'loss': 0.4625, 'grad_norm': 0.2728501558303833, 'learning_rate': 0.0002466666666666666, 'epoch': 0.53}\n",
      "{'loss': 0.47, 'grad_norm': 0.7378031015396118, 'learning_rate': 0.00024555555555555556, 'epoch': 0.54}\n",
      "{'loss': 0.4792, 'grad_norm': 1.2065180540084839, 'learning_rate': 0.00024444444444444443, 'epoch': 0.56}\n",
      "{'loss': 0.4775, 'grad_norm': 0.5565642714500427, 'learning_rate': 0.0002433333333333333, 'epoch': 0.57}\n",
      "{'loss': 0.5017, 'grad_norm': 1.3336877822875977, 'learning_rate': 0.0002422222222222222, 'epoch': 0.58}\n",
      "{'loss': 0.5171, 'grad_norm': 1.1836602687835693, 'learning_rate': 0.00024111111111111108, 'epoch': 0.59}\n",
      "{'loss': 0.4392, 'grad_norm': 1.2900110483169556, 'learning_rate': 0.00023999999999999998, 'epoch': 0.6}\n",
      "{'loss': 0.4709, 'grad_norm': 0.8482068181037903, 'learning_rate': 0.00023888888888888885, 'epoch': 0.61}\n",
      "{'loss': 0.4495, 'grad_norm': 0.8502635359764099, 'learning_rate': 0.00023777777777777775, 'epoch': 0.62}\n",
      "{'loss': 0.4865, 'grad_norm': 0.28968358039855957, 'learning_rate': 0.00023666666666666663, 'epoch': 0.63}\n",
      "{'loss': 0.4341, 'grad_norm': 3.018737554550171, 'learning_rate': 0.00023555555555555553, 'epoch': 0.64}\n",
      "{'loss': 0.5014, 'grad_norm': 1.4138656854629517, 'learning_rate': 0.0002344444444444444, 'epoch': 0.66}\n",
      "{'loss': 0.4591, 'grad_norm': 0.3535625636577606, 'learning_rate': 0.0002333333333333333, 'epoch': 0.67}\n",
      "{'loss': 0.4496, 'grad_norm': 3.651923656463623, 'learning_rate': 0.00023222222222222218, 'epoch': 0.68}\n",
      "{'loss': 0.4444, 'grad_norm': 2.1381242275238037, 'learning_rate': 0.00023111111111111108, 'epoch': 0.69}\n",
      "{'loss': 0.447, 'grad_norm': 1.4498811960220337, 'learning_rate': 0.00023, 'epoch': 0.7}\n",
      "{'loss': 0.4719, 'grad_norm': 1.252195119857788, 'learning_rate': 0.00022888888888888885, 'epoch': 0.71}\n",
      "{'loss': 0.4512, 'grad_norm': 4.307941436767578, 'learning_rate': 0.00022777777777777778, 'epoch': 0.72}\n",
      "{'loss': 0.4531, 'grad_norm': 0.7609859108924866, 'learning_rate': 0.00022666666666666663, 'epoch': 0.73}\n",
      "{'loss': 0.4701, 'grad_norm': 0.798515796661377, 'learning_rate': 0.00022555555555555556, 'epoch': 0.74}\n",
      "{'loss': 0.4734, 'grad_norm': 0.6073943972587585, 'learning_rate': 0.0002244444444444444, 'epoch': 0.76}\n",
      "{'loss': 0.41, 'grad_norm': 1.1918540000915527, 'learning_rate': 0.00022333333333333333, 'epoch': 0.77}\n",
      "{'loss': 0.4383, 'grad_norm': 0.4249314069747925, 'learning_rate': 0.00022222222222222218, 'epoch': 0.78}\n",
      "{'loss': 0.4591, 'grad_norm': 2.6979305744171143, 'learning_rate': 0.0002211111111111111, 'epoch': 0.79}\n",
      "{'loss': 0.4231, 'grad_norm': 0.47687533497810364, 'learning_rate': 0.00021999999999999995, 'epoch': 0.8}\n",
      "{'loss': 0.478, 'grad_norm': 1.2516300678253174, 'learning_rate': 0.00021888888888888888, 'epoch': 0.81}\n",
      "{'loss': 0.4577, 'grad_norm': 1.1336251497268677, 'learning_rate': 0.00021777777777777778, 'epoch': 0.82}\n",
      "{'loss': 0.4424, 'grad_norm': 1.9486653804779053, 'learning_rate': 0.00021666666666666666, 'epoch': 0.83}\n",
      "{'loss': 0.5015, 'grad_norm': 0.719343900680542, 'learning_rate': 0.00021555555555555556, 'epoch': 0.84}\n",
      "{'loss': 0.4211, 'grad_norm': 0.7023884057998657, 'learning_rate': 0.00021444444444444443, 'epoch': 0.86}\n",
      "{'loss': 0.4365, 'grad_norm': 0.8847843408584595, 'learning_rate': 0.00021333333333333333, 'epoch': 0.87}\n",
      "{'loss': 0.4812, 'grad_norm': 1.243647575378418, 'learning_rate': 0.0002122222222222222, 'epoch': 0.88}\n",
      "{'loss': 0.4753, 'grad_norm': 0.6692889332771301, 'learning_rate': 0.0002111111111111111, 'epoch': 0.89}\n",
      "{'loss': 0.4381, 'grad_norm': 3.0629403591156006, 'learning_rate': 0.00020999999999999998, 'epoch': 0.9}\n",
      "{'loss': 0.4896, 'grad_norm': 0.4290778338909149, 'learning_rate': 0.00020888888888888888, 'epoch': 0.91}\n",
      "{'loss': 0.4294, 'grad_norm': 1.067704439163208, 'learning_rate': 0.00020777777777777776, 'epoch': 0.92}\n",
      "{'loss': 0.4379, 'grad_norm': 0.5653840899467468, 'learning_rate': 0.00020666666666666666, 'epoch': 0.93}\n",
      "{'loss': 0.4228, 'grad_norm': 0.9947461485862732, 'learning_rate': 0.00020555555555555556, 'epoch': 0.94}\n",
      "{'loss': 0.467, 'grad_norm': 1.1120744943618774, 'learning_rate': 0.00020444444444444443, 'epoch': 0.96}\n",
      "{'loss': 0.3953, 'grad_norm': 0.528828501701355, 'learning_rate': 0.00020333333333333333, 'epoch': 0.97}\n",
      "{'loss': 0.4606, 'grad_norm': 0.2659591734409332, 'learning_rate': 0.0002022222222222222, 'epoch': 0.98}\n",
      "{'loss': 0.484, 'grad_norm': 0.7532833218574524, 'learning_rate': 0.0002011111111111111, 'epoch': 0.99}\n",
      "{'loss': 0.424, 'grad_norm': 0.738866925239563, 'learning_rate': 0.00019999999999999998, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e370762caa4143738c6fc10edc980797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44188573956489563, 'eval_bleu': 0.0289, 'eval_chrf': 5.8752, 'eval_gen_len': 17.7136, 'eval_runtime': 626.1055, 'eval_samples_per_second': 7.986, 'eval_steps_per_second': 0.998, 'epoch': 1.0}\n",
      "{'loss': 0.4678, 'grad_norm': 1.1088224649429321, 'learning_rate': 0.00019888888888888888, 'epoch': 1.01}\n",
      "{'loss': 0.4089, 'grad_norm': 1.4486067295074463, 'learning_rate': 0.00019777777777777776, 'epoch': 1.02}\n",
      "{'loss': 0.4101, 'grad_norm': 2.5721092224121094, 'learning_rate': 0.00019666666666666666, 'epoch': 1.03}\n",
      "{'loss': 0.4479, 'grad_norm': 2.835995674133301, 'learning_rate': 0.00019555555555555556, 'epoch': 1.04}\n",
      "{'loss': 0.4837, 'grad_norm': 1.2498879432678223, 'learning_rate': 0.00019444444444444443, 'epoch': 1.06}\n",
      "{'loss': 0.4332, 'grad_norm': 0.8397011756896973, 'learning_rate': 0.00019333333333333333, 'epoch': 1.07}\n",
      "{'loss': 0.4152, 'grad_norm': 0.9923557043075562, 'learning_rate': 0.0001922222222222222, 'epoch': 1.08}\n",
      "{'loss': 0.4028, 'grad_norm': 0.7900404334068298, 'learning_rate': 0.0001911111111111111, 'epoch': 1.09}\n",
      "{'loss': 0.3953, 'grad_norm': 0.939396858215332, 'learning_rate': 0.00018999999999999998, 'epoch': 1.1}\n",
      "{'loss': 0.3956, 'grad_norm': 0.31442686915397644, 'learning_rate': 0.00018888888888888888, 'epoch': 1.11}\n",
      "{'loss': 0.4154, 'grad_norm': 3.267437696456909, 'learning_rate': 0.00018777777777777776, 'epoch': 1.12}\n",
      "{'loss': 0.4234, 'grad_norm': 2.648733139038086, 'learning_rate': 0.00018666666666666666, 'epoch': 1.13}\n",
      "{'loss': 0.4189, 'grad_norm': 3.3892107009887695, 'learning_rate': 0.00018555555555555553, 'epoch': 1.14}\n",
      "{'loss': 0.4222, 'grad_norm': 1.367417812347412, 'learning_rate': 0.00018444444444444443, 'epoch': 1.16}\n",
      "{'loss': 0.4272, 'grad_norm': 0.3995196521282196, 'learning_rate': 0.00018333333333333334, 'epoch': 1.17}\n",
      "{'loss': 0.4663, 'grad_norm': 1.045475721359253, 'learning_rate': 0.0001822222222222222, 'epoch': 1.18}\n",
      "{'loss': 0.3744, 'grad_norm': 1.5102932453155518, 'learning_rate': 0.0001811111111111111, 'epoch': 1.19}\n",
      "{'loss': 0.4329, 'grad_norm': 0.7214946746826172, 'learning_rate': 0.00017999999999999998, 'epoch': 1.2}\n",
      "{'loss': 0.4263, 'grad_norm': 0.9166098237037659, 'learning_rate': 0.00017888888888888889, 'epoch': 1.21}\n",
      "{'loss': 0.4221, 'grad_norm': 0.8436998128890991, 'learning_rate': 0.00017777777777777776, 'epoch': 1.22}\n",
      "{'loss': 0.4173, 'grad_norm': 2.6880104541778564, 'learning_rate': 0.00017666666666666666, 'epoch': 1.23}\n",
      "{'loss': 0.4214, 'grad_norm': 1.0955971479415894, 'learning_rate': 0.00017555555555555553, 'epoch': 1.24}\n",
      "{'loss': 0.4175, 'grad_norm': 0.3656390309333801, 'learning_rate': 0.00017444444444444444, 'epoch': 1.26}\n",
      "{'loss': 0.4544, 'grad_norm': 0.542625904083252, 'learning_rate': 0.0001733333333333333, 'epoch': 1.27}\n",
      "{'loss': 0.4417, 'grad_norm': 0.37180888652801514, 'learning_rate': 0.0001722222222222222, 'epoch': 1.28}\n",
      "{'loss': 0.4129, 'grad_norm': 1.0877777338027954, 'learning_rate': 0.0001711111111111111, 'epoch': 1.29}\n",
      "{'loss': 0.4318, 'grad_norm': 0.5138905644416809, 'learning_rate': 0.00016999999999999999, 'epoch': 1.3}\n",
      "{'loss': 0.4039, 'grad_norm': 0.29385724663734436, 'learning_rate': 0.00016888888888888889, 'epoch': 1.31}\n",
      "{'loss': 0.431, 'grad_norm': 1.3491313457489014, 'learning_rate': 0.00016777777777777776, 'epoch': 1.32}\n",
      "{'loss': 0.425, 'grad_norm': 0.7348652482032776, 'learning_rate': 0.00016666666666666666, 'epoch': 1.33}\n",
      "{'loss': 0.4235, 'grad_norm': 0.7324416041374207, 'learning_rate': 0.00016555555555555554, 'epoch': 1.34}\n",
      "{'loss': 0.4191, 'grad_norm': 1.7418384552001953, 'learning_rate': 0.00016444444444444444, 'epoch': 1.36}\n",
      "{'loss': 0.4613, 'grad_norm': 1.073986291885376, 'learning_rate': 0.0001633333333333333, 'epoch': 1.37}\n",
      "{'loss': 0.401, 'grad_norm': 1.006299614906311, 'learning_rate': 0.0001622222222222222, 'epoch': 1.38}\n",
      "{'loss': 0.4178, 'grad_norm': 0.6739492416381836, 'learning_rate': 0.0001611111111111111, 'epoch': 1.39}\n",
      "{'loss': 0.3604, 'grad_norm': 0.42712706327438354, 'learning_rate': 0.00015999999999999999, 'epoch': 1.4}\n",
      "{'loss': 0.4096, 'grad_norm': 1.235188364982605, 'learning_rate': 0.0001588888888888889, 'epoch': 1.41}\n",
      "{'loss': 0.4461, 'grad_norm': 0.32373568415641785, 'learning_rate': 0.00015777777777777776, 'epoch': 1.42}\n",
      "{'loss': 0.4424, 'grad_norm': 0.5825139284133911, 'learning_rate': 0.00015666666666666666, 'epoch': 1.43}\n",
      "{'loss': 0.4466, 'grad_norm': 0.7025707364082336, 'learning_rate': 0.00015555555555555554, 'epoch': 1.44}\n",
      "{'loss': 0.388, 'grad_norm': 0.6012658476829529, 'learning_rate': 0.00015444444444444444, 'epoch': 1.46}\n",
      "{'loss': 0.4219, 'grad_norm': 2.5865585803985596, 'learning_rate': 0.0001533333333333333, 'epoch': 1.47}\n",
      "{'loss': 0.457, 'grad_norm': 0.4967256188392639, 'learning_rate': 0.0001522222222222222, 'epoch': 1.48}\n",
      "{'loss': 0.4049, 'grad_norm': 0.6254533529281616, 'learning_rate': 0.00015111111111111109, 'epoch': 1.49}\n",
      "{'loss': 0.4382, 'grad_norm': 0.615781307220459, 'learning_rate': 0.00015, 'epoch': 1.5}\n",
      "{'loss': 0.4458, 'grad_norm': 1.5564579963684082, 'learning_rate': 0.00014888888888888886, 'epoch': 1.51}\n",
      "{'loss': 0.4086, 'grad_norm': 0.5870106220245361, 'learning_rate': 0.00014777777777777776, 'epoch': 1.52}\n",
      "{'loss': 0.4092, 'grad_norm': 0.37292271852493286, 'learning_rate': 0.00014666666666666664, 'epoch': 1.53}\n",
      "{'loss': 0.4631, 'grad_norm': 2.907212495803833, 'learning_rate': 0.00014555555555555554, 'epoch': 1.54}\n",
      "{'loss': 0.4379, 'grad_norm': 0.5905381441116333, 'learning_rate': 0.0001444444444444444, 'epoch': 1.56}\n",
      "{'loss': 0.4253, 'grad_norm': 0.5329226851463318, 'learning_rate': 0.00014333333333333334, 'epoch': 1.57}\n",
      "{'loss': 0.4597, 'grad_norm': 0.4976203441619873, 'learning_rate': 0.0001422222222222222, 'epoch': 1.58}\n",
      "{'loss': 0.4442, 'grad_norm': 0.7138796448707581, 'learning_rate': 0.00014111111111111111, 'epoch': 1.59}\n",
      "{'loss': 0.4069, 'grad_norm': 0.7879993915557861, 'learning_rate': 0.00014, 'epoch': 1.6}\n",
      "{'loss': 0.4586, 'grad_norm': 0.43333712220191956, 'learning_rate': 0.0001388888888888889, 'epoch': 1.61}\n",
      "{'loss': 0.4068, 'grad_norm': 1.0893275737762451, 'learning_rate': 0.00013777777777777776, 'epoch': 1.62}\n",
      "{'loss': 0.404, 'grad_norm': 1.4268996715545654, 'learning_rate': 0.00013666666666666666, 'epoch': 1.63}\n",
      "{'loss': 0.3732, 'grad_norm': 1.968685269355774, 'learning_rate': 0.00013555555555555554, 'epoch': 1.64}\n",
      "{'loss': 0.4267, 'grad_norm': 0.6083936095237732, 'learning_rate': 0.00013444444444444444, 'epoch': 1.66}\n",
      "{'loss': 0.3893, 'grad_norm': 0.4889756143093109, 'learning_rate': 0.0001333333333333333, 'epoch': 1.67}\n",
      "{'loss': 0.4221, 'grad_norm': 1.0037579536437988, 'learning_rate': 0.00013222222222222221, 'epoch': 1.68}\n",
      "{'loss': 0.4328, 'grad_norm': 0.5280351042747498, 'learning_rate': 0.00013111111111111111, 'epoch': 1.69}\n",
      "{'loss': 0.4195, 'grad_norm': 1.308287262916565, 'learning_rate': 0.00013, 'epoch': 1.7}\n",
      "{'loss': 0.4201, 'grad_norm': 1.4611812829971313, 'learning_rate': 0.0001288888888888889, 'epoch': 1.71}\n",
      "{'loss': 0.3824, 'grad_norm': 1.335010290145874, 'learning_rate': 0.00012777777777777776, 'epoch': 1.72}\n",
      "{'loss': 0.439, 'grad_norm': 0.5733071565628052, 'learning_rate': 0.00012666666666666666, 'epoch': 1.73}\n",
      "{'loss': 0.4414, 'grad_norm': 0.6747393012046814, 'learning_rate': 0.00012555555555555554, 'epoch': 1.74}\n",
      "{'loss': 0.4144, 'grad_norm': 0.4665201008319855, 'learning_rate': 0.00012444444444444444, 'epoch': 1.76}\n",
      "{'loss': 0.3953, 'grad_norm': 0.5613764524459839, 'learning_rate': 0.0001233333333333333, 'epoch': 1.77}\n",
      "{'loss': 0.384, 'grad_norm': 0.21296094357967377, 'learning_rate': 0.00012222222222222221, 'epoch': 1.78}\n",
      "{'loss': 0.4264, 'grad_norm': 1.2238270044326782, 'learning_rate': 0.0001211111111111111, 'epoch': 1.79}\n",
      "{'loss': 0.3976, 'grad_norm': 0.4583112895488739, 'learning_rate': 0.00011999999999999999, 'epoch': 1.8}\n",
      "{'loss': 0.4295, 'grad_norm': 0.4235183298587799, 'learning_rate': 0.00011888888888888888, 'epoch': 1.81}\n",
      "{'loss': 0.4469, 'grad_norm': 1.5701619386672974, 'learning_rate': 0.00011777777777777776, 'epoch': 1.82}\n",
      "{'loss': 0.432, 'grad_norm': 1.1410330533981323, 'learning_rate': 0.00011666666666666665, 'epoch': 1.83}\n",
      "{'loss': 0.4234, 'grad_norm': 0.7772518992424011, 'learning_rate': 0.00011555555555555554, 'epoch': 1.84}\n",
      "{'loss': 0.4032, 'grad_norm': 2.621457576751709, 'learning_rate': 0.00011444444444444443, 'epoch': 1.86}\n",
      "{'loss': 0.4332, 'grad_norm': 1.8480035066604614, 'learning_rate': 0.00011333333333333331, 'epoch': 1.87}\n",
      "{'loss': 0.3936, 'grad_norm': 0.7359156608581543, 'learning_rate': 0.0001122222222222222, 'epoch': 1.88}\n",
      "{'loss': 0.4533, 'grad_norm': 0.841476321220398, 'learning_rate': 0.00011111111111111109, 'epoch': 1.89}\n",
      "{'loss': 0.4149, 'grad_norm': 0.6003208756446838, 'learning_rate': 0.00010999999999999998, 'epoch': 1.9}\n",
      "{'loss': 0.4254, 'grad_norm': 0.6592839360237122, 'learning_rate': 0.00010888888888888889, 'epoch': 1.91}\n",
      "{'loss': 0.43, 'grad_norm': 0.770951509475708, 'learning_rate': 0.00010777777777777778, 'epoch': 1.92}\n",
      "{'loss': 0.4454, 'grad_norm': 1.9022375345230103, 'learning_rate': 0.00010666666666666667, 'epoch': 1.93}\n",
      "{'loss': 0.4343, 'grad_norm': 0.8224101066589355, 'learning_rate': 0.00010555555555555555, 'epoch': 1.94}\n",
      "{'loss': 0.4248, 'grad_norm': 1.1805545091629028, 'learning_rate': 0.00010444444444444444, 'epoch': 1.96}\n",
      "{'loss': 0.4111, 'grad_norm': 3.8943564891815186, 'learning_rate': 0.00010333333333333333, 'epoch': 1.97}\n",
      "{'loss': 0.4242, 'grad_norm': 0.6031368374824524, 'learning_rate': 0.00010222222222222222, 'epoch': 1.98}\n",
      "{'loss': 0.3965, 'grad_norm': 0.6939055323600769, 'learning_rate': 0.0001011111111111111, 'epoch': 1.99}\n",
      "{'loss': 0.4161, 'grad_norm': 1.0782604217529297, 'learning_rate': 9.999999999999999e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca631c15b3db4613a7398fb8ffb18462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4259887635707855, 'eval_bleu': 0.0308, 'eval_chrf': 5.8944, 'eval_gen_len': 17.3682, 'eval_runtime': 626.3455, 'eval_samples_per_second': 7.983, 'eval_steps_per_second': 0.998, 'epoch': 2.0}\n",
      "{'loss': 0.3554, 'grad_norm': 1.9407774209976196, 'learning_rate': 9.888888888888888e-05, 'epoch': 2.01}\n",
      "{'loss': 0.3943, 'grad_norm': 3.4188263416290283, 'learning_rate': 9.777777777777778e-05, 'epoch': 2.02}\n",
      "{'loss': 0.3784, 'grad_norm': 2.7708675861358643, 'learning_rate': 9.666666666666667e-05, 'epoch': 2.03}\n",
      "{'loss': 0.3642, 'grad_norm': 1.0754587650299072, 'learning_rate': 9.555555555555555e-05, 'epoch': 2.04}\n",
      "{'loss': 0.4215, 'grad_norm': 0.6976504325866699, 'learning_rate': 9.444444444444444e-05, 'epoch': 2.06}\n",
      "{'loss': 0.4266, 'grad_norm': 1.4583238363265991, 'learning_rate': 9.333333333333333e-05, 'epoch': 2.07}\n",
      "{'loss': 0.3756, 'grad_norm': 1.0563803911209106, 'learning_rate': 9.222222222222222e-05, 'epoch': 2.08}\n",
      "{'loss': 0.4135, 'grad_norm': 0.24083465337753296, 'learning_rate': 9.11111111111111e-05, 'epoch': 2.09}\n",
      "{'loss': 0.4126, 'grad_norm': 2.5065865516662598, 'learning_rate': 8.999999999999999e-05, 'epoch': 2.1}\n",
      "{'loss': 0.4394, 'grad_norm': 2.9864604473114014, 'learning_rate': 8.888888888888888e-05, 'epoch': 2.11}\n",
      "{'loss': 0.4326, 'grad_norm': 0.7001160383224487, 'learning_rate': 8.777777777777777e-05, 'epoch': 2.12}\n",
      "{'loss': 0.439, 'grad_norm': 0.6380119323730469, 'learning_rate': 8.666666666666665e-05, 'epoch': 2.13}\n",
      "{'loss': 0.4043, 'grad_norm': 2.583871603012085, 'learning_rate': 8.555555555555556e-05, 'epoch': 2.14}\n",
      "{'loss': 0.4054, 'grad_norm': 1.0574053525924683, 'learning_rate': 8.444444444444444e-05, 'epoch': 2.16}\n",
      "{'loss': 0.3344, 'grad_norm': 0.7625771164894104, 'learning_rate': 8.333333333333333e-05, 'epoch': 2.17}\n",
      "{'loss': 0.4385, 'grad_norm': 1.25551438331604, 'learning_rate': 8.222222222222222e-05, 'epoch': 2.18}\n",
      "{'loss': 0.3902, 'grad_norm': 0.42983052134513855, 'learning_rate': 8.11111111111111e-05, 'epoch': 2.19}\n",
      "{'loss': 0.4019, 'grad_norm': 4.006007671356201, 'learning_rate': 7.999999999999999e-05, 'epoch': 2.2}\n",
      "{'loss': 0.3825, 'grad_norm': 1.1705454587936401, 'learning_rate': 7.888888888888888e-05, 'epoch': 2.21}\n",
      "{'loss': 0.3899, 'grad_norm': 1.7896685600280762, 'learning_rate': 7.777777777777777e-05, 'epoch': 2.22}\n",
      "{'loss': 0.3867, 'grad_norm': 0.7243733406066895, 'learning_rate': 7.666666666666666e-05, 'epoch': 2.23}\n",
      "{'loss': 0.3573, 'grad_norm': 0.9433265328407288, 'learning_rate': 7.555555555555554e-05, 'epoch': 2.24}\n",
      "{'loss': 0.3633, 'grad_norm': 0.3122985064983368, 'learning_rate': 7.444444444444443e-05, 'epoch': 2.26}\n",
      "{'loss': 0.4029, 'grad_norm': 2.421358585357666, 'learning_rate': 7.333333333333332e-05, 'epoch': 2.27}\n",
      "{'loss': 0.3992, 'grad_norm': 0.9304724335670471, 'learning_rate': 7.22222222222222e-05, 'epoch': 2.28}\n",
      "{'loss': 0.4323, 'grad_norm': 1.2505075931549072, 'learning_rate': 7.11111111111111e-05, 'epoch': 2.29}\n",
      "{'loss': 0.3934, 'grad_norm': 0.7111437320709229, 'learning_rate': 7e-05, 'epoch': 2.3}\n",
      "{'loss': 0.4507, 'grad_norm': 0.7676581740379333, 'learning_rate': 6.888888888888888e-05, 'epoch': 2.31}\n",
      "{'loss': 0.369, 'grad_norm': 0.3918127715587616, 'learning_rate': 6.777777777777777e-05, 'epoch': 2.32}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrat-single-context\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     auto_find_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     15\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/accelerate/utils/memory.py:153\u001b[0m, in \u001b[0;36mfind_executable_batch_size.<locals>.decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo executable batch size found, reached zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"rat-multi-context\",\n",
    "    auto_find_batch_size=True,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, #check this\n",
    "    push_to_hub=False,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_with_contexts['train'],\n",
    "    eval_dataset=dataset_with_contexts['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
