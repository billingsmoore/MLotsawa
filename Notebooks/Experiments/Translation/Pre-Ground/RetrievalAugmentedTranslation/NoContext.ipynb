{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('rat-poc-ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': 'འཇིག་ལས་འདས་པའི་གང་འདུལ་ལོ།།',\n",
       " 'Target': 'Taming with transcendent beings.',\n",
       " 'File_Name': 'TM3076',\n",
       " 'Machine Aligned': False,\n",
       " '__index_level_0__': 1176089,\n",
       " 'Tag': 'Intrinsic Existence, Conventional Existence'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': ' དབང་ཤེས་ནི་རྟགས་ལས་དཔག་མི་དགོས་པར་མངོན་སུམ་དུ་ངེས་པའི་ཕྱིར་རོ།།',\n",
       " 'Target': ' Sense cognitions need not infer from signs but can ascertain things directly.',\n",
       " 'File_Name': 'TM0713',\n",
       " 'Machine Aligned': False,\n",
       " '__index_level_0__': 168915,\n",
       " 'Tag': 'Prophecies, Rituals'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer, Model, and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc7139b644d49c696c1c9ad1a6c8bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2797f6a87a6849d8b7faca1dcd862abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc598a0cee25472cafac086ef94556f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f82b16acdd6451493af30f2eb206164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9a4320a4774fac94fb3e1176488b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3986d72119d4fb89ea6d0cb4cb114cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, device_map=\"cuda:0\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32355, 512)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a list of all Tibetan Unicode characters (U+0F00 to U+0FFF)\n",
    "tibetan_chars = [chr(codepoint) for codepoint in range(0x0F00, 0x0FFF)]\n",
    "\n",
    "# Add the Tibetan characters to the tokenizer's vocabulary\n",
    "new_tokens = [char for char in tibetan_chars if char not in tokenizer.get_vocab()]\n",
    "\n",
    "# Add new tokens to the tokenizer\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Resize model embeddings to accommodate the new vocabulary size\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'འཇིག་ལས་འདས་པའི་གང་འདུལ་ལོ།།</s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = tokenizer.encode(dataset['train'][0]['Source'])\n",
    "dec = tokenizer.decode(enc)\n",
    "dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "The dataset can now be tokenized for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = 'Source'\n",
    "target_lang = 'Target'\n",
    "\n",
    "def preprocess_function(examples):\n",
    "\n",
    "    inputs = [example for example in examples[source_lang]]\n",
    "    targets = [example for example in examples[target_lang]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c2a4d554ce48089c5e0aa6bd2ba365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d26e34c7de4247a3c8b60b9b44cd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d46f38ed3a429986c6176ff08d835b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/9.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU and CHRF metrics\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Postprocess text\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_score = bleu_result[\"score\"]\n",
    "\n",
    "    # Compute CHRF score\n",
    "    chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    chrf_score = chrf_result[\"score\"]\n",
    "\n",
    "    # Compute generation length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    avg_gen_len = np.mean(prediction_lens)\n",
    "\n",
    "    # Return rounded results\n",
    "    return {\n",
    "        \"bleu\": round(bleu_score, 4),\n",
    "        \"chrf\": round(chrf_score, 4),\n",
    "        \"gen_len\": round(avg_gen_len, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, Adafactor\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    scale_parameter=True, \n",
    "    relative_step=False, \n",
    "    warmup_init=False, \n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "model, optimizer = accelerator.prepare(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/j/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbillingsmoore\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/j/Documents/MLotsawa/Notebooks/Models/ProofOfConcepts/RAT/wandb/run-20250104_145005-w862nkzu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/billingsmoore/huggingface/runs/w862nkzu' target=\"_blank\">rat-poc-no-context</a></strong> to <a href='https://wandb.ai/billingsmoore/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/billingsmoore/huggingface' target=\"_blank\">https://wandb.ai/billingsmoore/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/billingsmoore/huggingface/runs/w862nkzu' target=\"_blank\">https://wandb.ai/billingsmoore/huggingface/runs/w862nkzu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae23ea7d699427cbc25e3a7c548466d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3069, 'grad_norm': 0.965528130531311, 'learning_rate': 0.0002911111111111111, 'epoch': 0.09}\n",
      "{'loss': 1.0191, 'grad_norm': 0.27212992310523987, 'learning_rate': 0.00028222222222222223, 'epoch': 0.18}\n",
      "{'loss': 0.9983, 'grad_norm': 0.18509145081043243, 'learning_rate': 0.00027333333333333333, 'epoch': 0.27}\n",
      "{'loss': 0.9976, 'grad_norm': 0.33916306495666504, 'learning_rate': 0.00026444444444444443, 'epoch': 0.36}\n",
      "{'loss': 1.0109, 'grad_norm': 0.33182889223098755, 'learning_rate': 0.00025555555555555553, 'epoch': 0.44}\n",
      "{'loss': 1.0051, 'grad_norm': 0.48836204409599304, 'learning_rate': 0.0002466666666666666, 'epoch': 0.53}\n",
      "{'loss': 0.9593, 'grad_norm': 0.3102256655693054, 'learning_rate': 0.00023777777777777775, 'epoch': 0.62}\n",
      "{'loss': 0.973, 'grad_norm': 0.1904025375843048, 'learning_rate': 0.00022888888888888885, 'epoch': 0.71}\n",
      "{'loss': 0.9951, 'grad_norm': 0.5006190538406372, 'learning_rate': 0.00021999999999999995, 'epoch': 0.8}\n",
      "{'loss': 0.9268, 'grad_norm': 0.32680538296699524, 'learning_rate': 0.0002111111111111111, 'epoch': 0.89}\n",
      "{'loss': 0.9618, 'grad_norm': 0.3516484498977661, 'learning_rate': 0.0002022222222222222, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d3a7f9995b4fe0be3bc822bb93421b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8925486207008362, 'eval_bleu': 0.0486, 'eval_chrf': 4.9351, 'eval_gen_len': 18.4756, 'eval_runtime': 105.0071, 'eval_samples_per_second': 47.616, 'eval_steps_per_second': 5.952, 'epoch': 1.0}\n",
      "{'loss': 0.9535, 'grad_norm': 0.3119215965270996, 'learning_rate': 0.00019333333333333333, 'epoch': 1.07}\n",
      "{'loss': 0.9136, 'grad_norm': 0.5302177667617798, 'learning_rate': 0.00018444444444444443, 'epoch': 1.16}\n",
      "{'loss': 0.933, 'grad_norm': 0.4272601008415222, 'learning_rate': 0.00017555555555555553, 'epoch': 1.24}\n",
      "{'loss': 0.9164, 'grad_norm': 0.2883760929107666, 'learning_rate': 0.00016666666666666666, 'epoch': 1.33}\n",
      "{'loss': 0.9557, 'grad_norm': 0.3059743344783783, 'learning_rate': 0.00015777777777777776, 'epoch': 1.42}\n",
      "{'loss': 0.9457, 'grad_norm': 0.3589041233062744, 'learning_rate': 0.00014888888888888886, 'epoch': 1.51}\n",
      "{'loss': 0.943, 'grad_norm': 0.31390276551246643, 'learning_rate': 0.00014, 'epoch': 1.6}\n",
      "{'loss': 0.949, 'grad_norm': 0.36544880270957947, 'learning_rate': 0.00013111111111111111, 'epoch': 1.69}\n",
      "{'loss': 0.9484, 'grad_norm': 0.3174722492694855, 'learning_rate': 0.00012222222222222221, 'epoch': 1.78}\n",
      "{'loss': 0.9458, 'grad_norm': 0.47305819392204285, 'learning_rate': 0.00011333333333333331, 'epoch': 1.87}\n",
      "{'loss': 0.9619, 'grad_norm': 0.3102725148200989, 'learning_rate': 0.00010444444444444444, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9a2cb8ec764ce58e54856981a6201f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8728377223014832, 'eval_bleu': 0.044, 'eval_chrf': 5.2642, 'eval_gen_len': 18.483, 'eval_runtime': 108.0452, 'eval_samples_per_second': 46.277, 'eval_steps_per_second': 5.785, 'epoch': 2.0}\n",
      "{'loss': 0.9175, 'grad_norm': 0.2940704822540283, 'learning_rate': 9.555555555555555e-05, 'epoch': 2.04}\n",
      "{'loss': 0.9474, 'grad_norm': 0.4207407236099243, 'learning_rate': 8.666666666666665e-05, 'epoch': 2.13}\n",
      "{'loss': 0.9303, 'grad_norm': 0.335420697927475, 'learning_rate': 7.777777777777777e-05, 'epoch': 2.22}\n",
      "{'loss': 0.9133, 'grad_norm': 0.5305365920066833, 'learning_rate': 6.888888888888888e-05, 'epoch': 2.31}\n",
      "{'loss': 0.9284, 'grad_norm': 0.39540573954582214, 'learning_rate': 5.9999999999999995e-05, 'epoch': 2.4}\n",
      "{'loss': 0.9084, 'grad_norm': 0.24254003167152405, 'learning_rate': 5.111111111111111e-05, 'epoch': 2.49}\n",
      "{'loss': 0.9081, 'grad_norm': 0.37868040800094604, 'learning_rate': 4.222222222222222e-05, 'epoch': 2.58}\n",
      "{'loss': 0.8959, 'grad_norm': 0.18758004903793335, 'learning_rate': 3.333333333333333e-05, 'epoch': 2.67}\n",
      "{'loss': 0.9181, 'grad_norm': 0.22718282043933868, 'learning_rate': 2.4444444444444445e-05, 'epoch': 2.76}\n",
      "{'loss': 0.9485, 'grad_norm': 0.20096950232982635, 'learning_rate': 1.5555555555555555e-05, 'epoch': 2.84}\n",
      "{'loss': 0.9252, 'grad_norm': 0.44983750581741333, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab97a0f0c7c43728fd43296ef4e54cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8670628666877747, 'eval_bleu': 0.0433, 'eval_chrf': 5.4459, 'eval_gen_len': 18.3346, 'eval_runtime': 108.7933, 'eval_samples_per_second': 45.959, 'eval_steps_per_second': 5.745, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4896.7708, 'train_samples_per_second': 27.569, 'train_steps_per_second': 3.446, 'train_loss': 0.957993236400463, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16875, training_loss=0.957993236400463, metrics={'train_runtime': 4896.7708, 'train_samples_per_second': 27.569, 'train_steps_per_second': 3.446, 'total_flos': 9135571599360000.0, 'train_loss': 0.957993236400463, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"rat-poc-no-context\",\n",
    "    auto_find_batch_size=True,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, #check this\n",
    "    push_to_hub=False,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
