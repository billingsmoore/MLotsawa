{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['bo', 'en', 'source_file'],\n",
       "        num_rows: 122058\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['bo', 'en', 'source_file'],\n",
       "        num_rows: 13562\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Unfinetuned Tokenizer, Model, and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('billingsmoore/getok-v0')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-efficient-tiny\", device_map=\"cuda:0\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "The dataset can now be tokenized for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bo_en_preprocess_function(examples):\n",
    "\n",
    "    # Prepare translation inputs and targets\n",
    "    translation_inputs = ['Translate Tibetan to English: ' + example for example in examples['bo']]\n",
    "    translation_targets = [example for example in examples['en']]\n",
    "    \n",
    "    # Tokenize translation inputs and targets\n",
    "    bo_en_model_inputs = tokenizer(translation_inputs, text_target=translation_targets, \n",
    "                                         max_length=300, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    \n",
    "    return bo_en_model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74870ea08e5c4b9f943b06ce406b3547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/122058 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef188d21586949c880df8cc3b259da04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13562 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(bo_en_preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Finally, we can train the model. Note that the optimizer used is Adafactor. This is the optimizer that is preferred for translation tasks and for the T5 model in general. The transformers api includes a built in version of Adafactor, but I define it separately here so that we can optimize it with the 'accelerate' library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, Adafactor\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    scale_parameter=True, \n",
    "    relative_step=False, \n",
    "    warmup_init=False, \n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "model, optimizer = accelerator.prepare(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU and CHRF metrics\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "ter_metric = evaluate.load(\"ter\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Postprocess text\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_score = bleu_result[\"score\"]\n",
    "\n",
    "    # Compute CHRF score\n",
    "    chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    chrf_score = chrf_result[\"score\"]\n",
    "\n",
    "    # Compute TER score\n",
    "    ter_result = ter_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    ter_score = ter_result[\"score\"]\n",
    "\n",
    "    # Return rounded results\n",
    "    metrics = {\n",
    "        \"bleu\": round(bleu_score, 4),\n",
    "        \"chrf\": round(chrf_score, 4),\n",
    "        \"ter\": round(ter_score, 4)\n",
    "    }\n",
    "\n",
    "    #print(\"Computed Metrics:\", metrics)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbillingsmoore\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5215/4140109040.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/j/Documents/MLotsawa/Notebooks/Experiments/Bidirectional/wandb/run-20250922_200212-a64ahbyq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/billingsmoore/huggingface/runs/a64ahbyq' target=\"_blank\">bidirectional</a></strong> to <a href='https://wandb.ai/billingsmoore/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/billingsmoore/huggingface' target=\"_blank\">https://wandb.ai/billingsmoore/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/billingsmoore/huggingface/runs/a64ahbyq' target=\"_blank\">https://wandb.ai/billingsmoore/huggingface/runs/a64ahbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be36692aaf042ea92cb7eae497d8a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45774 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3098, 'grad_norm': 0.11990063637495041, 'learning_rate': 0.00029672303054135535, 'epoch': 0.03}\n",
      "{'loss': 0.7439, 'grad_norm': 0.10276002436876297, 'learning_rate': 0.0002934460610827107, 'epoch': 0.07}\n",
      "{'loss': 0.6824, 'grad_norm': 0.07932163029909134, 'learning_rate': 0.00029016909162406603, 'epoch': 0.1}\n",
      "{'loss': 0.666, 'grad_norm': 0.0764455795288086, 'learning_rate': 0.0002868921221654214, 'epoch': 0.13}\n",
      "{'loss': 0.6616, 'grad_norm': 0.09991148114204407, 'learning_rate': 0.0002836151527067767, 'epoch': 0.16}\n",
      "{'loss': 0.6315, 'grad_norm': 0.29187387228012085, 'learning_rate': 0.0002803381832481321, 'epoch': 0.2}\n",
      "{'loss': 0.6206, 'grad_norm': 0.16956818103790283, 'learning_rate': 0.00027706121378948747, 'epoch': 0.23}\n",
      "{'loss': 0.6217, 'grad_norm': 0.14320552349090576, 'learning_rate': 0.00027378424433084284, 'epoch': 0.26}\n",
      "{'loss': 0.62, 'grad_norm': 0.05835013836622238, 'learning_rate': 0.00027050727487219815, 'epoch': 0.29}\n",
      "{'loss': 0.5842, 'grad_norm': 0.21402224898338318, 'learning_rate': 0.0002672303054135535, 'epoch': 0.33}\n",
      "{'loss': 0.5822, 'grad_norm': 0.35514751076698303, 'learning_rate': 0.00026395333595490884, 'epoch': 0.36}\n",
      "{'loss': 0.5677, 'grad_norm': 0.2954308092594147, 'learning_rate': 0.0002606763664962642, 'epoch': 0.39}\n",
      "{'loss': 0.5832, 'grad_norm': 0.0739196166396141, 'learning_rate': 0.0002573993970376196, 'epoch': 0.43}\n",
      "{'loss': 0.5866, 'grad_norm': 0.3476189076900482, 'learning_rate': 0.00025412242757897496, 'epoch': 0.46}\n",
      "{'loss': 0.5634, 'grad_norm': 0.13008780777454376, 'learning_rate': 0.0002508454581203303, 'epoch': 0.49}\n",
      "{'loss': 0.5424, 'grad_norm': 0.14379292726516724, 'learning_rate': 0.00024756848866168565, 'epoch': 0.52}\n",
      "{'loss': 0.5345, 'grad_norm': 0.08666198700666428, 'learning_rate': 0.000244291519203041, 'epoch': 0.56}\n",
      "{'loss': 0.53, 'grad_norm': 0.4201622009277344, 'learning_rate': 0.00024101454974439633, 'epoch': 0.59}\n",
      "{'loss': 0.5354, 'grad_norm': 0.07170198112726212, 'learning_rate': 0.0002377375802857517, 'epoch': 0.62}\n",
      "{'loss': 0.5276, 'grad_norm': 0.09922764450311661, 'learning_rate': 0.00023446061082710708, 'epoch': 0.66}\n",
      "{'loss': 0.5353, 'grad_norm': 0.12693026661872864, 'learning_rate': 0.00023118364136846242, 'epoch': 0.69}\n",
      "{'loss': 0.5213, 'grad_norm': 0.09149295836687088, 'learning_rate': 0.0002279066719098178, 'epoch': 0.72}\n",
      "{'loss': 0.5188, 'grad_norm': 0.28058338165283203, 'learning_rate': 0.00022462970245117314, 'epoch': 0.75}\n",
      "{'loss': 0.5196, 'grad_norm': 0.12819397449493408, 'learning_rate': 0.0002213527329925285, 'epoch': 0.79}\n",
      "{'loss': 0.517, 'grad_norm': 0.490683913230896, 'learning_rate': 0.00021807576353388383, 'epoch': 0.82}\n",
      "{'loss': 0.5153, 'grad_norm': 0.171138197183609, 'learning_rate': 0.0002147987940752392, 'epoch': 0.85}\n",
      "{'loss': 0.5126, 'grad_norm': 0.10621077567338943, 'learning_rate': 0.00021152182461659454, 'epoch': 0.88}\n",
      "{'loss': 0.4924, 'grad_norm': 0.1294487565755844, 'learning_rate': 0.0002082448551579499, 'epoch': 0.92}\n",
      "{'loss': 0.5049, 'grad_norm': 0.21499058604240417, 'learning_rate': 0.00020496788569930526, 'epoch': 0.95}\n",
      "{'loss': 0.5161, 'grad_norm': 0.14562080800533295, 'learning_rate': 0.00020169091624066063, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78bc978c6d24c83b0ca34cb5d6cd6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1696 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46067285537719727, 'eval_bleu': 5.4703, 'eval_chrf': 15.7753, 'eval_ter': 100.0108, 'eval_runtime': 177.0661, 'eval_samples_per_second': 76.593, 'eval_steps_per_second': 9.578, 'epoch': 1.0}\n",
      "{'loss': 0.49, 'grad_norm': 0.1757616400718689, 'learning_rate': 0.000198413946782016, 'epoch': 1.02}\n",
      "{'loss': 0.4969, 'grad_norm': 0.08000422269105911, 'learning_rate': 0.00019513697732337132, 'epoch': 1.05}\n",
      "{'loss': 0.4991, 'grad_norm': 0.12460128962993622, 'learning_rate': 0.00019186000786472666, 'epoch': 1.08}\n",
      "{'loss': 0.4881, 'grad_norm': 0.09993338584899902, 'learning_rate': 0.00018858303840608203, 'epoch': 1.11}\n",
      "{'loss': 0.4846, 'grad_norm': 0.09094216674566269, 'learning_rate': 0.0001853060689474374, 'epoch': 1.15}\n",
      "{'loss': 0.4845, 'grad_norm': 0.25737836956977844, 'learning_rate': 0.00018202909948879275, 'epoch': 1.18}\n",
      "{'loss': 0.474, 'grad_norm': 0.46137696504592896, 'learning_rate': 0.00017875213003014812, 'epoch': 1.21}\n",
      "{'loss': 0.469, 'grad_norm': 0.10449015349149704, 'learning_rate': 0.00017547516057150346, 'epoch': 1.25}\n",
      "{'loss': 0.4703, 'grad_norm': 0.10654912143945694, 'learning_rate': 0.0001721981911128588, 'epoch': 1.28}\n",
      "{'loss': 0.4751, 'grad_norm': 0.11682230234146118, 'learning_rate': 0.00016892122165421415, 'epoch': 1.31}\n",
      "{'loss': 0.4842, 'grad_norm': 0.6449070572853088, 'learning_rate': 0.00016564425219556952, 'epoch': 1.34}\n",
      "{'loss': 0.4898, 'grad_norm': 0.23854853212833405, 'learning_rate': 0.00016236728273692487, 'epoch': 1.38}\n",
      "{'loss': 0.4792, 'grad_norm': 0.1079496517777443, 'learning_rate': 0.00015909031327828024, 'epoch': 1.41}\n",
      "{'loss': 0.4707, 'grad_norm': 0.1537863314151764, 'learning_rate': 0.00015581334381963558, 'epoch': 1.44}\n",
      "{'loss': 0.4527, 'grad_norm': 0.1153273656964302, 'learning_rate': 0.00015253637436099096, 'epoch': 1.47}\n",
      "{'loss': 0.47, 'grad_norm': 0.12549637258052826, 'learning_rate': 0.0001492594049023463, 'epoch': 1.51}\n",
      "{'loss': 0.4778, 'grad_norm': 0.14110304415225983, 'learning_rate': 0.00014598243544370164, 'epoch': 1.54}\n",
      "{'loss': 0.4483, 'grad_norm': 0.09372760355472565, 'learning_rate': 0.000142705465985057, 'epoch': 1.57}\n",
      "{'loss': 0.4497, 'grad_norm': 0.08315800130367279, 'learning_rate': 0.00013942849652641236, 'epoch': 1.61}\n",
      "{'loss': 0.4597, 'grad_norm': 0.14127099514007568, 'learning_rate': 0.00013615152706776773, 'epoch': 1.64}\n",
      "{'loss': 0.4494, 'grad_norm': 0.10125923901796341, 'learning_rate': 0.00013287455760912308, 'epoch': 1.67}\n",
      "{'loss': 0.4584, 'grad_norm': 0.22344113886356354, 'learning_rate': 0.00012959758815047842, 'epoch': 1.7}\n",
      "{'loss': 0.4663, 'grad_norm': 0.18596713244915009, 'learning_rate': 0.0001263206186918338, 'epoch': 1.74}\n",
      "{'loss': 0.4528, 'grad_norm': 0.17946083843708038, 'learning_rate': 0.00012304364923318914, 'epoch': 1.77}\n",
      "{'loss': 0.4266, 'grad_norm': 0.1312120407819748, 'learning_rate': 0.00011976667977454448, 'epoch': 1.8}\n",
      "{'loss': 0.445, 'grad_norm': 0.12777182459831238, 'learning_rate': 0.00011648971031589984, 'epoch': 1.84}\n",
      "{'loss': 0.4539, 'grad_norm': 0.14150072634220123, 'learning_rate': 0.0001132127408572552, 'epoch': 1.87}\n",
      "{'loss': 0.4413, 'grad_norm': 0.14981280267238617, 'learning_rate': 0.00010993577139861057, 'epoch': 1.9}\n",
      "{'loss': 0.453, 'grad_norm': 0.1601843684911728, 'learning_rate': 0.0001066588019399659, 'epoch': 1.93}\n",
      "{'loss': 0.4479, 'grad_norm': 0.17449656128883362, 'learning_rate': 0.00010338183248132127, 'epoch': 1.97}\n",
      "{'loss': 0.4505, 'grad_norm': 0.24314357340335846, 'learning_rate': 0.00010010486302267663, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58433373a4364118b14b515f871e6ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1696 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4104212522506714, 'eval_bleu': 7.3623, 'eval_chrf': 17.6914, 'eval_ter': 97.9027, 'eval_runtime': 155.4546, 'eval_samples_per_second': 87.241, 'eval_steps_per_second': 10.91, 'epoch': 2.0}\n",
      "{'loss': 0.4509, 'grad_norm': 0.2191893458366394, 'learning_rate': 9.682789356403198e-05, 'epoch': 2.03}\n",
      "{'loss': 0.4575, 'grad_norm': 0.17363891005516052, 'learning_rate': 9.355092410538733e-05, 'epoch': 2.06}\n",
      "{'loss': 0.4233, 'grad_norm': 0.12224459648132324, 'learning_rate': 9.027395464674269e-05, 'epoch': 2.1}\n",
      "{'loss': 0.4338, 'grad_norm': 0.09863822162151337, 'learning_rate': 8.699698518809804e-05, 'epoch': 2.13}\n",
      "{'loss': 0.4413, 'grad_norm': 0.09346724301576614, 'learning_rate': 8.372001572945339e-05, 'epoch': 2.16}\n",
      "{'loss': 0.4491, 'grad_norm': 0.23711109161376953, 'learning_rate': 8.044304627080875e-05, 'epoch': 2.2}\n",
      "{'loss': 0.4558, 'grad_norm': 0.07281932234764099, 'learning_rate': 7.71660768121641e-05, 'epoch': 2.23}\n",
      "{'loss': 0.4388, 'grad_norm': 0.14305223524570465, 'learning_rate': 7.388910735351946e-05, 'epoch': 2.26}\n",
      "{'loss': 0.4508, 'grad_norm': 0.11409971863031387, 'learning_rate': 7.061213789487481e-05, 'epoch': 2.29}\n",
      "{'loss': 0.4315, 'grad_norm': 0.23285327851772308, 'learning_rate': 6.733516843623016e-05, 'epoch': 2.33}\n",
      "{'loss': 0.4533, 'grad_norm': 0.20203419029712677, 'learning_rate': 6.405819897758552e-05, 'epoch': 2.36}\n",
      "{'loss': 0.4381, 'grad_norm': 0.1296616941690445, 'learning_rate': 6.078122951894088e-05, 'epoch': 2.39}\n",
      "{'loss': 0.4395, 'grad_norm': 0.13700135052204132, 'learning_rate': 5.750426006029623e-05, 'epoch': 2.42}\n",
      "{'loss': 0.4124, 'grad_norm': 0.1330510377883911, 'learning_rate': 5.422729060165159e-05, 'epoch': 2.46}\n",
      "{'loss': 0.4404, 'grad_norm': 0.22396348416805267, 'learning_rate': 5.095032114300694e-05, 'epoch': 2.49}\n",
      "{'loss': 0.4279, 'grad_norm': 0.12622760236263275, 'learning_rate': 4.76733516843623e-05, 'epoch': 2.52}\n",
      "{'loss': 0.432, 'grad_norm': 0.10445460677146912, 'learning_rate': 4.439638222571765e-05, 'epoch': 2.56}\n",
      "{'loss': 0.413, 'grad_norm': 0.1267005205154419, 'learning_rate': 4.1119412767073e-05, 'epoch': 2.59}\n",
      "{'loss': 0.436, 'grad_norm': 0.09885681420564651, 'learning_rate': 3.7842443308428365e-05, 'epoch': 2.62}\n",
      "{'loss': 0.4536, 'grad_norm': 0.7923817038536072, 'learning_rate': 3.4565473849783716e-05, 'epoch': 2.65}\n",
      "{'loss': 0.4278, 'grad_norm': 0.10296370089054108, 'learning_rate': 3.128850439113907e-05, 'epoch': 2.69}\n",
      "{'loss': 0.4198, 'grad_norm': 0.10010142624378204, 'learning_rate': 2.8011534932494425e-05, 'epoch': 2.72}\n",
      "{'loss': 0.4323, 'grad_norm': 0.26075461506843567, 'learning_rate': 2.473456547384978e-05, 'epoch': 2.75}\n",
      "{'loss': 0.4262, 'grad_norm': 0.11305622011423111, 'learning_rate': 2.1457596015205138e-05, 'epoch': 2.79}\n",
      "{'loss': 0.4241, 'grad_norm': 0.1240561306476593, 'learning_rate': 1.8180626556560492e-05, 'epoch': 2.82}\n",
      "{'loss': 0.4389, 'grad_norm': 0.10263541340827942, 'learning_rate': 1.4903657097915847e-05, 'epoch': 2.85}\n",
      "{'loss': 0.4254, 'grad_norm': 0.19341513514518738, 'learning_rate': 1.1626687639271201e-05, 'epoch': 2.88}\n",
      "{'loss': 0.432, 'grad_norm': 0.1693718135356903, 'learning_rate': 8.349718180626556e-06, 'epoch': 2.92}\n",
      "{'loss': 0.4238, 'grad_norm': 0.35743287205696106, 'learning_rate': 5.072748721981911e-06, 'epoch': 2.95}\n",
      "{'loss': 0.4344, 'grad_norm': 0.10825999826192856, 'learning_rate': 1.7957792633372655e-06, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059179974ff64a73a6d594c50b6f43d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1696 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3972046375274658, 'eval_bleu': 8.1267, 'eval_chrf': 18.7476, 'eval_ter': 97.4764, 'eval_runtime': 155.6257, 'eval_samples_per_second': 87.145, 'eval_steps_per_second': 10.898, 'epoch': 3.0}\n",
      "{'train_runtime': 6536.421, 'train_samples_per_second': 56.021, 'train_steps_per_second': 7.003, 'train_loss': 0.4981749754576642, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45774, training_loss=0.4981749754576642, metrics={'train_runtime': 6536.421, 'train_samples_per_second': 56.021, 'train_steps_per_second': 7.003, 'total_flos': 4841624105164800.0, 'train_loss': 0.4981749754576642, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"bidirectional\",\n",
    "    auto_find_batch_size=True,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, #check this\n",
    "    push_to_hub=False,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
