{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bo': 'བཅོམ་ལྡན་འདས་ཅི་དེ་བཞིན་གཤེགས་པ་དགྲ་བཅོམ་པ་ཡང་དག་པར་རྫོགས་པའི་སངས་རྒྱས་རྣམས་ཡོན་ཏན་དཔག་ཏུ་མ་མཆིས་པ་དང་ལྡན་ལགས་སམ། དེ་སྐད་ཅེས་གསོལ་པ་དང་།',\n",
       " 'en': 'Blessed One, do thus-gone, worthy, perfectly complete buddhas possess limitless good qualities?”',\n",
       " 'source_file': 'toh126'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "dataset = load_dataset('billingsmoore/84000-bo-en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Unfinetuned Tokenizer, Model, and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c8eef53c42449da08ff210850a7b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/628 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75671e8823c40519d5aee30acbdd7bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/62.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n\u001b[32m      3\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m'\u001b[39m\u001b[33mbillingsmoore/getok-v0\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = \u001b[43mAutoModelForSeq2SeqLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle/t5-efficient-tiny\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda:0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    563\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m564\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    568\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    570\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4224\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4214\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4215\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4217\u001b[39m     (\n\u001b[32m   4218\u001b[39m         model,\n\u001b[32m   4219\u001b[39m         missing_keys,\n\u001b[32m   4220\u001b[39m         unexpected_keys,\n\u001b[32m   4221\u001b[39m         mismatched_keys,\n\u001b[32m   4222\u001b[39m         offload_index,\n\u001b[32m   4223\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4224\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[32m   4228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4231\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4232\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4235\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4244\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   4245\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4727\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[39m\n\u001b[32m   4723\u001b[39m                 set_module_tensor_to_device(\n\u001b[32m   4724\u001b[39m                     model_to_load, key, \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, torch.empty(*param.size(), dtype=dtype)\n\u001b[32m   4725\u001b[39m                 )\n\u001b[32m   4726\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4727\u001b[39m         new_error_msgs, offload_index, state_dict_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4728\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4729\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4730\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4731\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4732\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4733\u001b[39m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4734\u001b[39m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4735\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4736\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4737\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4738\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4739\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4740\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4741\u001b[39m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4742\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4743\u001b[39m         error_msgs += new_error_msgs\n\u001b[32m   4744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4745\u001b[39m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:992\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[39m\n\u001b[32m    989\u001b[39m         param_device = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    991\u001b[39m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m992\u001b[39m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    994\u001b[39m     hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/accelerate/utils/modeling.py:337\u001b[39m, in \u001b[36mset_module_tensor_to_device\u001b[39m\u001b[34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[39m\n\u001b[32m    335\u001b[39m             module._parameters[tensor_name] = param_cls(new_value, requires_grad=old_value.requires_grad)\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     new_value = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     new_value = torch.tensor(value, device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:372\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m    371\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mLAZY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[32m    376\u001b[39m _tls.is_initializing = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('billingsmoore/getok-v0')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/t5-efficient-tiny\", device_map=\"cuda:0\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "The dataset can now be tokenized for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bo_en_preprocess_function(examples):\n",
    "\n",
    "    # Prepare translation inputs and targets\n",
    "    translation_inputs = ['Translate Tibetan to English: ' + example for example in examples['tibetan']]\n",
    "    translation_targets = [example for example in examples['english']]\n",
    "    \n",
    "    # Tokenize translation inputs and targets\n",
    "    bo_en_model_inputs = tokenizer(translation_inputs, text_target=translation_targets, \n",
    "                                         max_length=300, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    \n",
    "    return bo_en_model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_bo_preprocess_function(examples):\n",
    "\n",
    "    # Prepare translation inputs and targets\n",
    "    translation_inputs = ['Translate English to Tibetan: ' + example for example in examples['english']]\n",
    "    translation_targets = [example for example in examples['tibetan']]\n",
    "    \n",
    "    # Tokenize translation inputs and targets\n",
    "    en_bo_model_inputs = tokenizer(translation_inputs, text_target=translation_targets, \n",
    "                                         max_length=300, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    \n",
    "    return en_bo_model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028e3dc52364489e846797eb96a78987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40eaa9f95db34cdf9ec23bb42791e384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12184 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bo_en_tokenized_dataset = dataset.map(bo_en_preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a6cd5f77c64eefb90ec849905aa39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9526b3670c41869bed666295124efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12184 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_bo_tokenized_dataset = dataset.map(en_bo_preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = {}\n",
    "\n",
    "tokenized_dataset['train'] = concatenate_datasets([bo_en_tokenized_dataset['train'], en_bo_tokenized_dataset['train']])\n",
    "tokenized_dataset['test'] = bo_en_tokenized_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Finally, we can train the model. Note that the optimizer used is Adafactor. This is the optimizer that is preferred for translation tasks and for the T5 model in general. The transformers api includes a built in version of Adafactor, but I define it separately here so that we can optimize it with the 'accelerate' library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, Adafactor\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    scale_parameter=True, \n",
    "    relative_step=False, \n",
    "    warmup_init=False, \n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "model, optimizer = accelerator.prepare(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbillingsmoore\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/j/Desktop/MLotsawa/Notebooks/Models/BidirectionalTest/wandb/run-20241012_175622-u509ue1f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/billingsmoore/huggingface/runs/u509ue1f' target=\"_blank\">bidirectional</a></strong> to <a href='https://wandb.ai/billingsmoore/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/billingsmoore/huggingface' target=\"_blank\">https://wandb.ai/billingsmoore/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/billingsmoore/huggingface/runs/u509ue1f' target=\"_blank\">https://wandb.ai/billingsmoore/huggingface/runs/u509ue1f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b2895632d6489b94fa44e6fdff2b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51783 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6332, 'grad_norm': 0.17786581814289093, 'learning_rate': 0.0002971032964486414, 'epoch': 0.03}\n",
      "{'loss': 0.299, 'grad_norm': 0.12014766037464142, 'learning_rate': 0.00029420659289728285, 'epoch': 0.06}\n",
      "{'loss': 0.2795, 'grad_norm': 0.16227394342422485, 'learning_rate': 0.0002913098893459243, 'epoch': 0.09}\n",
      "{'loss': 0.2722, 'grad_norm': 0.21368838846683502, 'learning_rate': 0.00028841318579456577, 'epoch': 0.12}\n",
      "{'loss': 0.2595, 'grad_norm': 0.14164778590202332, 'learning_rate': 0.0002855164822432072, 'epoch': 0.14}\n",
      "{'loss': 0.2529, 'grad_norm': 0.1506086140871048, 'learning_rate': 0.00028261977869184864, 'epoch': 0.17}\n",
      "{'loss': 0.2533, 'grad_norm': 0.14093157649040222, 'learning_rate': 0.0002797230751404901, 'epoch': 0.2}\n",
      "{'loss': 0.2459, 'grad_norm': 0.13410639762878418, 'learning_rate': 0.0002768263715891315, 'epoch': 0.23}\n",
      "{'loss': 0.2392, 'grad_norm': 0.16665083169937134, 'learning_rate': 0.00027392966803777295, 'epoch': 0.26}\n",
      "{'loss': 0.2356, 'grad_norm': 0.12141254544258118, 'learning_rate': 0.00027103296448641444, 'epoch': 0.29}\n",
      "{'loss': 0.237, 'grad_norm': 0.14494578540325165, 'learning_rate': 0.0002681362609350559, 'epoch': 0.32}\n",
      "{'loss': 0.2297, 'grad_norm': 0.13729073107242584, 'learning_rate': 0.0002652395573836973, 'epoch': 0.35}\n",
      "{'loss': 0.2262, 'grad_norm': 0.13771331310272217, 'learning_rate': 0.00026234285383233875, 'epoch': 0.38}\n",
      "{'loss': 0.2256, 'grad_norm': 0.11324585974216461, 'learning_rate': 0.0002594461502809802, 'epoch': 0.41}\n",
      "{'loss': 0.2247, 'grad_norm': 0.19301949441432953, 'learning_rate': 0.0002565494467296217, 'epoch': 0.43}\n",
      "{'loss': 0.2192, 'grad_norm': 0.11749507486820221, 'learning_rate': 0.0002536527431782631, 'epoch': 0.46}\n",
      "{'loss': 0.2194, 'grad_norm': 0.23562310636043549, 'learning_rate': 0.00025075603962690455, 'epoch': 0.49}\n",
      "{'loss': 0.2129, 'grad_norm': 0.12827111780643463, 'learning_rate': 0.000247859336075546, 'epoch': 0.52}\n",
      "{'loss': 0.2147, 'grad_norm': 0.10488127171993256, 'learning_rate': 0.0002449626325241875, 'epoch': 0.55}\n",
      "{'loss': 0.2082, 'grad_norm': 0.09931923449039459, 'learning_rate': 0.00024206592897282889, 'epoch': 0.58}\n",
      "{'loss': 0.206, 'grad_norm': 0.19773146510124207, 'learning_rate': 0.00023916922542147032, 'epoch': 0.61}\n",
      "{'loss': 0.2072, 'grad_norm': 0.12132272869348526, 'learning_rate': 0.0002362725218701118, 'epoch': 0.64}\n",
      "{'loss': 0.2059, 'grad_norm': 0.17182865738868713, 'learning_rate': 0.00023337581831875325, 'epoch': 0.67}\n",
      "{'loss': 0.2057, 'grad_norm': 0.18233232200145721, 'learning_rate': 0.00023047911476739468, 'epoch': 0.7}\n",
      "{'loss': 0.199, 'grad_norm': 0.19675636291503906, 'learning_rate': 0.00022758241121603612, 'epoch': 0.72}\n",
      "{'loss': 0.204, 'grad_norm': 0.15161052346229553, 'learning_rate': 0.00022468570766467758, 'epoch': 0.75}\n",
      "{'loss': 0.2013, 'grad_norm': 0.1458878070116043, 'learning_rate': 0.00022178900411331902, 'epoch': 0.78}\n",
      "{'loss': 0.1948, 'grad_norm': 0.4383593797683716, 'learning_rate': 0.00021889230056196046, 'epoch': 0.81}\n",
      "{'loss': 0.1943, 'grad_norm': 0.15126468241214752, 'learning_rate': 0.00021599559701060192, 'epoch': 0.84}\n",
      "{'loss': 0.1931, 'grad_norm': 0.13386555016040802, 'learning_rate': 0.00021309889345924338, 'epoch': 0.87}\n",
      "{'loss': 0.1935, 'grad_norm': 0.14189554750919342, 'learning_rate': 0.00021020218990788482, 'epoch': 0.9}\n",
      "{'loss': 0.192, 'grad_norm': 0.13429906964302063, 'learning_rate': 0.00020730548635652625, 'epoch': 0.93}\n",
      "{'loss': 0.1916, 'grad_norm': 0.11079487949609756, 'learning_rate': 0.0002044087828051677, 'epoch': 0.96}\n",
      "{'loss': 0.1902, 'grad_norm': 0.15575429797172546, 'learning_rate': 0.00020151207925380913, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Desktop/MLotsawa/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1aac7bfbe84552bcbc4b6df0d482f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1523 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14627183973789215, 'eval_bleu': 3.6518, 'eval_gen_len': 16.1257, 'eval_runtime': 414.8777, 'eval_samples_per_second': 29.368, 'eval_steps_per_second': 3.671, 'epoch': 1.0}\n",
      "{'loss': 0.1894, 'grad_norm': 0.13471776247024536, 'learning_rate': 0.00019861537570245062, 'epoch': 1.01}\n",
      "{'loss': 0.1862, 'grad_norm': 0.1104794517159462, 'learning_rate': 0.00019571867215109205, 'epoch': 1.04}\n",
      "{'loss': 0.1858, 'grad_norm': 0.15300562977790833, 'learning_rate': 0.0001928219685997335, 'epoch': 1.07}\n",
      "{'loss': 0.1839, 'grad_norm': 0.13286134600639343, 'learning_rate': 0.00018992526504837493, 'epoch': 1.1}\n",
      "{'loss': 0.1819, 'grad_norm': 0.1635775864124298, 'learning_rate': 0.00018702856149701636, 'epoch': 1.13}\n",
      "{'loss': 0.1857, 'grad_norm': 0.11667294055223465, 'learning_rate': 0.00018413185794565783, 'epoch': 1.16}\n",
      "{'loss': 0.1816, 'grad_norm': 0.18541409075260162, 'learning_rate': 0.0001812351543942993, 'epoch': 1.19}\n",
      "{'loss': 0.1787, 'grad_norm': 0.1281498521566391, 'learning_rate': 0.00017833845084294072, 'epoch': 1.22}\n",
      "{'loss': 0.1776, 'grad_norm': 0.1520354300737381, 'learning_rate': 0.00017544174729158216, 'epoch': 1.25}\n",
      "{'loss': 0.1782, 'grad_norm': 0.1436372548341751, 'learning_rate': 0.0001725450437402236, 'epoch': 1.27}\n",
      "{'loss': 0.1783, 'grad_norm': 0.10992834717035294, 'learning_rate': 0.00016964834018886506, 'epoch': 1.3}\n",
      "{'loss': 0.1794, 'grad_norm': 0.14493317902088165, 'learning_rate': 0.0001667516366375065, 'epoch': 1.33}\n",
      "{'loss': 0.1762, 'grad_norm': 0.14009808003902435, 'learning_rate': 0.00016385493308614796, 'epoch': 1.36}\n",
      "{'loss': 0.179, 'grad_norm': 0.20203466713428497, 'learning_rate': 0.0001609582295347894, 'epoch': 1.39}\n",
      "{'loss': 0.1747, 'grad_norm': 0.15068554878234863, 'learning_rate': 0.00015806152598343086, 'epoch': 1.42}\n",
      "{'loss': 0.1757, 'grad_norm': 0.1184670627117157, 'learning_rate': 0.0001551648224320723, 'epoch': 1.45}\n",
      "{'loss': 0.1707, 'grad_norm': 0.13226626813411713, 'learning_rate': 0.00015226811888071373, 'epoch': 1.48}\n",
      "{'loss': 0.1784, 'grad_norm': 0.15169791877269745, 'learning_rate': 0.0001493714153293552, 'epoch': 1.51}\n",
      "{'loss': 0.172, 'grad_norm': 0.10360972583293915, 'learning_rate': 0.00014647471177799663, 'epoch': 1.54}\n",
      "{'loss': 0.1751, 'grad_norm': 0.15313060581684113, 'learning_rate': 0.00014357800822663807, 'epoch': 1.56}\n",
      "{'loss': 0.1753, 'grad_norm': 0.18761515617370605, 'learning_rate': 0.00014068130467527953, 'epoch': 1.59}\n",
      "{'loss': 0.1703, 'grad_norm': 0.1436864733695984, 'learning_rate': 0.00013778460112392097, 'epoch': 1.62}\n",
      "{'loss': 0.1709, 'grad_norm': 0.13446404039859772, 'learning_rate': 0.0001348878975725624, 'epoch': 1.65}\n",
      "{'loss': 0.1725, 'grad_norm': 0.1563652753829956, 'learning_rate': 0.00013199119402120387, 'epoch': 1.68}\n",
      "{'loss': 0.1712, 'grad_norm': 0.13539506494998932, 'learning_rate': 0.0001290944904698453, 'epoch': 1.71}\n",
      "{'loss': 0.1668, 'grad_norm': 0.1566898673772812, 'learning_rate': 0.00012619778691848674, 'epoch': 1.74}\n",
      "{'loss': 0.1716, 'grad_norm': 0.2360876202583313, 'learning_rate': 0.0001233010833671282, 'epoch': 1.77}\n",
      "{'loss': 0.1682, 'grad_norm': 0.11532856523990631, 'learning_rate': 0.00012040437981576965, 'epoch': 1.8}\n",
      "{'loss': 0.1644, 'grad_norm': 0.17140084505081177, 'learning_rate': 0.00011750767626441109, 'epoch': 1.82}\n",
      "{'loss': 0.1699, 'grad_norm': 0.1296064853668213, 'learning_rate': 0.00011461097271305252, 'epoch': 1.85}\n",
      "{'loss': 0.1663, 'grad_norm': 0.16760723292827606, 'learning_rate': 0.00011171426916169399, 'epoch': 1.88}\n",
      "{'loss': 0.1663, 'grad_norm': 0.1417946070432663, 'learning_rate': 0.00010881756561033542, 'epoch': 1.91}\n",
      "{'loss': 0.1659, 'grad_norm': 0.14406202733516693, 'learning_rate': 0.00010592086205897687, 'epoch': 1.94}\n",
      "{'loss': 0.1651, 'grad_norm': 0.18205967545509338, 'learning_rate': 0.00010302415850761832, 'epoch': 1.97}\n",
      "{'loss': 0.1675, 'grad_norm': 0.17796412110328674, 'learning_rate': 0.00010012745495625977, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Desktop/MLotsawa/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e333f05f4d49c7a7f23ee93078e8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1523 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1341230273246765, 'eval_bleu': 4.6624, 'eval_gen_len': 16.1466, 'eval_runtime': 422.1308, 'eval_samples_per_second': 28.863, 'eval_steps_per_second': 3.608, 'epoch': 2.0}\n",
      "{'loss': 0.1629, 'grad_norm': 0.18075750768184662, 'learning_rate': 9.723075140490121e-05, 'epoch': 2.03}\n",
      "{'loss': 0.1627, 'grad_norm': 0.29260414838790894, 'learning_rate': 9.433404785354267e-05, 'epoch': 2.06}\n",
      "{'loss': 0.1626, 'grad_norm': 0.1274515837430954, 'learning_rate': 9.14373443021841e-05, 'epoch': 2.09}\n",
      "{'loss': 0.1629, 'grad_norm': 0.16370031237602234, 'learning_rate': 8.854064075082554e-05, 'epoch': 2.11}\n",
      "{'loss': 0.164, 'grad_norm': 0.1507793813943863, 'learning_rate': 8.5643937199467e-05, 'epoch': 2.14}\n",
      "{'loss': 0.1666, 'grad_norm': 0.11839388310909271, 'learning_rate': 8.274723364810844e-05, 'epoch': 2.17}\n",
      "{'loss': 0.1612, 'grad_norm': 0.15337583422660828, 'learning_rate': 7.985053009674989e-05, 'epoch': 2.2}\n",
      "{'loss': 0.1639, 'grad_norm': 0.2055007815361023, 'learning_rate': 7.695382654539134e-05, 'epoch': 2.23}\n",
      "{'loss': 0.1623, 'grad_norm': 0.13200953602790833, 'learning_rate': 7.405712299403279e-05, 'epoch': 2.26}\n",
      "{'loss': 0.1638, 'grad_norm': 0.1409507840871811, 'learning_rate': 7.116041944267423e-05, 'epoch': 2.29}\n",
      "{'loss': 0.1619, 'grad_norm': 0.15658685564994812, 'learning_rate': 6.826371589131568e-05, 'epoch': 2.32}\n",
      "{'loss': 0.1619, 'grad_norm': 0.1291464865207672, 'learning_rate': 6.536701233995713e-05, 'epoch': 2.35}\n",
      "{'loss': 0.162, 'grad_norm': 0.15704958140850067, 'learning_rate': 6.247030878859856e-05, 'epoch': 2.38}\n",
      "{'loss': 0.1586, 'grad_norm': 0.15808796882629395, 'learning_rate': 5.957360523724002e-05, 'epoch': 2.4}\n",
      "{'loss': 0.1599, 'grad_norm': 0.10375279188156128, 'learning_rate': 5.667690168588146e-05, 'epoch': 2.43}\n",
      "{'loss': 0.1577, 'grad_norm': 0.19029079377651215, 'learning_rate': 5.3780198134522905e-05, 'epoch': 2.46}\n",
      "{'loss': 0.1599, 'grad_norm': 0.13976408541202545, 'learning_rate': 5.0883494583164354e-05, 'epoch': 2.49}\n",
      "{'loss': 0.1603, 'grad_norm': 0.2075130194425583, 'learning_rate': 4.7986791031805804e-05, 'epoch': 2.52}\n",
      "{'loss': 0.1606, 'grad_norm': 0.12486708909273148, 'learning_rate': 4.509008748044725e-05, 'epoch': 2.55}\n",
      "{'loss': 0.1577, 'grad_norm': 0.20161373913288116, 'learning_rate': 4.2193383929088697e-05, 'epoch': 2.58}\n",
      "{'loss': 0.1561, 'grad_norm': 0.12639252841472626, 'learning_rate': 3.929668037773013e-05, 'epoch': 2.61}\n",
      "{'loss': 0.162, 'grad_norm': 0.1343013048171997, 'learning_rate': 3.639997682637158e-05, 'epoch': 2.64}\n",
      "{'loss': 0.1608, 'grad_norm': 0.1224772110581398, 'learning_rate': 3.350327327501303e-05, 'epoch': 2.66}\n",
      "{'loss': 0.1586, 'grad_norm': 0.16602283716201782, 'learning_rate': 3.060656972365448e-05, 'epoch': 2.69}\n",
      "{'loss': 0.1603, 'grad_norm': 0.13062161207199097, 'learning_rate': 2.7709866172295925e-05, 'epoch': 2.72}\n",
      "{'loss': 0.1594, 'grad_norm': 0.18520332872867584, 'learning_rate': 2.481316262093737e-05, 'epoch': 2.75}\n",
      "{'loss': 0.1606, 'grad_norm': 0.21514829993247986, 'learning_rate': 2.191645906957882e-05, 'epoch': 2.78}\n",
      "{'loss': 0.1595, 'grad_norm': 0.14966285228729248, 'learning_rate': 1.9019755518220264e-05, 'epoch': 2.81}\n",
      "{'loss': 0.1617, 'grad_norm': 0.1444791555404663, 'learning_rate': 1.612305196686171e-05, 'epoch': 2.84}\n",
      "{'loss': 0.1603, 'grad_norm': 0.12736007571220398, 'learning_rate': 1.3226348415503156e-05, 'epoch': 2.87}\n",
      "{'loss': 0.1595, 'grad_norm': 0.158569797873497, 'learning_rate': 1.0329644864144602e-05, 'epoch': 2.9}\n",
      "{'loss': 0.1585, 'grad_norm': 0.16774699091911316, 'learning_rate': 7.432941312786049e-06, 'epoch': 2.93}\n",
      "{'loss': 0.1619, 'grad_norm': 0.19029484689235687, 'learning_rate': 4.536237761427495e-06, 'epoch': 2.95}\n",
      "{'loss': 0.1611, 'grad_norm': 0.11147435754537582, 'learning_rate': 1.6395342100689414e-06, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Desktop/MLotsawa/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e02ecfaa224f448cee3bf7f8db4402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1523 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1302887201309204, 'eval_bleu': 5.4383, 'eval_gen_len': 16.0153, 'eval_runtime': 421.8028, 'eval_samples_per_second': 28.886, 'eval_steps_per_second': 3.611, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10903.968, 'train_samples_per_second': 37.991, 'train_steps_per_second': 4.749, 'train_loss': 0.18972355383442457, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=51783, training_loss=0.18972355383442457, metrics={'train_runtime': 10903.968, 'train_samples_per_second': 37.991, 'train_steps_per_second': 4.749, 'total_flos': 3.28509444980736e+16, 'train_loss': 0.18972355383442457, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"bidirectional\",\n",
    "    auto_find_batch_size=True,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, #check this\n",
    "    push_to_hub=False,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
