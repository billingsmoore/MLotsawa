{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "84d2e440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "from local_gemma import LocalGemma2ForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = LocalGemma2ForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", preset=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "]\n",
    "\n",
    "model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs.to(model.device), max_new_tokens=1024, do_sample=True)\n",
    "decoded_text = tokenizer.batch_decode(generated_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd309315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Now that's a condiment most folks struggle with... \n",
      "\n",
      " ðŸ˜…\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get only the new tokens generated by the model (excluding the input)\n",
    "input_length = model_inputs[\"input_ids\"].shape[1]\n",
    "generated_response_ids = generated_ids[:, input_length:]\n",
    "\n",
    "# Decode only the generated response\n",
    "decoded_response = tokenizer.batch_decode(generated_response_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(decoded_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "803474db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<bos><start_of_turn>user\\nWhat is your favourite condiment?<end_of_turn>\\n<start_of_turn>model\\nWell, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!<end_of_turn>\\n<start_of_turn>user\\nDo you have mayonnaise recipes?<end_of_turn>\\n\\n\\nNow that's a condiment most folks struggle with... \\n\\n ðŸ˜…\\n\\n\\n<end_of_turn>\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99161e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "What is your favourite condiment?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!<end_of_turn>\n",
      "<start_of_turn>user\n",
      "Do you have mayonnaise recipes?<end_of_turn>\n",
      "\n",
      "\n",
      "Now that's a condiment most folks struggle with... \n",
      "\n",
      " ðŸ˜…\n",
      "\n",
      "\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "print(decoded_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1a7db052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemma_chat(model=model, tokenizer=tokenizer, messages=[]):\n",
    "\n",
    "    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True)\n",
    "\n",
    "    generated_ids = model.generate(**model_inputs.to(model.device), max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "    # Get only the new tokens generated by the model (excluding the input)\n",
    "    input_length = model_inputs[\"input_ids\"].shape[1]\n",
    "    generated_response_ids = generated_ids[:, input_length:]\n",
    "\n",
    "    # Decode only the generated response\n",
    "    decoded_response = tokenizer.batch_decode(generated_response_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return decoded_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3a2846b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class Agent(abc.ABC):\n",
    "    def __init__(self):\n",
    "        self.user_initial_message = {'role': 'user', 'content': \"Introduce yourself.\"}\n",
    "        self.sys_message = {'role': 'assistant', 'content': self.system_prompt()}\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def system_prompt(self):\n",
    "        \"\"\"Return the system prompt content\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def user_prompt(self):\n",
    "        pass\n",
    "\n",
    "    def chat(self, user_input=None, model=model):\n",
    "\n",
    "        if user_input:\n",
    "            user_content = self.user_prompt(user_input)\n",
    "        else:\n",
    "            user_content = self.user_message_default\n",
    "\n",
    "        #print(user_content)\n",
    "\n",
    "        user_message = {'role': 'user', 'content': user_content}\n",
    "\n",
    "        messages = [self.user_initial_message, self.sys_message, user_message]\n",
    "\n",
    "        print(f'Passing the following messages to model: {messages}')\n",
    "\n",
    "        response = gemma_chat(model=model, messages=messages)\n",
    "\n",
    "        return response.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9acf5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(Agent):\n",
    "    def system_prompt(self):\n",
    "        return \"I am an expert editor. I preprocess material for translation with no additional output.\"\n",
    "    \n",
    "    def user_prompt(self, user_input):\n",
    "        return f\"\"\"Preprocess the following: {user_input}\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2544e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(Agent):\n",
    "    def system_prompt(self):\n",
    "        return \"I am an expert translator. I translate clearly and concisely with no additional output.\"\n",
    "    \n",
    "    def user_prompt(self, user_input):\n",
    "        return f\"Translate the following to German:{user_input}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "49bc24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(Agent):\n",
    "    def system_prompt(self):\n",
    "        return \"I am an expert in translation evaluation.\"\n",
    "    \n",
    "    def user_prompt(self, user_input):\n",
    "        return f\"\"\"Score the following translation on a continuous scale from 0 to 100, where score of zero means\n",
    "                \"no meaning preserved\" and score of one hundred means \"perfect meaning and grammar\" but provide no additional output.\n",
    "\n",
    "                Source: {user_input['source']}\n",
    "                Target: {user_input['target']}\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ad99fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostEditor(Agent):\n",
    "    def system_prompt(self):\n",
    "        return \"\"\"\"I am an expert translation editor. I provide improved translations based on an original translation and a score from 0 to 100, where score of zero means\n",
    "                \"no meaning preserved\" and score of one hundred means \"perfect meaning and grammar\".\"\"\"\n",
    "    \n",
    "    def user_prompt(self, user_input):\n",
    "        return f\"\"\"Improv the following translation:\n",
    "        Source: {user_input['source']},\n",
    "        Translation: {user_input['translation']}\n",
    "        Score: {user_input['score']}\n",
    "\n",
    "        Only output the corrected target translation.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1c338df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro = Preprocessor()\n",
    "translator = Translator()\n",
    "eval = Evaluator()\n",
    "post = PostEditor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d2088b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing the following messages to model: [{'role': 'user', 'content': 'Introduce yourself.'}, {'role': 'assistant', 'content': 'I am an expert editor. I preprocess material for translation with no additional output.'}, {'role': 'user', 'content': 'Preprocess the following: Im eat cake.\\n                '}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'* **Capitalize all the first letters of the text:**  I eat Cake.'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepro.chat(user_input='Im eat cake.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9636c15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing the following messages to model: [{'role': 'user', 'content': 'Introduce yourself.'}, {'role': 'assistant', 'content': 'I am an expert translator. I translate clearly and concisely with no additional output.'}, {'role': 'user', 'content': 'Translate the following to German:I am eating cake.'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Ich esse Kuchen. \\n\\n\\n**Here's the grammar breakdown:**\\n\\n* **Ich** -  I\\n* ** esse** - eat (present tense)\\n* **Kuchen** - cake \\n\\nLet me know if you'd like to try translating anything else.\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.chat(user_input='I am eating cake.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac9d8141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing the following messages to model: [{'role': 'user', 'content': 'Introduce yourself.'}, {'role': 'assistant', 'content': 'I am an expert in translation evaluation.'}, {'role': 'user', 'content': 'Score the following translation on a continuous scale from 0 to 100, where score of zero means\\n                \"no meaning preserved\" and score of one hundred means \"perfect meaning and grammar\" but provide no additional output.\\n\\n                Source: I am a dog\\n                Target: Ich bin ein Berliner\\n                '}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'*Please provide your reasoning where applicable.*\\n\\n\\n**Score: ** 95\\n\\n**Reasoning:** \\n\\nThe core meaning of the sentence \"I am a dog\" is preserved in \"Ich bin ein Berliner.\" While some nuances of direct object choice or grammatical variations might differ, the core message of the sentence is not lost.'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.chat({'source': 'I am a dog', 'target': 'Ich bin ein Berliner'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ef814728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing the following messages to model: [{'role': 'user', 'content': 'Introduce yourself.'}, {'role': 'assistant', 'content': '\"I am an expert translation editor. I provide improved translations based on an original translation and a score from 0 to 100, where score of zero means\\n                \"no meaning preserved\" and score of one hundred means \"perfect meaning and grammar\".'}, {'role': 'user', 'content': 'Improv the following translation:\\n        Source: I am a dog,\\n        Translation: Ich bin ein Berliner\\n        Score: 30.5\\n\\n        Only output the corrected target translation.\\n        '}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Ich bin ein Berliner\"\\n\\n\\nThis translation is incorrect because it\\'s an exact opposite, a dog can\\'t fly. The correct translation of \"dog\" in German is \"Hund,\" and \"I\" is \"ich\".'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post.chat({'source': 'I am a dog', 'translation': 'Ich bin ein Berliner', 'score':30.5})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
