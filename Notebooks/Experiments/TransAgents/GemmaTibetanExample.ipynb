{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2e440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_gemma import LocalGemma2ForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = LocalGemma2ForCausalLM.from_pretrained(\"google/gemma-2-27b-it\", preset=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-27b-it\")\n",
    "\n",
    "def gemma_chat(model=model, tokenizer=tokenizer, messages=[]):\n",
    "\n",
    "    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True)\n",
    "\n",
    "    generated_ids = model.generate(**model_inputs.to(model.device), max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "    # Get only the new tokens generated by the model (excluding the input)\n",
    "    input_length = model_inputs[\"input_ids\"].shape[1]\n",
    "    generated_response_ids = generated_ids[:, input_length:]\n",
    "\n",
    "    # Decode only the generated response\n",
    "    decoded_response = tokenizer.batch_decode(generated_response_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return decoded_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2846b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "class Agent(abc.ABC):\n",
    "    def __init__(self):\n",
    "        self.user_initial_message = {'role': 'user', 'content': \"Introduce yourself.\"}\n",
    "        self.sys_message = {'role': 'assistant', 'content': self.system_prompt()}\n",
    "        self.user_message_default = {'role': 'user', 'content': 'Give me a demonstration.'}\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def system_prompt(self):\n",
    "        \"\"\"Return the system prompt content\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def user_prompt(self):\n",
    "        pass\n",
    "\n",
    "    def chat(self, user_input=None, model=model):\n",
    "\n",
    "        if user_input:\n",
    "            user_content = self.user_prompt(user_input)\n",
    "        else:\n",
    "            user_content = self.user_message_default\n",
    "\n",
    "        #print(user_content)\n",
    "\n",
    "        user_message = {'role': 'user', 'content': user_content}\n",
    "\n",
    "        messages = [self.user_initial_message, self.sys_message, user_message]\n",
    "\n",
    "        print(f'Passing the following messages to model: {messages}')\n",
    "\n",
    "        response = gemma_chat(model=model, messages=messages)\n",
    "\n",
    "        return response.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf5fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(Agent):\n",
    "    def system_prompt(self):\n",
    "        return \"I am an expert editor of Tibetan Buddhist texts. I preprocess Tibetan material for translation.\"\n",
    "    \n",
    "    def user_prompt(self, user_input):\n",
    "        return f\"\"\"Preprocess the following: {user_input}\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2544e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(Agent):\n",
    "    def system_prompt(self):\n",
    "        return \"I am an expert translator of Tibetan Buddhist texts. I translate clearly and concisely, preserving the original intent of the text.\"\n",
    "    \n",
    "    def user_prompt(self, user_input):\n",
    "        return f\"Translate the following from Tibetan to English:{user_input}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator(Agent):\n",
    "    def system_prompt(self):\n",
    "        return \"I am an expert in translation evaluation. I score translations based on fluency, literary quality, and meaning preservation.\"\n",
    "    \n",
    "    def user_prompt(self, user_input):\n",
    "        return f\"\"\"Score the following translation on a continuous scale from 0 to 100, where score of zero means\n",
    "                \"no meaning preserved\" and score of one hundred means \"perfect meaning and grammar\".\n",
    "\n",
    "                Source: {user_input['source']}\n",
    "                Target: {user_input['target']}\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad99fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostEditor(Agent):\n",
    "    def system_prompt(self):\n",
    "        return \"\"\"\"I am an expert translation editor. I provide improved translations based on an original translation and a score from an evaluator. \n",
    "        I preserve the good qualities of the first draft but improve clarity and fluency.\"\"\"\n",
    "    \n",
    "    def user_prompt(self, user_input):\n",
    "        return f\"\"\"Improv the following translation:\n",
    "        Source: {user_input['source']},\n",
    "        Translation: {user_input['translation']}\n",
    "        Score: {user_input['score']}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c338df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro = Preprocessor()\n",
    "translator = Translator()\n",
    "eval = Evaluator()\n",
    "post = PostEditor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tib = \"\"\"སེམས་ཅན་ཐམས་ཅད་ཀྱི་སྡུག་བསྔལ་སེལ་བ་ལ་ཕྱག་འཚལ་ལོ། །\n",
    "སེམས་ཅན་ཐམས་ཅད་སྡུག་བསྔལ་ལས་གྲོལ་བར་མཛད་པ་ལ་ཕྱག་འཚལ་ལོ། །\n",
    "སེམས་ཅན་ཐམས་ཅད་ཀྱི་བཅིངས་པ་གཅོད་པ་ལ་ཕྱག་འཚལ་ལོ། །\n",
    "སེམས་ཅན་ཐམས་ཅད་ལ་མི་འཇིགས་པ་སྦྱིན་པ་ལ་ཕྱག་འཚལ་ལོ། །\"\"\"\n",
    "\n",
    "eng = \"\"\"Homage to the one who dispels the suffering of all beings.\n",
    "Homage to the one who liberates all beings from suffering.\n",
    "Homage to the one who cuts the bonds of all beings.\n",
    "Homage to the one who grants fearlessness to all beings.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2088b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_tib = \"\"\"སེམས་ཅན་ཐམས་ཅད་ཀྱི་སྡུག་བསལ་བ་ལ་ཕྱག་འཚལ་ལོ། །\n",
    "སེམསན་ཐམས་ཅད་སྡུག་བསྔལ་ལས་གྲོལ་བར་མཛད་པ་ལ་ཕྱག་འཚལ་ལོ། །\n",
    "སེམས་ཅན་ཐམས་ཅད་ཀྱི་བཅ་པ་གཅོད་པ་ལ་ཕྱག་འཚལ་ལོ། །\n",
    "སེམས་ཅན་ཐམས་ཅད་ལ་མི་འཇིགས་པ་སྦྱིན་པ་ལ་ཕག་འཚལ་ལོ། །\"\"\"\n",
    "\n",
    "prepro.chat(user_input=corrupted_tib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9636c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = translator.chat(user_input=tib)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d8141",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval.chat({'source': tib, 'target': response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef814728",
   "metadata": {},
   "outputs": [],
   "source": [
    "post.chat({'source': tib, 'translation': response , 'score':50})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
