{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238f206c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2fcd6f505e648d38f26580d2e60a314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    Wav2Vec2ForPreTraining, \n",
    "    Wav2Vec2FeatureExtractor, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import torch\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "# Load data and model\n",
    "dataset = load_from_disk(\"../Data/kham_asr_preprocessed\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b598f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask time prob: 0.065\n",
      "Mask time length: 10\n"
     ]
    }
   ],
   "source": [
    "# Load model with proper configuration for pretraining\n",
    "model = Wav2Vec2ForPreTraining.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\",\n",
    ")\n",
    "\n",
    "# CRITICAL: Set masking config - without this, no loss is computed\n",
    "model.config.mask_time_prob = 0.065  # Probability of masking time steps\n",
    "model.config.mask_time_length = 10    # Length of mask spans\n",
    "model.config.mask_feature_prob = 0.0  # No feature masking\n",
    "model.config.mask_feature_length = 10\n",
    "\n",
    "# Verify config\n",
    "print(f\"Mask time prob: {model.config.mask_time_prob}\")\n",
    "print(f\"Mask time length: {model.config.mask_time_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8dad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Optional\n",
    "import numpy as np\n",
    "\n",
    "# Custom DataCollator that generates mask_time_indices\n",
    "@dataclass\n",
    "class DataCollatorForWav2Vec2Pretraining:\n",
    "    model: Wav2Vec2ForPreTraining\n",
    "    feature_extractor: Wav2Vec2FeatureExtractor\n",
    "    padding: Union[bool, str] = True\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract input_values and convert to list if tensor\n",
    "        input_values = []\n",
    "        for feature in features:\n",
    "            val = feature[\"input_values\"]\n",
    "            if isinstance(val, torch.Tensor):\n",
    "                val = val.squeeze().tolist()\n",
    "            input_values.append(val)\n",
    "        \n",
    "        # Pad using feature extractor\n",
    "        batch = self.feature_extractor.pad(\n",
    "            {\"input_values\": input_values},\n",
    "            padding=self.padding,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        batch_size = batch[\"input_values\"].shape[0]\n",
    "        \n",
    "        # Create attention mask\n",
    "        mask_indices_seq_length = self.model._get_feat_extract_output_lengths(\n",
    "            batch[\"input_values\"].shape[-1]\n",
    "        )\n",
    "        \n",
    "        # Make sure masked sequence length is a Python scalar\n",
    "        if isinstance(mask_indices_seq_length, torch.Tensor):\n",
    "            mask_indices_seq_length = mask_indices_seq_length.item()\n",
    "        \n",
    "        # Create attention mask for the transformer (keep on CPU)\n",
    "        attention_mask = torch.ones(\n",
    "            (batch_size, mask_indices_seq_length), \n",
    "            dtype=torch.long,\n",
    "        )\n",
    "        \n",
    "        # Generate mask_time_indices\n",
    "        mask_time_indices = None\n",
    "        if self.model.config.mask_time_prob > 0:\n",
    "            mask_time_indices = self._compute_mask_indices(\n",
    "                (batch_size, mask_indices_seq_length),\n",
    "                mask_prob=self.model.config.mask_time_prob,\n",
    "                mask_length=self.model.config.mask_time_length,\n",
    "                min_masks=2,\n",
    "            )\n",
    "            # CRITICAL: Must be boolean tensor\n",
    "            mask_time_indices = torch.from_numpy(mask_time_indices).bool()\n",
    "        \n",
    "        # CRITICAL: Sample negative indices for contrastive loss\n",
    "        sampled_negative_indices = None\n",
    "        if self.model.config.num_negatives > 0 and mask_time_indices is not None:\n",
    "            sampled_negative_indices = self._sample_negative_indices(\n",
    "                (batch_size, mask_indices_seq_length),\n",
    "                num_negatives=self.model.config.num_negatives,\n",
    "                mask_time_indices=mask_time_indices,\n",
    "            )\n",
    "            sampled_negative_indices = torch.from_numpy(sampled_negative_indices).long()\n",
    "        \n",
    "        return {\n",
    "            \"input_values\": batch[\"input_values\"],\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"mask_time_indices\": mask_time_indices,\n",
    "            \"sampled_negative_indices\": sampled_negative_indices,\n",
    "        }\n",
    "    \n",
    "    def _compute_mask_indices(\n",
    "        self,\n",
    "        shape: tuple,\n",
    "        mask_prob: float,\n",
    "        mask_length: int,\n",
    "        min_masks: int = 0,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Computes random mask spans for a given shape. Used for masking time steps.\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length = shape\n",
    "        \n",
    "        if mask_length < 1:\n",
    "            raise ValueError(\"`mask_length` has to be bigger than 0.\")\n",
    "        \n",
    "        if mask_length > sequence_length:\n",
    "            raise ValueError(\n",
    "                f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n",
    "                f\" and `sequence_length`: {sequence_length}`\"\n",
    "            )\n",
    "        \n",
    "        # Compute number of masked spans\n",
    "        num_masked_spans = int(mask_prob * sequence_length / mask_length + np.random.rand(1).item())\n",
    "        num_masked_spans = max(num_masked_spans, min_masks)\n",
    "        \n",
    "        # Make sure num masked indices <= sequence_length\n",
    "        if num_masked_spans * mask_length > sequence_length:\n",
    "            num_masked_spans = sequence_length // mask_length\n",
    "        \n",
    "        # Create mask\n",
    "        mask = np.zeros((batch_size, sequence_length), dtype=bool)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            if num_masked_spans == 0:\n",
    "                continue\n",
    "                \n",
    "            # Get random indices to mask\n",
    "            mask_indices = np.random.choice(\n",
    "                sequence_length - mask_length + 1,\n",
    "                num_masked_spans,\n",
    "                replace=False\n",
    "            )\n",
    "            \n",
    "            # Expand to mask_length\n",
    "            mask_indices = np.concatenate([\n",
    "                mask_indices + j for j in range(mask_length)\n",
    "            ])\n",
    "            \n",
    "            mask[i, mask_indices] = True\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def _sample_negative_indices(\n",
    "        self,\n",
    "        shape: tuple,\n",
    "        num_negatives: int,\n",
    "        mask_time_indices: torch.Tensor,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Sample negative indices for contrastive loss.\n",
    "        For each masked position, sample num_negatives random negative samples.\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length = shape\n",
    "        \n",
    "        # Convert mask to numpy if needed\n",
    "        if isinstance(mask_time_indices, torch.Tensor):\n",
    "            mask_time_indices_np = mask_time_indices.cpu().numpy()\n",
    "        else:\n",
    "            mask_time_indices_np = mask_time_indices\n",
    "        \n",
    "        # For each masked position, sample num_negatives negatives\n",
    "        # Shape: (batch, sequence_length, num_negatives)\n",
    "        sampled_negatives = np.zeros((batch_size, sequence_length, num_negatives), dtype=np.int32)\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            # Get masked positions for this batch item\n",
    "            masked_positions = np.where(mask_time_indices_np[batch_idx])[0]\n",
    "            \n",
    "            if len(masked_positions) == 0:\n",
    "                continue\n",
    "            \n",
    "            # For each position (masked or not), sample negatives\n",
    "            for seq_idx in range(sequence_length):\n",
    "                # Sample num_negatives random indices\n",
    "                # These should be different from seq_idx\n",
    "                candidates = list(range(sequence_length))\n",
    "                if seq_idx in candidates:\n",
    "                    candidates.remove(seq_idx)\n",
    "                \n",
    "                if len(candidates) >= num_negatives:\n",
    "                    neg_indices = np.random.choice(candidates, num_negatives, replace=False)\n",
    "                else:\n",
    "                    # If not enough candidates, sample with replacement\n",
    "                    neg_indices = np.random.choice(candidates, num_negatives, replace=True)\n",
    "                \n",
    "                sampled_negatives[batch_idx, seq_idx] = neg_indices\n",
    "        \n",
    "        return sampled_negatives\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = DataCollatorForWav2Vec2Pretraining(\n",
    "    model=model,\n",
    "    feature_extractor=feature_extractor,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e63c2d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec_pretrain\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-5,  # Slightly lower learning rate\n",
    "    warmup_steps=500,    # Add warmup\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    save_strategy='epoch',\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_drop_last=True,\n",
    ")\n",
    "\n",
    "# Custom Trainer that manually computes the loss\n",
    "class Wav2Vec2PretrainingTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # CRITICAL: Ensure model is in training mode\n",
    "        if not model.training:\n",
    "            model.train()\n",
    "        \n",
    "        # Debug: print input shapes on first call\n",
    "        if not hasattr(self, '_printed_shapes'):\n",
    "            print(f\"Input shapes:\")\n",
    "            print(f\"  input_values: {inputs['input_values'].shape}\")\n",
    "            print(f\"  attention_mask: {inputs['attention_mask'].shape}\")\n",
    "            if inputs.get('mask_time_indices') is not None:\n",
    "                print(f\"  mask_time_indices: {inputs['mask_time_indices'].shape}\")\n",
    "                print(f\"  mask_time_indices dtype: {inputs['mask_time_indices'].dtype}\")\n",
    "                print(f\"  mask_time_indices sum: {inputs['mask_time_indices'].sum()}\")\n",
    "            if inputs.get('sampled_negative_indices') is not None:\n",
    "                print(f\"  sampled_negative_indices: {inputs['sampled_negative_indices'].shape}\")\n",
    "            print(f\"Model training mode: {model.training}\")\n",
    "            print(f\"Model config mask_time_prob: {model.config.mask_time_prob}\")\n",
    "            self._printed_shapes = True\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # If model doesn't compute loss, compute it manually\n",
    "        if outputs.loss is None:\n",
    "            # Manually compute contrastive loss\n",
    "            import torch.nn.functional as F\n",
    "            \n",
    "            # Get the outputs we need\n",
    "            hidden_states = outputs.projected_states  # (batch, seq_len, hidden_size)\n",
    "            quantized_states = outputs.projected_quantized_states  # (batch, seq_len, hidden_size)\n",
    "            \n",
    "            mask_time_indices = inputs.get('mask_time_indices')\n",
    "            sampled_negative_indices = inputs.get('sampled_negative_indices')\n",
    "            \n",
    "            if mask_time_indices is None or sampled_negative_indices is None:\n",
    "                raise ValueError(\"Need mask_time_indices and sampled_negative_indices to compute loss\")\n",
    "            \n",
    "            batch_size, sequence_length, hidden_size = hidden_states.shape\n",
    "            \n",
    "            # Mask the hidden states - only compute loss on masked positions\n",
    "            # mask_time_indices shape: (batch, seq_len)\n",
    "            masked_hidden = hidden_states[mask_time_indices]  # (num_masked, hidden_size)\n",
    "            masked_quantized = quantized_states[mask_time_indices]  # (num_masked, hidden_size)\n",
    "            \n",
    "            # Sample negatives\n",
    "            # sampled_negative_indices shape: (batch, seq_len, num_negatives)\n",
    "            num_negatives = sampled_negative_indices.shape[-1]\n",
    "            \n",
    "            # Expand quantized_states for negative sampling\n",
    "            # Shape: (batch, seq_len, num_negatives, hidden_size)\n",
    "            negative_quantized_vectors = quantized_states.unsqueeze(2).expand(\n",
    "                batch_size, sequence_length, num_negatives, hidden_size\n",
    "            )\n",
    "            \n",
    "            # Gather the negative samples\n",
    "            # Shape: (batch, seq_len, num_negatives, hidden_size)\n",
    "            sampled_negative_indices_expanded = sampled_negative_indices.unsqueeze(-1).expand(\n",
    "                batch_size, sequence_length, num_negatives, hidden_size\n",
    "            )\n",
    "            negative_quantized = torch.gather(\n",
    "                negative_quantized_vectors, \n",
    "                dim=1,\n",
    "                index=sampled_negative_indices_expanded\n",
    "            )\n",
    "            \n",
    "            # Only keep negatives for masked positions\n",
    "            negative_quantized = negative_quantized[mask_time_indices]  # (num_masked, num_negatives, hidden_size)\n",
    "            \n",
    "            # Compute cosine similarity\n",
    "            # Positive similarity (correct targets)\n",
    "            positive_similarity = F.cosine_similarity(\n",
    "                masked_hidden.unsqueeze(1),  # (num_masked, 1, hidden_size)\n",
    "                masked_quantized.unsqueeze(1),  # (num_masked, 1, hidden_size)\n",
    "                dim=-1\n",
    "            )  # (num_masked, 1)\n",
    "            \n",
    "            # Negative similarity\n",
    "            negative_similarity = F.cosine_similarity(\n",
    "                masked_hidden.unsqueeze(1),  # (num_masked, 1, hidden_size)\n",
    "                negative_quantized,  # (num_masked, num_negatives, hidden_size)\n",
    "                dim=-1\n",
    "            )  # (num_masked, num_negatives)\n",
    "            \n",
    "            # Concatenate positive and negative logits\n",
    "            logits = torch.cat([positive_similarity, negative_similarity], dim=1)  # (num_masked, 1 + num_negatives)\n",
    "            \n",
    "            # Apply temperature\n",
    "            temperature = getattr(model.config, 'contrastive_logits_temperature', 0.1)\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Target is always index 0 (the positive example)\n",
    "            targets = torch.zeros(logits.shape[0], dtype=torch.long, device=logits.device)\n",
    "            \n",
    "            # Compute cross entropy loss\n",
    "            contrastive_loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "            # Add diversity loss (codevector_perplexity encourages diversity)\n",
    "            diversity_loss = ((num_negatives - outputs.codevector_perplexity) / num_negatives) * model.config.num_codevectors\n",
    "            diversity_loss_weight = getattr(model.config, 'diversity_loss_weight', 0.1)\n",
    "            \n",
    "            loss = contrastive_loss + diversity_loss_weight * diversity_loss\n",
    "            \n",
    "            print(f\"Manually computed loss: {loss.item():.4f} (contrastive: {contrastive_loss.item():.4f}, diversity: {diversity_loss.item():.4f})\")\n",
    "        else:\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Use custom Trainer\n",
    "trainer = Wav2Vec2PretrainingTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfb891ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=wav2vec\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=wav2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18db5738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes:\n",
      "  input_values: torch.Size([4, 126900])\n",
      "  attention_mask: torch.Size([4, 396])\n",
      "  mask_time_indices: torch.Size([4, 396])\n",
      "  mask_time_indices dtype: torch.bool\n",
      "  mask_time_indices sum: 80\n",
      "  sampled_negative_indices: torch.Size([4, 396, 100])\n",
      "Model training mode: True\n",
      "Model config mask_time_prob: 0.065\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10515' max='10515' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10515/10515 10:03:49, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>207.948900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>151.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>139.807000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>134.325500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>128.535600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>123.976200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>120.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>118.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>115.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>114.095200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>113.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>113.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>110.615500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>110.061700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>110.253100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>109.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>108.798900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>106.425200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>105.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>106.421000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>105.961000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10515, training_loss=121.58965200151198, metrics={'train_runtime': 36233.4201, 'train_samples_per_second': 9.283, 'train_steps_per_second': 0.29, 'total_flos': 2.4061203328424436e+19, 'train_loss': 121.58965200151198, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfefff31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('five_epoch_kham_pretrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5924a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
