{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file_name', 'uni', 'wylie', 'url', 'dept', 'grade', 'char_len', 'audio_len', '__index_level_0__', 'audio', 'transcript'],\n",
       "    num_rows: 3131\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"../Data/garchen_dataset\")['validation']\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForCTC(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (lm_head): Linear(in_features=1024, out_features=131, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, AutoTokenizer, Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2Config\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"openpecha/Garchen_Rinpoche_stt\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openpecha/Garchen_Rinpoche_stt\")\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "config = Wav2Vec2Config.from_pretrained(\"openpecha/Garchen_Rinpoche_stt\")\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"../Checkpoints/nicttib1-pre-ft/model\", \n",
    "    config=config\n",
    ")\n",
    "\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function generate_predictions at 0x7d4f1c43c360> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2f8d7e447541be93473cefd39c37df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def generate_predictions(batch):\n",
    "    # Load and resample the audio\n",
    "    audio = batch[\"audio\"]\n",
    "    inputs = processor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"], \n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    ).input_values.to(\"cuda\")\n",
    "\n",
    "    # Generate logits and get argmax predictions\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Decode predictions to text\n",
    "    batch[\"prediction\"] = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "# Apply the function to the test dataset\n",
    "processed_test_dataset = ds.map(generate_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Error Rate (CER): 0.29316734677785006\n",
      "Syllable Error Rate (SER): 0.5647187026707546\n",
      "Word Error Rate (WER): 0.5978872139342342\n"
     ]
    }
   ],
   "source": [
    "import jiwer\n",
    "from tibetan_wer.metrics import wer, ser\n",
    "\n",
    "# Extract predictions and references\n",
    "predictions = [elt[0].replace(' ', 'à¼‹') for elt in processed_test_dataset[\"prediction\"]]\n",
    "references = [elt for elt in processed_test_dataset[\"uni\"]]\n",
    "\n",
    "# Compute metrics\n",
    "cer = jiwer.cer(predictions, references)\n",
    "ser = ser(predictions, references)['micro_ser']\n",
    "wer = wer(predictions, references)['micro_wer']\n",
    "\n",
    "print(f\"Character Error Rate (CER): {cer}\")\n",
    "print(f\"Syllable Error Rate (SER): {ser}\")\n",
    "print(f\"Word Error Rate (WER): {wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('1_ep_nicttib1_ft_val_preds.pickle', 'wb') as f:\n",
    "    pickle.dump(predictions, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
