{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk('ape-ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bo': 'ཞི་ཁྲོ་སྤྲུལ་པའི་སྐུ་ལ་ཕྱག་འཚལ་ལོ༔',\n",
       " 'en': 'Nirmāṇakāya peaceful and wrathful: to you I pay homage!',\n",
       " 'topic': 'Confession, Termas, Tibetan Masters, Nyala Pema Dündul',\n",
       " 'for-post-edit': 'Homage to the tathāgatas and the Victorious Ones of the Victorious Ones: homage to you!'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Unfinetuned Tokenizer, Model, and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint = \"google-t5/t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('my-tokenizer')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "The dataset can now be tokenized for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ape_preprocess_function(examples):\n",
    "\n",
    "    # Prepare translation inputs and targets\n",
    "    inputs = ['Post-Edit Translation: ' + example for example in examples['for-post-edit']]\n",
    "    targets = [example for example in examples['en']]\n",
    "    \n",
    "    # Tokenize translation inputs and targets\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, \n",
    "                                         max_length=256, truncation=False, padding=\"max_length\")\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "def trans_preprocess_function(examples):\n",
    "\n",
    "    # Prepare translation inputs and targets\n",
    "    inputs = ['Translate Tibetan to English: ' + example for example in examples['bo']]\n",
    "    targets = [example for example in examples['en']]\n",
    "    \n",
    "    # Tokenize translation inputs and targets\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, \n",
    "                                         max_length=256, truncation=False, padding=\"max_length\")\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c35a25bbd8e407783721670317346d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21506 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "tokenized_ape_dataset = dataset.map(ape_preprocess_function, batched=True)\n",
    "tokenized_trans_dataset = dataset.map(trans_preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "tokenized_dataset = DatasetDict()\n",
    "\n",
    "tokenized_dataset['train'] = concatenate_datasets([tokenized_ape_dataset['train'], tokenized_trans_dataset['train']])\n",
    "#tokenized_dataset['test'] = concatenate_datasets([tokenized_ape_dataset['test'], tokenized_trans_dataset['test']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Finally, we can train the model. Note that the optimizer used is Adafactor. This is the optimizer that is preferred for translation tasks and for the T5 model in general. The transformers api includes a built in version of Adafactor, but I define it separately here so that we can optimize it with the 'accelerate' library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, Adafactor, EarlyStoppingCallback\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    scale_parameter=True, \n",
    "    relative_step=False, \n",
    "    warmup_init=False, \n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "model, optimizer = accelerator.prepare(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU and CHRF metrics\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "ter_metric = evaluate.load(\"ter\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Postprocess text\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_score = bleu_result[\"score\"]\n",
    "\n",
    "    # Compute CHRF score\n",
    "    chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    chrf_score = chrf_result[\"score\"]\n",
    "\n",
    "    # Compute TER score\n",
    "    ter_result = ter_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    ter_score = ter_result[\"score\"]\n",
    "\n",
    "    # Return rounded results\n",
    "    metrics = {\n",
    "        \"bleu\": round(bleu_score, 4),\n",
    "        \"chrf\": round(chrf_score, 4),\n",
    "        \"ter\": round(ter_score, 4)\n",
    "    }\n",
    "\n",
    "    #print(\"Computed Metrics:\", metrics)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=ape-experiment\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT=ape-experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4617/2482796188.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/j/Desktop/MLotsawa/Notebooks/Experiments/AutomaticPostEditing/wandb/run-20250306_101444-9zq494g4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/billingsmoore/ape-experiment/runs/9zq494g4' target=\"_blank\">dual-model</a></strong> to <a href='https://wandb.ai/billingsmoore/ape-experiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/billingsmoore/ape-experiment' target=\"_blank\">https://wandb.ai/billingsmoore/ape-experiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/billingsmoore/ape-experiment/runs/9zq494g4' target=\"_blank\">https://wandb.ai/billingsmoore/ape-experiment/runs/9zq494g4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='483860' max='483860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [483860/483860 26:33:13, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Translation Loss</th>\n",
       "      <th>Translation Bleu</th>\n",
       "      <th>Translation Chrf</th>\n",
       "      <th>Translation Ter</th>\n",
       "      <th>Post-edit Loss</th>\n",
       "      <th>Post-edit Bleu</th>\n",
       "      <th>Post-edit Chrf</th>\n",
       "      <th>Post-edit Ter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.167500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.170563</td>\n",
       "      <td>10.264300</td>\n",
       "      <td>23.951300</td>\n",
       "      <td>100.361300</td>\n",
       "      <td>0.162716</td>\n",
       "      <td>21.460300</td>\n",
       "      <td>35.151800</td>\n",
       "      <td>83.319900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.133600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.126804</td>\n",
       "      <td>20.757400</td>\n",
       "      <td>35.261900</td>\n",
       "      <td>82.559000</td>\n",
       "      <td>0.138191</td>\n",
       "      <td>22.502400</td>\n",
       "      <td>35.349600</td>\n",
       "      <td>80.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.102051</td>\n",
       "      <td>28.934900</td>\n",
       "      <td>43.458500</td>\n",
       "      <td>72.932800</td>\n",
       "      <td>0.121697</td>\n",
       "      <td>23.187900</td>\n",
       "      <td>36.344700</td>\n",
       "      <td>80.877800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.091200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.086048</td>\n",
       "      <td>35.142300</td>\n",
       "      <td>48.950100</td>\n",
       "      <td>66.033300</td>\n",
       "      <td>0.109127</td>\n",
       "      <td>25.329700</td>\n",
       "      <td>37.592800</td>\n",
       "      <td>78.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.081200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.075692</td>\n",
       "      <td>40.594200</td>\n",
       "      <td>52.876400</td>\n",
       "      <td>60.245800</td>\n",
       "      <td>0.100380</td>\n",
       "      <td>26.560500</td>\n",
       "      <td>38.919400</td>\n",
       "      <td>77.851300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.068580</td>\n",
       "      <td>44.521300</td>\n",
       "      <td>56.608100</td>\n",
       "      <td>56.058900</td>\n",
       "      <td>0.093610</td>\n",
       "      <td>28.098800</td>\n",
       "      <td>39.895900</td>\n",
       "      <td>76.041900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.063813</td>\n",
       "      <td>48.022000</td>\n",
       "      <td>59.376000</td>\n",
       "      <td>52.604600</td>\n",
       "      <td>0.088778</td>\n",
       "      <td>29.197100</td>\n",
       "      <td>40.931300</td>\n",
       "      <td>75.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.060741</td>\n",
       "      <td>50.389200</td>\n",
       "      <td>61.374900</td>\n",
       "      <td>50.308800</td>\n",
       "      <td>0.085367</td>\n",
       "      <td>30.541200</td>\n",
       "      <td>41.802300</td>\n",
       "      <td>73.769400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.058941</td>\n",
       "      <td>52.066100</td>\n",
       "      <td>62.720200</td>\n",
       "      <td>48.660900</td>\n",
       "      <td>0.083466</td>\n",
       "      <td>31.172600</td>\n",
       "      <td>42.481200</td>\n",
       "      <td>73.338400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.059700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.058384</td>\n",
       "      <td>52.910800</td>\n",
       "      <td>63.009800</td>\n",
       "      <td>47.872700</td>\n",
       "      <td>0.082719</td>\n",
       "      <td>31.611300</td>\n",
       "      <td>42.746100</td>\n",
       "      <td>72.735000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=483860, training_loss=0.09868416666950067, metrics={'train_runtime': 95598.2227, 'train_samples_per_second': 40.491, 'train_steps_per_second': 5.061, 'total_flos': 2.6194593624096768e+17, 'train_loss': 0.09868416666950067, 'epoch': 10.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"dual-model\",\n",
    "    auto_find_batch_size=True,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    push_to_hub=False,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=10\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset={'translation': tokenized_trans_dataset['test'], 'post-edit': tokenized_ape_dataset['test']},\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
