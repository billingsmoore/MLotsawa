{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('rat-poc-ds-w-context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': 'འཇིག་ལས་འདས་པའི་གང་འདུལ་ལོ།།',\n",
       " 'Target': 'Taming with transcendent beings.',\n",
       " 'File_Name': 'TM3076',\n",
       " 'Machine Aligned': False,\n",
       " '__index_level_0__': 1176089,\n",
       " 'Tag': 'Intrinsic Existence, Conventional Existence',\n",
       " 'context': ['འགྲོ་ཀུན་སྒྲིབ་པ་གཉིས་སྤངས་ཏེ།\\xa0། -> May all beings conquer the two obscurations',\n",
       "  'དགེ་བས་མཁའ་མཉམ་ལུས་ཅན་མ་ལུས་པ།། ཐེག་མཆོག་གོ་གྱོན་ཤེས་རབ་མཚོན་ཐོགས་ནས།། བདུད་བཞིའི་དགྲ་སྡེ་མ་ལུས་ཀུན་བཅོམ་སྟེ།། སྐུ་གསུམ་ནོར་བུའི་ཁྲི་ལ་འཁོད་གྱུར་ཅིག། -> Through this virtue, may all embodied beings throughout space without exception, Put on the armor of the Supreme Vehicle and having raised the weapon of wisdom, May they overcome all without exception of the host of enemies which are the four demons And be set on the jeweled throne of the three bodies.',\n",
       "  'སྐྱེ་འགག་ཡོད་མེད་ལ་སོགས་པའི་དམིགས་པ་དང་འཛིན་པའི་ཡུལ་ལས་འདས་པའི་རིག་སྟོང་སྤྲོས་བྲལ་མཉམ་པ་ཉིད་ཀྱི་ཁོར་ཡུག་ཡིན་ཏེ། -> Phenomena therefore transcend all objects of reference and clinging, such as origin and cessation, existence and nonexistence. They are the all-pervading, aware, and empty state of equality beyond mental elaboration.']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': ' དབང་ཤེས་ནི་རྟགས་ལས་དཔག་མི་དགོས་པར་མངོན་སུམ་དུ་ངེས་པའི་ཕྱིར་རོ།།',\n",
       " 'Target': ' Sense cognitions need not infer from signs but can ascertain things directly.',\n",
       " 'File_Name': 'TM0713',\n",
       " 'Machine Aligned': False,\n",
       " '__index_level_0__': 168915,\n",
       " 'Tag': 'Prophecies, Rituals',\n",
       " 'context': ['འགྲོ་ཀུན་སྒྲིབ་པ་གཉིས་སྤངས་ཏེ།\\xa0། -> May all beings conquer the two obscurations',\n",
       "  'དགེ་བས་མཁའ་མཉམ་ལུས་ཅན་མ་ལུས་པ།། ཐེག་མཆོག་གོ་གྱོན་ཤེས་རབ་མཚོན་ཐོགས་ནས།། བདུད་བཞིའི་དགྲ་སྡེ་མ་ལུས་ཀུན་བཅོམ་སྟེ།། སྐུ་གསུམ་ནོར་བུའི་ཁྲི་ལ་འཁོད་གྱུར་ཅིག། -> Through this virtue, may all embodied beings throughout space without exception, Put on the armor of the Supreme Vehicle and having raised the weapon of wisdom, May they overcome all without exception of the host of enemies which are the four demons And be set on the jeweled throne of the three bodies.',\n",
       "  'སྐྱེ་འགག་ཡོད་མེད་ལ་སོགས་པའི་དམིགས་པ་དང་འཛིན་པའི་ཡུལ་ལས་འདས་པའི་རིག་སྟོང་སྤྲོས་བྲལ་མཉམ་པ་ཉིད་ཀྱི་ཁོར་ཡུག་ཡིན་ཏེ། -> Phenomena therefore transcend all objects of reference and clinging, such as origin and cessation, existence and nonexistence. They are the all-pervading, aware, and empty state of equality beyond mental elaboration.']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer, Model, and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, device_map=\"cuda:0\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32355, 512)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a list of all Tibetan Unicode characters (U+0F00 to U+0FFF)\n",
    "tibetan_chars = [chr(codepoint) for codepoint in range(0x0F00, 0x0FFF)]\n",
    "\n",
    "# Add the Tibetan characters to the tokenizer's vocabulary\n",
    "new_tokens = [char for char in tibetan_chars if char not in tokenizer.get_vocab()]\n",
    "\n",
    "# Add new tokens to the tokenizer\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Resize model embeddings to accommodate the new vocabulary size\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'འཇིག་ལས་འདས་པའི་གང་འདུལ་ལོ།།</s>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = tokenizer.encode(dataset['train'][0]['Source'])\n",
    "dec = tokenizer.decode(enc)\n",
    "dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "The dataset can now be tokenized for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31cf9c4a2d34a3ea965c27bec3952aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_with_single_context(example):\n",
    "    # Use the first context if it exists, otherwise use a fallback\n",
    "    context = example['context'][0] if example['context'] and len(example['context']) > 0 else \"\"\n",
    "    # Create input text using the first context\n",
    "    input_text = f\"translate Tibetan to English: {example['Source']} Context: {context}\"\n",
    "    target_text = example['Target']\n",
    "\n",
    "    # Tokenize input and target\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=512).input_ids,\n",
    "        \"labels\": tokenizer(target_text, padding=\"max_length\", truncation=True, max_length=512).input_ids,\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "dataset_with_contexts = dataset.map(\n",
    "    preprocess_with_single_context,\n",
    "    batched=False,  # Process one example at a time\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU and CHRF metrics\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Postprocess text\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_score = bleu_result[\"score\"]\n",
    "\n",
    "    # Compute CHRF score\n",
    "    chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    chrf_score = chrf_result[\"score\"]\n",
    "\n",
    "    # Compute generation length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    avg_gen_len = np.mean(prediction_lens)\n",
    "\n",
    "    # Return rounded results\n",
    "    return {\n",
    "        \"bleu\": round(bleu_score, 4),\n",
    "        \"chrf\": round(chrf_score, 4),\n",
    "        \"gen_len\": round(avg_gen_len, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, Adafactor\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    scale_parameter=True, \n",
    "    relative_step=False, \n",
    "    warmup_init=False, \n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "model, optimizer = accelerator.prepare(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbillingsmoore\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/j/Documents/MLotsawa/Notebooks/Models/ProofOfConcepts/RAT/wandb/run-20250104_192757-wggx3at3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/billingsmoore/huggingface/runs/wggx3at3' target=\"_blank\">rat-poc-single-context</a></strong> to <a href='https://wandb.ai/billingsmoore/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/billingsmoore/huggingface' target=\"_blank\">https://wandb.ai/billingsmoore/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/billingsmoore/huggingface/runs/wggx3at3' target=\"_blank\">https://wandb.ai/billingsmoore/huggingface/runs/wggx3at3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1767e4ee14fe42b9a0400720976249b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc41cd67982e4500b81aecc019b01f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6562, 'grad_norm': 0.2886229455471039, 'learning_rate': 0.0002955555555555555, 'epoch': 0.04}\n",
      "{'loss': 0.5483, 'grad_norm': 0.29981422424316406, 'learning_rate': 0.0002911111111111111, 'epoch': 0.09}\n",
      "{'loss': 0.4934, 'grad_norm': 0.20636606216430664, 'learning_rate': 0.0002866666666666667, 'epoch': 0.13}\n",
      "{'loss': 0.5085, 'grad_norm': 0.23460017144680023, 'learning_rate': 0.00028222222222222223, 'epoch': 0.18}\n",
      "{'loss': 0.4873, 'grad_norm': 0.34310856461524963, 'learning_rate': 0.0002777777777777778, 'epoch': 0.22}\n",
      "{'loss': 0.4961, 'grad_norm': 0.11381765455007553, 'learning_rate': 0.00027333333333333333, 'epoch': 0.27}\n",
      "{'loss': 0.4859, 'grad_norm': 0.20495426654815674, 'learning_rate': 0.0002688888888888889, 'epoch': 0.31}\n",
      "{'loss': 0.4955, 'grad_norm': 0.2735089063644409, 'learning_rate': 0.00026444444444444443, 'epoch': 0.36}\n",
      "{'loss': 0.4901, 'grad_norm': 0.4512806832790375, 'learning_rate': 0.00026, 'epoch': 0.4}\n",
      "{'loss': 0.5071, 'grad_norm': 0.4021315276622772, 'learning_rate': 0.00025555555555555553, 'epoch': 0.44}\n",
      "{'loss': 0.4936, 'grad_norm': 0.2002948671579361, 'learning_rate': 0.0002511111111111111, 'epoch': 0.49}\n",
      "{'loss': 0.4976, 'grad_norm': 0.5703001618385315, 'learning_rate': 0.0002466666666666666, 'epoch': 0.53}\n",
      "{'loss': 0.4813, 'grad_norm': 0.4269771873950958, 'learning_rate': 0.0002422222222222222, 'epoch': 0.58}\n",
      "{'loss': 0.464, 'grad_norm': 0.2679884433746338, 'learning_rate': 0.00023777777777777775, 'epoch': 0.62}\n",
      "{'loss': 0.4832, 'grad_norm': 0.273825466632843, 'learning_rate': 0.0002333333333333333, 'epoch': 0.67}\n",
      "{'loss': 0.4804, 'grad_norm': 0.10759934037923813, 'learning_rate': 0.00022888888888888885, 'epoch': 0.71}\n",
      "{'loss': 0.4973, 'grad_norm': 0.32570162415504456, 'learning_rate': 0.0002244444444444444, 'epoch': 0.76}\n",
      "{'loss': 0.4857, 'grad_norm': 0.36924123764038086, 'learning_rate': 0.00021999999999999995, 'epoch': 0.8}\n",
      "{'loss': 0.4562, 'grad_norm': 0.24751496315002441, 'learning_rate': 0.00021555555555555556, 'epoch': 0.84}\n",
      "{'loss': 0.4545, 'grad_norm': 0.12360823899507523, 'learning_rate': 0.0002111111111111111, 'epoch': 0.89}\n",
      "{'loss': 0.475, 'grad_norm': 0.3696591854095459, 'learning_rate': 0.00020666666666666666, 'epoch': 0.93}\n",
      "{'loss': 0.4719, 'grad_norm': 0.24821706116199493, 'learning_rate': 0.0002022222222222222, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559ec2b35d8946eca6bc637de09b8f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46073564887046814, 'eval_bleu': 0.037, 'eval_chrf': 5.4949, 'eval_gen_len': 17.2704, 'eval_runtime': 184.8403, 'eval_samples_per_second': 27.05, 'eval_steps_per_second': 3.381, 'epoch': 1.0}\n",
      "{'loss': 0.4623, 'grad_norm': 0.15039609372615814, 'learning_rate': 0.00019777777777777776, 'epoch': 1.02}\n",
      "{'loss': 0.4741, 'grad_norm': 0.24750585854053497, 'learning_rate': 0.00019333333333333333, 'epoch': 1.07}\n",
      "{'loss': 0.4664, 'grad_norm': 0.15490373969078064, 'learning_rate': 0.00018888888888888888, 'epoch': 1.11}\n",
      "{'loss': 0.4298, 'grad_norm': 0.30336010456085205, 'learning_rate': 0.00018444444444444443, 'epoch': 1.16}\n",
      "{'loss': 0.4514, 'grad_norm': 0.15125642716884613, 'learning_rate': 0.00017999999999999998, 'epoch': 1.2}\n",
      "{'loss': 0.4639, 'grad_norm': 0.30076542496681213, 'learning_rate': 0.00017555555555555553, 'epoch': 1.24}\n",
      "{'loss': 0.4461, 'grad_norm': 0.12467702478170395, 'learning_rate': 0.0001711111111111111, 'epoch': 1.29}\n",
      "{'loss': 0.4525, 'grad_norm': 0.3839297890663147, 'learning_rate': 0.00016666666666666666, 'epoch': 1.33}\n",
      "{'loss': 0.4742, 'grad_norm': 0.15039484202861786, 'learning_rate': 0.0001622222222222222, 'epoch': 1.38}\n",
      "{'loss': 0.4632, 'grad_norm': 0.17816826701164246, 'learning_rate': 0.00015777777777777776, 'epoch': 1.42}\n",
      "{'loss': 0.4562, 'grad_norm': 0.19187544286251068, 'learning_rate': 0.0001533333333333333, 'epoch': 1.47}\n",
      "{'loss': 0.4729, 'grad_norm': 0.14996923506259918, 'learning_rate': 0.00014888888888888886, 'epoch': 1.51}\n",
      "{'loss': 0.4613, 'grad_norm': 0.2417820394039154, 'learning_rate': 0.0001444444444444444, 'epoch': 1.56}\n",
      "{'loss': 0.4626, 'grad_norm': 0.17137417197227478, 'learning_rate': 0.00014, 'epoch': 1.6}\n",
      "{'loss': 0.469, 'grad_norm': 0.16489818692207336, 'learning_rate': 0.00013555555555555554, 'epoch': 1.64}\n",
      "{'loss': 0.4659, 'grad_norm': 0.16705071926116943, 'learning_rate': 0.00013111111111111111, 'epoch': 1.69}\n",
      "{'loss': 0.4714, 'grad_norm': 0.2589819133281708, 'learning_rate': 0.00012666666666666666, 'epoch': 1.73}\n",
      "{'loss': 0.4601, 'grad_norm': 0.16303054988384247, 'learning_rate': 0.00012222222222222221, 'epoch': 1.78}\n",
      "{'loss': 0.4629, 'grad_norm': 0.33883410692214966, 'learning_rate': 0.00011777777777777776, 'epoch': 1.82}\n",
      "{'loss': 0.4659, 'grad_norm': 0.2507806420326233, 'learning_rate': 0.00011333333333333331, 'epoch': 1.87}\n",
      "{'loss': 0.4563, 'grad_norm': 0.24283352494239807, 'learning_rate': 0.00010888888888888889, 'epoch': 1.91}\n",
      "{'loss': 0.4909, 'grad_norm': 0.20071126520633698, 'learning_rate': 0.00010444444444444444, 'epoch': 1.96}\n",
      "{'loss': 0.4414, 'grad_norm': 0.1738353818655014, 'learning_rate': 9.999999999999999e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9db16ff0ef492ea6371f5ee9dd2ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.45134130120277405, 'eval_bleu': 0.0281, 'eval_chrf': 5.3487, 'eval_gen_len': 17.4444, 'eval_runtime': 184.8085, 'eval_samples_per_second': 27.055, 'eval_steps_per_second': 3.382, 'epoch': 2.0}\n",
      "{'loss': 0.457, 'grad_norm': 0.23814740777015686, 'learning_rate': 9.555555555555555e-05, 'epoch': 2.04}\n",
      "{'loss': 0.4631, 'grad_norm': 0.23814186453819275, 'learning_rate': 9.11111111111111e-05, 'epoch': 2.09}\n",
      "{'loss': 0.4657, 'grad_norm': 0.18390421569347382, 'learning_rate': 8.666666666666665e-05, 'epoch': 2.13}\n",
      "{'loss': 0.4441, 'grad_norm': 0.28060999512672424, 'learning_rate': 8.222222222222222e-05, 'epoch': 2.18}\n",
      "{'loss': 0.4681, 'grad_norm': 0.1278783231973648, 'learning_rate': 7.777777777777777e-05, 'epoch': 2.22}\n",
      "{'loss': 0.4468, 'grad_norm': 0.17634181678295135, 'learning_rate': 7.333333333333332e-05, 'epoch': 2.27}\n",
      "{'loss': 0.4474, 'grad_norm': 0.38656339049339294, 'learning_rate': 6.888888888888888e-05, 'epoch': 2.31}\n",
      "{'loss': 0.4585, 'grad_norm': 0.13152901828289032, 'learning_rate': 6.444444444444444e-05, 'epoch': 2.36}\n",
      "{'loss': 0.4526, 'grad_norm': 0.2668720483779907, 'learning_rate': 5.9999999999999995e-05, 'epoch': 2.4}\n",
      "{'loss': 0.4473, 'grad_norm': 0.32671523094177246, 'learning_rate': 5.5555555555555545e-05, 'epoch': 2.44}\n",
      "{'loss': 0.4409, 'grad_norm': 0.22178351879119873, 'learning_rate': 5.111111111111111e-05, 'epoch': 2.49}\n",
      "{'loss': 0.4542, 'grad_norm': 0.38679051399230957, 'learning_rate': 4.6666666666666665e-05, 'epoch': 2.53}\n",
      "{'loss': 0.4359, 'grad_norm': 0.12758201360702515, 'learning_rate': 4.222222222222222e-05, 'epoch': 2.58}\n",
      "{'loss': 0.444, 'grad_norm': 0.22205981612205505, 'learning_rate': 3.777777777777777e-05, 'epoch': 2.62}\n",
      "{'loss': 0.4327, 'grad_norm': 0.14950844645500183, 'learning_rate': 3.333333333333333e-05, 'epoch': 2.67}\n",
      "{'loss': 0.4439, 'grad_norm': 0.14400966465473175, 'learning_rate': 2.8888888888888885e-05, 'epoch': 2.71}\n",
      "{'loss': 0.456, 'grad_norm': 0.12962773442268372, 'learning_rate': 2.4444444444444445e-05, 'epoch': 2.76}\n",
      "{'loss': 0.4577, 'grad_norm': 0.292339026927948, 'learning_rate': 1.9999999999999998e-05, 'epoch': 2.8}\n",
      "{'loss': 0.4709, 'grad_norm': 0.2332707792520523, 'learning_rate': 1.5555555555555555e-05, 'epoch': 2.84}\n",
      "{'loss': 0.4507, 'grad_norm': 0.1268654614686966, 'learning_rate': 1.111111111111111e-05, 'epoch': 2.89}\n",
      "{'loss': 0.4559, 'grad_norm': 0.17496943473815918, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.93}\n",
      "{'loss': 0.4515, 'grad_norm': 0.4313948154449463, 'learning_rate': 2.222222222222222e-06, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4604aaa1c64515801b0527045e6dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44861406087875366, 'eval_bleu': 0.0369, 'eval_chrf': 5.56, 'eval_gen_len': 17.274, 'eval_runtime': 183.7488, 'eval_samples_per_second': 27.211, 'eval_steps_per_second': 3.401, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10777.0717, 'train_samples_per_second': 12.527, 'train_steps_per_second': 3.132, 'train_loss': 0.46936164731626157, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=33750, training_loss=0.46936164731626157, metrics={'train_runtime': 10777.0717, 'train_samples_per_second': 12.527, 'train_steps_per_second': 3.132, 'total_flos': 1.827114319872e+16, 'train_loss': 0.46936164731626157, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"rat-poc-single-context\",\n",
    "    auto_find_batch_size=True,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, #check this\n",
    "    push_to_hub=False,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_with_contexts['train'],\n",
    "    eval_dataset=dataset_with_contexts['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
