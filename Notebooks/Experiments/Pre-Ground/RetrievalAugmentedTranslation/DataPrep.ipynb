{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Subset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('openpecha/tagged_cleaned_MT_v1.0.3', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition(example):\n",
    "    return example['Tag'] != ''\n",
    "\n",
    "ds = ds.filter(condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the random subset\n",
    "subset_size = 50_000\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffled_ds = ds.shuffle(seed=0)  # Use a fixed seed for reproducibility\n",
    "\n",
    "# Select the first `subset_size` examples\n",
    "random_subset = shuffled_ds.select(range(subset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_subset = random_subset.train_test_split(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_subset.save_to_disk('rat-poc-ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Contexts\n",
    "\n",
    "Add similar sentences as context to mimic retrieval augmentation. The context for both train and eval come from the train set to mimic having a set of contexts from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk('rat-poc-ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load pre-trained embedding model\n",
    "embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # Or a domain-specific model\n",
    "\n",
    "# Encode all source sentences into vectors\n",
    "sentences = ds['train']['Target']\n",
    "embeddings = embedding_model.encode(sentences, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexts for the first sentence: [tensor(34123, device='cuda:0'), tensor(18495, device='cuda:0'), tensor(34931, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def get_top_n_contexts(source_idx, source_embeddings, n=3):\n",
    "    # Get the embedding of the target sentence\n",
    "    query_embedding = source_embeddings[source_idx]\n",
    "\n",
    "    # Compute cosine similarities with all other embeddings\n",
    "    similarities = cosine_similarity(query_embedding.unsqueeze(0), source_embeddings)\n",
    "\n",
    "    # Get the indices of the top-N most similar sentences (excluding itself)\n",
    "    top_n_indices = torch.topk(similarities, n + 1).indices[1:]  # Skip the first (self)\n",
    "    \n",
    "    # Retrieve the corresponding sentences\n",
    "    return list(top_n_indices)\n",
    "\n",
    "# Example usage for the first sentence\n",
    "context_idxs = get_top_n_contexts(0, embeddings, n=3)\n",
    "print(\"Contexts for the first sentence:\", context_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add contexts to each example using `dataset.map`\n",
    "def add_contexts(example, idx):\n",
    "    # Call get_top_n_contexts with only necessary arguments\n",
    "    context_idxs = get_top_n_contexts(idx, embeddings, n=3)\n",
    "    \n",
    "    # Construct the context by accessing the dataset using the indices\n",
    "    example[\"context\"] = [\n",
    "        ds['train'][int(context_idx)]['Source'] + ' -> ' + ds['train'][int(context_idx)]['Target']\n",
    "        for context_idx in context_idxs\n",
    "    ]\n",
    "    return example\n",
    "\n",
    "# Apply the mapping\n",
    "dataset_with_contexts = ds.map(\n",
    "    add_contexts, \n",
    "    with_indices=True,  # Pass the index to `add_contexts`\n",
    "    batched=False  # Process one example at a time\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Source': 'འཇིག་ལས་འདས་པའི་གང་འདུལ་ལོ།།', 'Target': 'Taming with transcendent beings.', 'File_Name': 'TM3076', 'Machine Aligned': False, '__index_level_0__': 1176089, 'Tag': 'Intrinsic Existence, Conventional Existence', 'context': ['འགྲོ་ཀུན་སྒྲིབ་པ་གཉིས་སྤངས་ཏེ།\\xa0། -> May all beings conquer the two obscurations', 'དགེ་བས་མཁའ་མཉམ་ལུས་ཅན་མ་ལུས་པ།། ཐེག་མཆོག་གོ་གྱོན་ཤེས་རབ་མཚོན་ཐོགས་ནས།། བདུད་བཞིའི་དགྲ་སྡེ་མ་ལུས་ཀུན་བཅོམ་སྟེ།། སྐུ་གསུམ་ནོར་བུའི་ཁྲི་ལ་འཁོད་གྱུར་ཅིག། -> Through this virtue, may all embodied beings throughout space without exception, Put on the armor of the Supreme Vehicle and having raised the weapon of wisdom, May they overcome all without exception of the host of enemies which are the four demons And be set on the jeweled throne of the three bodies.', 'སྐྱེ་འགག་ཡོད་མེད་ལ་སོགས་པའི་དམིགས་པ་དང་འཛིན་པའི་ཡུལ་ལས་འདས་པའི་རིག་སྟོང་སྤྲོས་བྲལ་མཉམ་པ་ཉིད་ཀྱི་ཁོར་ཡུག་ཡིན་ཏེ། -> Phenomena therefore transcend all objects of reference and clinging, such as origin and cessation, existence and nonexistence. They are the all-pervading, aware, and empty state of equality beyond mental elaboration.']}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the new dataset\n",
    "print(dataset_with_contexts['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b4d0abdf3c4036ac1dd453265048a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b92739448348d4afb2b7ea7777e97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_with_contexts.save_to_disk('rat-poc-ds-w-context')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
