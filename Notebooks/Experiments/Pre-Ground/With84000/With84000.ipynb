{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk('../../../tibetan_english_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer, Model, and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint = \"billingsmoore/tibetan-to-english-translation\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, device_map=\"cuda:0\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of all Tibetan Unicode characters (U+0F00 to U+0FFF)\n",
    "tibetan_chars = [chr(codepoint) for codepoint in range(0x0F00, 0x0FFF)]\n",
    "\n",
    "# Add the Tibetan characters to the tokenizer's vocabulary\n",
    "new_tokens = [char for char in tibetan_chars if char not in tokenizer.get_vocab()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32407, 1024)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add new tokens to the tokenizer\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Resize model embeddings to accommodate the new vocabulary size\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'སྣ་ཚོགས་གོས་ནི་འཆང་བ་དང་།།</s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = tokenizer.encode(dataset['train'][0]['tibetan'])\n",
    "dec = tokenizer.decode(enc)\n",
    "dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "The dataset can now be tokenized for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = 'tibetan'\n",
    "target_lang = 'english'\n",
    "\n",
    "def preprocess_function(examples):\n",
    "\n",
    "    inputs = [example for example in examples[source_lang]]\n",
    "    targets = [example for example in examples[target_lang]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b65af4c4494f9697ef68ff8ff9442b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/112006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e4e7f6ac00403bae3833daa6b82c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Finally, we can train the model. Note that the optimizer used is Adafactor. This is the optimizer that is preferred for translation tasks and for the T5 model in general. The transformers api includes a built in version of Adafactor, but I define it separately here so that we can optimize it with the 'accelerate' library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, Adafactor\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    scale_parameter=True, \n",
    "    relative_step=False, \n",
    "    warmup_init=False, \n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "model, optimizer = accelerator.prepare(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57132fb8ff6546a69166bdd9a50cbccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf669c3daf5f46dabd4c06a6d55d43f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/84006 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4933, 'grad_norm': 0.5370489358901978, 'learning_rate': 0.00029821441325619596, 'epoch': 0.02}\n",
      "{'loss': 0.4026, 'grad_norm': 0.3901846706867218, 'learning_rate': 0.00029642882651239195, 'epoch': 0.04}\n",
      "{'loss': 0.3724, 'grad_norm': 0.3839608430862427, 'learning_rate': 0.00029464323976858794, 'epoch': 0.05}\n",
      "{'loss': 0.3316, 'grad_norm': 0.5529310703277588, 'learning_rate': 0.0002928576530247839, 'epoch': 0.07}\n",
      "{'loss': 0.3331, 'grad_norm': 0.288876473903656, 'learning_rate': 0.0002910720662809799, 'epoch': 0.09}\n",
      "{'loss': 0.3235, 'grad_norm': 0.36818790435791016, 'learning_rate': 0.0002892864795371759, 'epoch': 0.11}\n",
      "{'loss': 0.2949, 'grad_norm': 0.326768159866333, 'learning_rate': 0.0002875008927933719, 'epoch': 0.12}\n",
      "{'loss': 0.3075, 'grad_norm': 0.2530459761619568, 'learning_rate': 0.0002857153060495679, 'epoch': 0.14}\n",
      "{'loss': 0.285, 'grad_norm': 0.2755714952945709, 'learning_rate': 0.00028392971930576386, 'epoch': 0.16}\n",
      "{'loss': 0.2915, 'grad_norm': 0.26459965109825134, 'learning_rate': 0.00028214413256195985, 'epoch': 0.18}\n",
      "{'loss': 0.2762, 'grad_norm': 0.31409308314323425, 'learning_rate': 0.00028035854581815584, 'epoch': 0.2}\n",
      "{'loss': 0.2737, 'grad_norm': 0.34383460879325867, 'learning_rate': 0.0002785729590743518, 'epoch': 0.21}\n",
      "{'loss': 0.2706, 'grad_norm': 0.27173617482185364, 'learning_rate': 0.0002767873723305478, 'epoch': 0.23}\n",
      "{'loss': 0.2658, 'grad_norm': 0.39451053738594055, 'learning_rate': 0.0002750017855867438, 'epoch': 0.25}\n",
      "{'loss': 0.2741, 'grad_norm': 0.3191083073616028, 'learning_rate': 0.0002732161988429398, 'epoch': 0.27}\n",
      "{'loss': 0.2531, 'grad_norm': 0.3517323136329651, 'learning_rate': 0.0002714306120991357, 'epoch': 0.29}\n",
      "{'loss': 0.255, 'grad_norm': 0.33252158761024475, 'learning_rate': 0.0002696450253553317, 'epoch': 0.3}\n",
      "{'loss': 0.261, 'grad_norm': 0.15381966531276703, 'learning_rate': 0.0002678594386115277, 'epoch': 0.32}\n",
      "{'loss': 0.2548, 'grad_norm': 0.37364739179611206, 'learning_rate': 0.0002660738518677237, 'epoch': 0.34}\n",
      "{'loss': 0.2551, 'grad_norm': 0.2947228252887726, 'learning_rate': 0.00026428826512391967, 'epoch': 0.36}\n",
      "{'loss': 0.2515, 'grad_norm': 0.5241034626960754, 'learning_rate': 0.00026250267838011566, 'epoch': 0.37}\n",
      "{'loss': 0.2416, 'grad_norm': 0.22257983684539795, 'learning_rate': 0.00026071709163631165, 'epoch': 0.39}\n",
      "{'loss': 0.233, 'grad_norm': 0.31575819849967957, 'learning_rate': 0.00025893150489250764, 'epoch': 0.41}\n",
      "{'loss': 0.2443, 'grad_norm': 0.24340589344501495, 'learning_rate': 0.0002571459181487036, 'epoch': 0.43}\n",
      "{'loss': 0.2387, 'grad_norm': 0.3639555871486664, 'learning_rate': 0.0002553603314048996, 'epoch': 0.45}\n",
      "{'loss': 0.2431, 'grad_norm': 0.17406825721263885, 'learning_rate': 0.0002535747446610956, 'epoch': 0.46}\n",
      "{'loss': 0.2465, 'grad_norm': 0.24814675748348236, 'learning_rate': 0.0002517891579172916, 'epoch': 0.48}\n",
      "{'loss': 0.2326, 'grad_norm': 0.4101546108722687, 'learning_rate': 0.0002500035711734876, 'epoch': 0.5}\n",
      "{'loss': 0.236, 'grad_norm': 0.3225639760494232, 'learning_rate': 0.00024821798442968356, 'epoch': 0.52}\n",
      "{'loss': 0.218, 'grad_norm': 0.338622510433197, 'learning_rate': 0.00024643239768587955, 'epoch': 0.54}\n",
      "{'loss': 0.2394, 'grad_norm': 0.22522884607315063, 'learning_rate': 0.00024464681094207554, 'epoch': 0.55}\n",
      "{'loss': 0.2455, 'grad_norm': 0.4041292369365692, 'learning_rate': 0.00024286122419827152, 'epoch': 0.57}\n",
      "{'loss': 0.2303, 'grad_norm': 0.7296491265296936, 'learning_rate': 0.0002410756374544675, 'epoch': 0.59}\n",
      "{'loss': 0.2514, 'grad_norm': 0.23670753836631775, 'learning_rate': 0.0002392900507106635, 'epoch': 0.61}\n",
      "{'loss': 0.2315, 'grad_norm': 0.24897480010986328, 'learning_rate': 0.0002375044639668595, 'epoch': 0.62}\n",
      "{'loss': 0.2297, 'grad_norm': 0.4146791100502014, 'learning_rate': 0.00023571887722305548, 'epoch': 0.64}\n",
      "{'loss': 0.2283, 'grad_norm': 0.29859739542007446, 'learning_rate': 0.00023393329047925146, 'epoch': 0.66}\n",
      "{'loss': 0.2229, 'grad_norm': 0.2608974277973175, 'learning_rate': 0.00023214770373544745, 'epoch': 0.68}\n",
      "{'loss': 0.2252, 'grad_norm': 0.19016113877296448, 'learning_rate': 0.00023036211699164344, 'epoch': 0.7}\n",
      "{'loss': 0.2242, 'grad_norm': 0.23184195160865784, 'learning_rate': 0.00022857653024783943, 'epoch': 0.71}\n",
      "{'loss': 0.23, 'grad_norm': 0.13937188684940338, 'learning_rate': 0.00022679094350403541, 'epoch': 0.73}\n",
      "{'loss': 0.226, 'grad_norm': 0.33067673444747925, 'learning_rate': 0.0002250053567602314, 'epoch': 0.75}\n",
      "{'loss': 0.2223, 'grad_norm': 0.29543590545654297, 'learning_rate': 0.0002232197700164274, 'epoch': 0.77}\n",
      "{'loss': 0.2074, 'grad_norm': 0.24948760867118835, 'learning_rate': 0.00022143418327262338, 'epoch': 0.79}\n",
      "{'loss': 0.2264, 'grad_norm': 0.24265693128108978, 'learning_rate': 0.00021964859652881936, 'epoch': 0.8}\n",
      "{'loss': 0.2208, 'grad_norm': 0.2689480185508728, 'learning_rate': 0.00021786300978501535, 'epoch': 0.82}\n",
      "{'loss': 0.2171, 'grad_norm': 0.49818655848503113, 'learning_rate': 0.00021607742304121134, 'epoch': 0.84}\n",
      "{'loss': 0.2209, 'grad_norm': 0.2969876229763031, 'learning_rate': 0.0002142918362974073, 'epoch': 0.86}\n",
      "{'loss': 0.2139, 'grad_norm': 0.34257254004478455, 'learning_rate': 0.0002125062495536033, 'epoch': 0.87}\n",
      "{'loss': 0.2236, 'grad_norm': 0.2040865421295166, 'learning_rate': 0.00021072066280979928, 'epoch': 0.89}\n",
      "{'loss': 0.2226, 'grad_norm': 0.1445813775062561, 'learning_rate': 0.00020893507606599526, 'epoch': 0.91}\n",
      "{'loss': 0.2201, 'grad_norm': 0.2072812020778656, 'learning_rate': 0.00020714948932219125, 'epoch': 0.93}\n",
      "{'loss': 0.2217, 'grad_norm': 0.23883052170276642, 'learning_rate': 0.00020536390257838724, 'epoch': 0.95}\n",
      "{'loss': 0.2223, 'grad_norm': 0.20970259606838226, 'learning_rate': 0.00020357831583458323, 'epoch': 0.96}\n",
      "{'loss': 0.2182, 'grad_norm': 0.1753452569246292, 'learning_rate': 0.00020179272909077921, 'epoch': 0.98}\n",
      "{'loss': 0.2089, 'grad_norm': 0.2766208350658417, 'learning_rate': 0.0002000071423469752, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Desktop/MLotsawa/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7792a0eeda47fea2f15e2423c24e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1556 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1986752450466156, 'eval_bleu': 8.2157, 'eval_gen_len': 17.8406, 'eval_runtime': 1739.7393, 'eval_samples_per_second': 7.154, 'eval_steps_per_second': 0.894, 'epoch': 1.0}\n",
      "{'loss': 0.2001, 'grad_norm': 0.6920514106750488, 'learning_rate': 0.0001982215556031712, 'epoch': 1.02}\n",
      "{'loss': 0.1998, 'grad_norm': 0.32884547114372253, 'learning_rate': 0.00019643596885936718, 'epoch': 1.04}\n",
      "{'loss': 0.1909, 'grad_norm': 0.30329251289367676, 'learning_rate': 0.00019465038211556316, 'epoch': 1.05}\n",
      "{'loss': 0.208, 'grad_norm': 0.37133264541625977, 'learning_rate': 0.00019286479537175915, 'epoch': 1.07}\n",
      "{'loss': 0.206, 'grad_norm': 0.2181055098772049, 'learning_rate': 0.00019107920862795514, 'epoch': 1.09}\n",
      "{'loss': 0.199, 'grad_norm': 0.14069849252700806, 'learning_rate': 0.00018929362188415113, 'epoch': 1.11}\n",
      "{'loss': 0.1949, 'grad_norm': 0.15009649097919464, 'learning_rate': 0.0001875080351403471, 'epoch': 1.12}\n",
      "{'loss': 0.2005, 'grad_norm': 0.3371492624282837, 'learning_rate': 0.00018572244839654308, 'epoch': 1.14}\n",
      "{'loss': 0.1893, 'grad_norm': 0.38276517391204834, 'learning_rate': 0.00018393686165273906, 'epoch': 1.16}\n",
      "{'loss': 0.1965, 'grad_norm': 0.2935982644557953, 'learning_rate': 0.00018215127490893505, 'epoch': 1.18}\n",
      "{'loss': 0.1899, 'grad_norm': 0.20053859055042267, 'learning_rate': 0.00018036568816513104, 'epoch': 1.2}\n",
      "{'loss': 0.1995, 'grad_norm': 0.21363550424575806, 'learning_rate': 0.00017858010142132703, 'epoch': 1.21}\n",
      "{'loss': 0.1909, 'grad_norm': 0.5737577676773071, 'learning_rate': 0.00017679451467752301, 'epoch': 1.23}\n",
      "{'loss': 0.1968, 'grad_norm': 0.11526323109865189, 'learning_rate': 0.000175008927933719, 'epoch': 1.25}\n",
      "{'loss': 0.1951, 'grad_norm': 0.15078948438167572, 'learning_rate': 0.000173223341189915, 'epoch': 1.27}\n",
      "{'loss': 0.1958, 'grad_norm': 0.4088202714920044, 'learning_rate': 0.00017143775444611098, 'epoch': 1.29}\n",
      "{'loss': 0.2087, 'grad_norm': 0.23674318194389343, 'learning_rate': 0.00016965216770230696, 'epoch': 1.3}\n",
      "{'loss': 0.1852, 'grad_norm': 0.13624317944049835, 'learning_rate': 0.00016786658095850295, 'epoch': 1.32}\n",
      "{'loss': 0.1976, 'grad_norm': 0.32427090406417847, 'learning_rate': 0.00016608099421469894, 'epoch': 1.34}\n",
      "{'loss': 0.1943, 'grad_norm': 0.272845059633255, 'learning_rate': 0.00016429540747089493, 'epoch': 1.36}\n",
      "{'loss': 0.1872, 'grad_norm': 0.2322627454996109, 'learning_rate': 0.00016250982072709092, 'epoch': 1.37}\n",
      "{'loss': 0.1893, 'grad_norm': 0.28097519278526306, 'learning_rate': 0.0001607242339832869, 'epoch': 1.39}\n",
      "{'loss': 0.1898, 'grad_norm': 0.19832992553710938, 'learning_rate': 0.00015893864723948286, 'epoch': 1.41}\n",
      "{'loss': 0.1925, 'grad_norm': 0.28080442547798157, 'learning_rate': 0.00015715306049567885, 'epoch': 1.43}\n",
      "{'loss': 0.1948, 'grad_norm': 0.3636437654495239, 'learning_rate': 0.00015536747375187484, 'epoch': 1.45}\n",
      "{'loss': 0.1868, 'grad_norm': 0.16799916326999664, 'learning_rate': 0.00015358188700807083, 'epoch': 1.46}\n",
      "{'loss': 0.1822, 'grad_norm': 0.5333071947097778, 'learning_rate': 0.00015179630026426681, 'epoch': 1.48}\n",
      "{'loss': 0.1876, 'grad_norm': 0.17681698501110077, 'learning_rate': 0.0001500107135204628, 'epoch': 1.5}\n",
      "{'loss': 0.2021, 'grad_norm': 0.1313200443983078, 'learning_rate': 0.00014822512677665882, 'epoch': 1.52}\n",
      "{'loss': 0.195, 'grad_norm': 0.1990492045879364, 'learning_rate': 0.00014643954003285478, 'epoch': 1.54}\n",
      "{'loss': 0.181, 'grad_norm': 0.15826912224292755, 'learning_rate': 0.00014465395328905076, 'epoch': 1.55}\n",
      "{'loss': 0.1825, 'grad_norm': 0.3651161193847656, 'learning_rate': 0.00014286836654524675, 'epoch': 1.57}\n",
      "{'loss': 0.2009, 'grad_norm': 0.23787853121757507, 'learning_rate': 0.00014108277980144274, 'epoch': 1.59}\n",
      "{'loss': 0.1921, 'grad_norm': 0.21041694283485413, 'learning_rate': 0.00013929719305763873, 'epoch': 1.61}\n",
      "{'loss': 0.1947, 'grad_norm': 0.26768702268600464, 'learning_rate': 0.00013751160631383472, 'epoch': 1.62}\n",
      "{'loss': 0.1939, 'grad_norm': 0.14954805374145508, 'learning_rate': 0.0001357260195700307, 'epoch': 1.64}\n",
      "{'loss': 0.1936, 'grad_norm': 0.2892279624938965, 'learning_rate': 0.0001339404328262267, 'epoch': 1.66}\n",
      "{'loss': 0.1815, 'grad_norm': 0.1904640793800354, 'learning_rate': 0.00013215484608242268, 'epoch': 1.68}\n",
      "{'loss': 0.1844, 'grad_norm': 0.28577861189842224, 'learning_rate': 0.00013036925933861867, 'epoch': 1.7}\n",
      "{'loss': 0.1928, 'grad_norm': 0.20372892916202545, 'learning_rate': 0.00012858367259481465, 'epoch': 1.71}\n",
      "{'loss': 0.1853, 'grad_norm': 0.17032966017723083, 'learning_rate': 0.00012679808585101064, 'epoch': 1.73}\n",
      "{'loss': 0.1828, 'grad_norm': 0.24696201086044312, 'learning_rate': 0.00012501249910720663, 'epoch': 1.75}\n",
      "{'loss': 0.1847, 'grad_norm': 0.5039744973182678, 'learning_rate': 0.00012322691236340262, 'epoch': 1.77}\n",
      "{'loss': 0.1901, 'grad_norm': 0.8165203332901001, 'learning_rate': 0.00012144132561959859, 'epoch': 1.79}\n",
      "{'loss': 0.1892, 'grad_norm': 0.2399917095899582, 'learning_rate': 0.00011965573887579458, 'epoch': 1.8}\n",
      "{'loss': 0.1931, 'grad_norm': 0.3083426356315613, 'learning_rate': 0.00011787015213199057, 'epoch': 1.82}\n",
      "{'loss': 0.181, 'grad_norm': 0.193948894739151, 'learning_rate': 0.00011608456538818654, 'epoch': 1.84}\n",
      "{'loss': 0.1837, 'grad_norm': 5.623788356781006, 'learning_rate': 0.00011429897864438253, 'epoch': 1.86}\n",
      "{'loss': 0.1866, 'grad_norm': 0.1691494733095169, 'learning_rate': 0.00011251339190057852, 'epoch': 1.87}\n",
      "{'loss': 0.1901, 'grad_norm': 0.04948817193508148, 'learning_rate': 0.0001107278051567745, 'epoch': 1.89}\n",
      "{'loss': 0.1874, 'grad_norm': 0.19023704528808594, 'learning_rate': 0.00010894221841297049, 'epoch': 1.91}\n",
      "{'loss': 0.1845, 'grad_norm': 0.4596047103404999, 'learning_rate': 0.00010715663166916648, 'epoch': 1.93}\n",
      "{'loss': 0.1806, 'grad_norm': 0.29536888003349304, 'learning_rate': 0.00010537104492536247, 'epoch': 1.95}\n",
      "{'loss': 0.1778, 'grad_norm': 0.16791273653507233, 'learning_rate': 0.00010358545818155845, 'epoch': 1.96}\n",
      "{'loss': 0.1866, 'grad_norm': 0.3580365777015686, 'learning_rate': 0.00010179987143775444, 'epoch': 1.98}\n",
      "{'loss': 0.1769, 'grad_norm': 0.19137346744537354, 'learning_rate': 0.00010001428469395043, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Desktop/MLotsawa/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e148900fb22445bbce3b6d1e2bf744e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1556 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18215888738632202, 'eval_bleu': 8.7419, 'eval_gen_len': 17.7906, 'eval_runtime': 1730.5984, 'eval_samples_per_second': 7.192, 'eval_steps_per_second': 0.899, 'epoch': 2.0}\n",
      "{'loss': 0.1713, 'grad_norm': 0.15688832104206085, 'learning_rate': 9.822869795014642e-05, 'epoch': 2.02}\n",
      "{'loss': 0.1682, 'grad_norm': 0.2840486764907837, 'learning_rate': 9.64431112063424e-05, 'epoch': 2.04}\n",
      "{'loss': 0.1713, 'grad_norm': 0.09711557626724243, 'learning_rate': 9.465752446253839e-05, 'epoch': 2.05}\n",
      "{'loss': 0.1671, 'grad_norm': 0.1733408123254776, 'learning_rate': 9.287193771873438e-05, 'epoch': 2.07}\n",
      "{'loss': 0.1802, 'grad_norm': 0.30460530519485474, 'learning_rate': 9.108635097493037e-05, 'epoch': 2.09}\n",
      "{'loss': 0.1609, 'grad_norm': 0.5159547328948975, 'learning_rate': 8.930076423112634e-05, 'epoch': 2.11}\n",
      "{'loss': 0.1733, 'grad_norm': 0.20846009254455566, 'learning_rate': 8.751517748732232e-05, 'epoch': 2.12}\n",
      "{'loss': 0.1729, 'grad_norm': 0.29432353377342224, 'learning_rate': 8.57295907435183e-05, 'epoch': 2.14}\n",
      "{'loss': 0.1644, 'grad_norm': 0.12221895158290863, 'learning_rate': 8.394400399971429e-05, 'epoch': 2.16}\n",
      "{'loss': 0.1658, 'grad_norm': 0.32811203598976135, 'learning_rate': 8.215841725591028e-05, 'epoch': 2.18}\n",
      "{'loss': 0.1652, 'grad_norm': 0.215159073472023, 'learning_rate': 8.037283051210627e-05, 'epoch': 2.2}\n",
      "{'loss': 0.1712, 'grad_norm': 0.5347215533256531, 'learning_rate': 7.858724376830225e-05, 'epoch': 2.21}\n",
      "{'loss': 0.1711, 'grad_norm': 0.325228750705719, 'learning_rate': 7.680165702449824e-05, 'epoch': 2.23}\n",
      "{'loss': 0.1699, 'grad_norm': 0.4489283859729767, 'learning_rate': 7.501607028069423e-05, 'epoch': 2.25}\n",
      "{'loss': 0.1649, 'grad_norm': 0.2509627342224121, 'learning_rate': 7.323048353689022e-05, 'epoch': 2.27}\n",
      "{'loss': 0.1716, 'grad_norm': 0.19610990583896637, 'learning_rate': 7.14448967930862e-05, 'epoch': 2.29}\n",
      "{'loss': 0.1785, 'grad_norm': 0.21191908419132233, 'learning_rate': 6.965931004928219e-05, 'epoch': 2.3}\n",
      "{'loss': 0.1677, 'grad_norm': 0.2573277950286865, 'learning_rate': 6.787372330547818e-05, 'epoch': 2.32}\n",
      "{'loss': 0.1698, 'grad_norm': 0.23898670077323914, 'learning_rate': 6.608813656167415e-05, 'epoch': 2.34}\n",
      "{'loss': 0.1786, 'grad_norm': 0.3083898723125458, 'learning_rate': 6.430254981787014e-05, 'epoch': 2.36}\n",
      "{'loss': 0.1707, 'grad_norm': 0.6429418921470642, 'learning_rate': 6.251696307406613e-05, 'epoch': 2.37}\n",
      "{'loss': 0.1769, 'grad_norm': 0.22211255133152008, 'learning_rate': 6.073137633026212e-05, 'epoch': 2.39}\n",
      "{'loss': 0.1797, 'grad_norm': 0.12483977526426315, 'learning_rate': 5.8945789586458105e-05, 'epoch': 2.41}\n",
      "{'loss': 0.1693, 'grad_norm': 0.2533513605594635, 'learning_rate': 5.716020284265409e-05, 'epoch': 2.43}\n",
      "{'loss': 0.1659, 'grad_norm': 0.174701526761055, 'learning_rate': 5.537461609885008e-05, 'epoch': 2.45}\n",
      "{'loss': 0.1735, 'grad_norm': 0.5434771180152893, 'learning_rate': 5.358902935504607e-05, 'epoch': 2.46}\n",
      "{'loss': 0.1715, 'grad_norm': 0.42363423109054565, 'learning_rate': 5.180344261124205e-05, 'epoch': 2.48}\n",
      "{'loss': 0.1634, 'grad_norm': 0.14056432247161865, 'learning_rate': 5.0017855867438037e-05, 'epoch': 2.5}\n",
      "{'loss': 0.171, 'grad_norm': 0.3460724651813507, 'learning_rate': 4.8232269123634024e-05, 'epoch': 2.52}\n",
      "{'loss': 0.1758, 'grad_norm': 0.1865750551223755, 'learning_rate': 4.6446682379830005e-05, 'epoch': 2.54}\n",
      "{'loss': 0.1713, 'grad_norm': 0.262044757604599, 'learning_rate': 4.466109563602599e-05, 'epoch': 2.55}\n",
      "{'loss': 0.174, 'grad_norm': 0.3134578764438629, 'learning_rate': 4.287550889222198e-05, 'epoch': 2.57}\n",
      "{'loss': 0.1704, 'grad_norm': 0.10248393565416336, 'learning_rate': 4.108992214841797e-05, 'epoch': 2.59}\n",
      "{'loss': 0.1662, 'grad_norm': 0.14305546879768372, 'learning_rate': 3.930433540461395e-05, 'epoch': 2.61}\n",
      "{'loss': 0.1667, 'grad_norm': 0.14114192128181458, 'learning_rate': 3.751874866080994e-05, 'epoch': 2.62}\n",
      "{'loss': 0.171, 'grad_norm': 0.12454444915056229, 'learning_rate': 3.5733161917005924e-05, 'epoch': 2.64}\n",
      "{'loss': 0.17, 'grad_norm': 0.3265804052352905, 'learning_rate': 3.394757517320191e-05, 'epoch': 2.66}\n",
      "{'loss': 0.1667, 'grad_norm': 0.2717335522174835, 'learning_rate': 3.21619884293979e-05, 'epoch': 2.68}\n",
      "{'loss': 0.1748, 'grad_norm': 0.31747138500213623, 'learning_rate': 3.0376401685593884e-05, 'epoch': 2.7}\n",
      "{'loss': 0.1762, 'grad_norm': 0.21596072614192963, 'learning_rate': 2.8590814941789868e-05, 'epoch': 2.71}\n",
      "{'loss': 0.1725, 'grad_norm': 0.15686845779418945, 'learning_rate': 2.6805228197985856e-05, 'epoch': 2.73}\n",
      "{'loss': 0.1613, 'grad_norm': 0.3220624029636383, 'learning_rate': 2.5019641454181844e-05, 'epoch': 2.75}\n",
      "{'loss': 0.1745, 'grad_norm': 0.2451379895210266, 'learning_rate': 2.3234054710377828e-05, 'epoch': 2.77}\n",
      "{'loss': 0.1728, 'grad_norm': 0.2139904797077179, 'learning_rate': 2.1448467966573812e-05, 'epoch': 2.79}\n",
      "{'loss': 0.1819, 'grad_norm': 0.299739807844162, 'learning_rate': 1.96628812227698e-05, 'epoch': 2.8}\n",
      "{'loss': 0.1663, 'grad_norm': 0.5179578065872192, 'learning_rate': 1.7877294478965787e-05, 'epoch': 2.82}\n",
      "{'loss': 0.1721, 'grad_norm': 0.7101140022277832, 'learning_rate': 1.609170773516177e-05, 'epoch': 2.84}\n",
      "{'loss': 0.1613, 'grad_norm': 0.256963312625885, 'learning_rate': 1.430612099135776e-05, 'epoch': 2.86}\n",
      "{'loss': 0.1703, 'grad_norm': 0.20549221336841583, 'learning_rate': 1.2520534247553744e-05, 'epoch': 2.87}\n",
      "{'loss': 0.1544, 'grad_norm': 0.3311336636543274, 'learning_rate': 1.0734947503749731e-05, 'epoch': 2.89}\n",
      "{'loss': 0.1846, 'grad_norm': 0.17149075865745544, 'learning_rate': 8.949360759945717e-06, 'epoch': 2.91}\n",
      "{'loss': 0.1731, 'grad_norm': 0.14231257140636444, 'learning_rate': 7.163774016141704e-06, 'epoch': 2.93}\n",
      "{'loss': 0.175, 'grad_norm': 0.28951331973075867, 'learning_rate': 5.37818727233769e-06, 'epoch': 2.95}\n",
      "{'loss': 0.1768, 'grad_norm': 0.23377597332000732, 'learning_rate': 3.5926005285336756e-06, 'epoch': 2.96}\n",
      "{'loss': 0.1685, 'grad_norm': 0.2931998074054718, 'learning_rate': 1.8070137847296622e-06, 'epoch': 2.98}\n",
      "{'loss': 0.1685, 'grad_norm': 0.6853328943252563, 'learning_rate': 2.1427040925648166e-08, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Desktop/MLotsawa/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2541d978f0e246d49a644a5ffea33975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1556 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17730243504047394, 'eval_bleu': 9.123, 'eval_gen_len': 17.7157, 'eval_runtime': 1733.5152, 'eval_samples_per_second': 7.18, 'eval_steps_per_second': 0.898, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 64914.4181, 'train_samples_per_second': 5.176, 'train_steps_per_second': 1.294, 'train_loss': 0.20581387315418448, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=84006, training_loss=0.20581387315418448, metrics={'train_runtime': 64914.4181, 'train_samples_per_second': 5.176, 'train_steps_per_second': 1.294, 'total_flos': 3.63747431153664e+17, 'train_loss': 0.20581387315418448, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"84000\",\n",
    "    auto_find_batch_size=True,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, #check this\n",
    "    push_to_hub=False,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe50432e3d14ac3b80006902fed2825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/12446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['test'].save_to_disk('test-set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
