{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Subset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('openpecha/tagged_cleaned_MT_v1.0.3', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition(example):\n",
    "    return example['Tag'] != ''\n",
    "\n",
    "ds = ds.filter(condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the random subset\n",
    "subset_size = 50_000\n",
    "\n",
    "# Shuffle the dataset\n",
    "shuffled_ds = ds.shuffle(seed=0)  # Use a fixed seed for reproducibility\n",
    "\n",
    "# Select the first `subset_size` examples\n",
    "random_subset = shuffled_ds.select(range(subset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_subset = random_subset.train_test_split(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Source', 'Target', 'File_Name', 'Machine Aligned', '__index_level_0__', 'Tag'],\n",
       "        num_rows: 45000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Source', 'Target', 'File_Name', 'Machine Aligned', '__index_level_0__', 'Tag'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe55e362eb74a768e81673cd3daaafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83f6c2a8944472cbfb14f0ed0aa1b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_subset.save_to_disk('rat-poc-ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Contexts\n",
    "\n",
    "Add similar sentences as context to mimic retrieval augmentation. The context for both train and eval come from the train set to mimic having a set of contexts from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk('rat-poc-ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load pre-trained embedding model\n",
    "embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # Or a domain-specific model\n",
    "\n",
    "# Encode all source sentences into vectors\n",
    "sentences = ds['train']['Target']\n",
    "embeddings = embedding_model.encode(sentences, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexts for the first sentence: [tensor(23107, device='cuda:0'), tensor(21350, device='cuda:0'), tensor(2325, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def get_top_n_contexts(source_idx, source_embeddings, n=3):\n",
    "    # Get the embedding of the target sentence\n",
    "    query_embedding = source_embeddings[source_idx]\n",
    "\n",
    "    # Compute cosine similarities with all other embeddings\n",
    "    similarities = cosine_similarity(query_embedding.unsqueeze(0), source_embeddings)\n",
    "\n",
    "    # Get the indices of the top-N most similar sentences (excluding itself)\n",
    "    top_n_indices = torch.topk(similarities, n + 1).indices[1:]  # Skip the first (self)\n",
    "    \n",
    "    # Retrieve the corresponding sentences\n",
    "    return list(top_n_indices)\n",
    "\n",
    "# Example usage for the first sentence\n",
    "context_idxs = get_top_n_contexts(0, embeddings, n=3)\n",
    "print(\"Contexts for the first sentence:\", context_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d77be9f8684bae9d90bf86848a4517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741c47b510eb48b49d1db1f46782c29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add contexts to each example using `dataset.map`\n",
    "def add_contexts(example, idx):\n",
    "    # Call get_top_n_contexts with only necessary arguments\n",
    "    context_idxs = get_top_n_contexts(idx, embeddings, n=3)\n",
    "    \n",
    "    # Construct the context by accessing the dataset using the indices\n",
    "    example[\"context\"] = [\n",
    "        ds['train'][int(context_idx)]['Source'] + ' -> ' + ds['train'][int(context_idx)]['Target']\n",
    "        for context_idx in context_idxs\n",
    "    ]\n",
    "    return example\n",
    "\n",
    "# Apply the mapping\n",
    "dataset_with_contexts = ds.map(\n",
    "    add_contexts, \n",
    "    with_indices=True,  # Pass the index to `add_contexts`\n",
    "    batched=False  # Process one example at a time\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Source': 'སྒོམ། སྤྱོད། འབྲས་ཀྱི་རྣམ་གཞག་ཅུང་ཟད་བརྗོད་ན། འཇུག་སྒོ་ནི། ཕྱི་ནང་སྒྲུབ་གསང་ཆུ་བོ་བཞི་རྫོགས་ཀྱི་དབང་སུམ་ཅུ་རྩ་དྲུག་གིས་རྒྱུད་སྨིན་ཅིང་། དམ་ཚིག་གཞུང་བཞིན་སྲུང་བའོ།། གཞི་ལྟ་བ་གཏན་ལ་འབེབས་ཚུལ་ནི། ཆོས་ཐམས་ཅད་ཀྱི་གཤིས་ལུགས་དཀྱིལ་འཁོར་གསུམ་དུ་གནས་པ་ཉིད་ཀྱི་ཆོས་ཅན་ཤེས་བྱའི་དོན་ལ་ཤེས་བྱེད་རིགས་པའི་གཏན་ཚིགས་རྣམས་ཀྱི་ཇི་ལྟ་བར་གཏན་ལ་ཕབ་སྟེ་རྟོགས་པའོ།། ལམ་སྒོམ་པས་ཉམས་སུ་ལེན་ཚུལ་ནི། ཆོས་ཉིད་སྙིང་པོ་ཇི་བཞིན་པའི་ངང་ལ་མཉམ་པར་འཇོག་པ་མཚན་མེད་རྣམ་པར་མི་རྟོག་པའི་ཏིང་ངེ་འཛིན་དང་། བསྐྱེད་སྔགས་བརྗོད་པ་ཙམ་གྱིས་རྟེན་དང་བརྟེན་པའི་འཁོར་ལོར་བསྒོམ་པ་མཚན་བཅས་ལྷའི་ཏིང་འཛིན་བསྒོམ་པ་གྲོལ་ལམ་དང་། ཐབས་ལམ་སྟེང་འོག་གི་སྒོ་ལ་བརྟེན་ནས་བདེ་སྟོང་གི་ཡེ་ཤེས་བསྐྱེད་པ་སྟེ་ལམ་གཉིས་ཀྱི་རྣལ་འབྱོར་བསྒོམ་པའོ།།', 'Target': 'Once again, let us say a little about its point of entry, view, meditation, conduct and results: i. Entry Point One’s mind is matured through the thirty-six empowerments in which the four rivers—outer, inner, accomplishing and secret—are complete, and one keeps the samayas as described in the texts. ii. View Through logical reasoning one determines that which is to be known, the fact that all phenomena are characterized as being the three mandalas in their fundamental nature, and realizes that this is so. iii. Meditation Meditation practice here consists of two paths. On the path of liberation one practises the non-conceptual samādhi of simply resting in a state that accords with the essence of reality itself, and the conceptual samādhi of deity practice, in which one visualizes the mandala of supporting palace and supported deities simply by reciting the mantra of generation. On the path of skilful means one generates the wisdom of bliss and emptiness through the practices of the upper and lower gateways.', 'File_Name': None, 'Machine Aligned': True, '__index_level_0__': 294714, 'Tag': {'Buddhist': True, 'LH labels': ['Longchen Nyingtik', 'Yumka Dechen Gyalmo'], 'Topic': 'Meditation'}, 'context': ['འདི་དག་ལ་བརྟེན་ནས་བསམ་གཏན་གྱི་སེམས་བསྒྲུབ་པར་བྱའོ།། -> It is through these that the mind of meditation is attained.', '1 ཞེས་སོགས་མང་དུ་བཤད་པ་ལྟར་རོ།། དེ་ལྟར་ན་སྐབས་འདིར་གཞན་དགེ་སྦྱོར་ཅི་རིགས་ཙམ་དང་། ཁྱད་པར་བླ་མའི་རྣལ་འབྱོར་གྱི་སྐབས་སུ་བླ་མ་ལ་ཆོས་སྐུའི་མོས་གུས་བསྐྱེད་ནས་སྣང་སྲིད་ཆོས་སྐུར་རྟོགས་པར་གསོལ་བ་དྲག་ཏུ་འདེབས་པ་སྔོན་དུ་འགྲོ་བས། སྔར་བཤད་པ་ལྟར་ངོ་བོའི་གཞག་ཐབས་ཀྱི་མཉམ་བཞག་རྩེ་གཅིག་ཏུ་བསྐྱངས་ཏེ་རིག་པ་དྭངས་སྙིགས་ཕྱེད་པར་བྱས་ལ། དེ་ནས་རིག་པ་མ་ཡེངས་ཤིང་འདི་སྒོམ་གྱི་གཏད་སོ་མེད་པར་སེམས་གུ་ཡངས་སང་ངེ་བའི་ངང་ནས། ཡིད་ལ་བདེ་དོག་ཆེར་མི་འབྱུང་བའི་རྣམ་རྟོག་ཕྲ་མོ་ཤར་བའམ་ཆེད་དུ་སྤྲོས་ཏེ། དེའི་རྣམ་པ་གསལ་བཞིན་པ་འི་ངོ་བོ་ལ་འཛིན་མེད་ཡེངས་མེད་དུ་ཅེ་རེ་བལྟ། དེ་བཞིན་དུ་ཡིད་ལ་དགའ་མི་དགའི་ཟུག་རྔུ་བསྐྱེད་པའི་རྣམ་རྟོག་རགས་པ་མངོན་ཚན་ཆེ་བ་ཤར་བའམ་ཆེད་དུ་སྤྲོས་ཏེ། ཕྲ་རགས་དེ་གཉིས་ཀྱི་ངོ་བོ་ལ་ཁྱད་པར་ཅི་འདུག་ཡང་ཡང་བལྟ། དེ་ལ་སྒྲོ་འདོགས་ཆོད་ན་དེ་དག་དང་སེམས་གནས་དུས་ཀྱི་ངོ་བོ་ལ་ཁྱད་པར་ཅི་འདུག་བལྟ། -> 1 There are many such statements. At this point, during whatever spiritual practices you do and particularly during guru yoga, begin by arousing the devotion in which you see your gurus as the dharmakāya. Make fervent supplications to realize all possible appearances are the dharmakāya. Next, as described before, cultivate one-pointed equipoise, which is the means for resting in the essence, and differentiate between brilliant states of mind and sullied ones. Then, in a spacious and vibrant state of mind—when your awareness is undistracted and your meditation is without reference point—there may arise a subtle thought that doesn’t create great mental pleasure or pain, or you should cause one to arise. Without clinging or wandering, look fixedly at its essence while its manifestation is vivid. Similarly, there may arise a clearly evident coarse thought, the kind that creates the miseries associated with mental joys and sorrows, or you should cause one to arise. Look again and again to see if there is any difference between the essence of a subtle thought and that of a coarse thought. When you sever misinterpretations regarding that, look to see if there is any difference between the essence of those thoughts and the essence of the still mind.', 'གཞི་ལྟ་བ་གཏན་ལ་འབེབས་ཚུལ་ནི། སེམས་ལས་འདས་པའི་རིག་པ་རྗེན་པ་འདི་ཀའི་ངོ་བོ་སྟོང་པ་ཆོས་ཀྱི་སྐུ། རང་བཞིན་གསལ་བ་ལོངས་སྐུ། ཐུགས་རྗེ་ཀུན་ཁྱབ་སྤྲུལ་སྐུ་སྟེ། སྐུ་གསུམ་དབྱེར་མེད་པའི་རང་བྱུང་གི་ཡེ་ཤེས་ཀྱི་རང་ཞལ་ལྟ་བའོ།། ལེ་ལོ་ཅན་འབད་མེད་དུ་གྲོལ་བ་ཀ་དག་ཁྲེགས་ཆོད་ཀྱི་ལམ་དང་། བརྩོན་འགྲུས་ཅན་འབད་བཅས་སུ་གྲོལ་བ་ལྷུན་གྲུབ་ཐོད་རྒལ་གྱི་ལམ་བསྒོམ་པའོ།། ཅིར་སྣང་ཐམས་ཅད་ཆོས་ཉིད་ཀྱི་རོལ་པར་ཤར་བས་སྤང་བླང་རེ་དོགས་དང་བྲལ་བར་སྤྱོད་པའོ།། འབྲས་བུ་ཇི་ལྟར་ཐོབ་ཚུལ་ནི། ལམ་གྱི་སྣང་བ་བཞི་མཐར་ཕྱིན་ཏེ། འཇའ་ལུས་འཕོ་བ་ཆེན་པོའི་སྐུ་མཆོག་བརྙེས་ནས་དཔལ་ཀུན་ཏུ་བཟང་པོའི་གོ་འཕང་ངམ་བཅུ་གསུམ་ཡེ་ཤེས་བླ་མའི་ས་ཐོབ་པའོ།། -> View The view is definitively established by looking directly into the naturally arising wisdom in which the three kāyas are inseparable: the empty essence of naked awareness beyond the ordinary mind is the dharmakāya, its cognizant nature is the sambhogakāya, and its all-pervasive compassionate energy is the nirmāṇakāya. Meditation The meditation consists of the approach of cutting through resistance to primordial purity (kadak trekchö), through which the lazy can reach liberation without effort, and the approach of the direct realization of spontaneous presence (lhundrup tögal), through which the diligent can reach liberation with exertion. Conduct The conduct is free from hope and fear and adopting and abandoning, because all that appears manifests as the display of reality itself. v. Results Perfecting the four visions of the path, one gains the supreme kāya, the rainbow body of great transference, and attains the level of glorious Samantabhadra, the thirteenth bhūmi known as ‘Unexcelled Wisdom’ (yeshe lama). ']}\n"
     ]
    }
   ],
   "source": [
    "# Inspect the new dataset\n",
    "print(dataset_with_contexts['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2289150ce5e64ade843aaf41997cb464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3ad1cd6bee4f64ac478296b79697f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_with_contexts.save_to_disk('rat-poc-ds-w-context')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
