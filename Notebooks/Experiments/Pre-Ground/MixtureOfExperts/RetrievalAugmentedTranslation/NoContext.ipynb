{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('rat-poc-ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': 'སྒོམ། སྤྱོད། འབྲས་ཀྱི་རྣམ་གཞག་ཅུང་ཟད་བརྗོད་ན། འཇུག་སྒོ་ནི། ཕྱི་ནང་སྒྲུབ་གསང་ཆུ་བོ་བཞི་རྫོགས་ཀྱི་དབང་སུམ་ཅུ་རྩ་དྲུག་གིས་རྒྱུད་སྨིན་ཅིང་། དམ་ཚིག་གཞུང་བཞིན་སྲུང་བའོ།། གཞི་ལྟ་བ་གཏན་ལ་འབེབས་ཚུལ་ནི། ཆོས་ཐམས་ཅད་ཀྱི་གཤིས་ལུགས་དཀྱིལ་འཁོར་གསུམ་དུ་གནས་པ་ཉིད་ཀྱི་ཆོས་ཅན་ཤེས་བྱའི་དོན་ལ་ཤེས་བྱེད་རིགས་པའི་གཏན་ཚིགས་རྣམས་ཀྱི་ཇི་ལྟ་བར་གཏན་ལ་ཕབ་སྟེ་རྟོགས་པའོ།། ལམ་སྒོམ་པས་ཉམས་སུ་ལེན་ཚུལ་ནི། ཆོས་ཉིད་སྙིང་པོ་ཇི་བཞིན་པའི་ངང་ལ་མཉམ་པར་འཇོག་པ་མཚན་མེད་རྣམ་པར་མི་རྟོག་པའི་ཏིང་ངེ་འཛིན་དང་། བསྐྱེད་སྔགས་བརྗོད་པ་ཙམ་གྱིས་རྟེན་དང་བརྟེན་པའི་འཁོར་ལོར་བསྒོམ་པ་མཚན་བཅས་ལྷའི་ཏིང་འཛིན་བསྒོམ་པ་གྲོལ་ལམ་དང་། ཐབས་ལམ་སྟེང་འོག་གི་སྒོ་ལ་བརྟེན་ནས་བདེ་སྟོང་གི་ཡེ་ཤེས་བསྐྱེད་པ་སྟེ་ལམ་གཉིས་ཀྱི་རྣལ་འབྱོར་བསྒོམ་པའོ།།',\n",
       " 'Target': 'Once again, let us say a little about its point of entry, view, meditation, conduct and results: i. Entry Point One’s mind is matured through the thirty-six empowerments in which the four rivers—outer, inner, accomplishing and secret—are complete, and one keeps the samayas as described in the texts. ii. View Through logical reasoning one determines that which is to be known, the fact that all phenomena are characterized as being the three mandalas in their fundamental nature, and realizes that this is so. iii. Meditation Meditation practice here consists of two paths. On the path of liberation one practises the non-conceptual samādhi of simply resting in a state that accords with the essence of reality itself, and the conceptual samādhi of deity practice, in which one visualizes the mandala of supporting palace and supported deities simply by reciting the mantra of generation. On the path of skilful means one generates the wisdom of bliss and emptiness through the practices of the upper and lower gateways.',\n",
       " 'File_Name': None,\n",
       " 'Machine Aligned': True,\n",
       " '__index_level_0__': 294714,\n",
       " 'Tag': {'Buddhist': True,\n",
       "  'LH labels': ['Longchen Nyingtik', 'Yumka Dechen Gyalmo'],\n",
       "  'Topic': 'Meditation'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': 'འཇམ་དབལ་གྱིས་རྗེ་བླ་མར། ཟླ་བ་གྲགས་བ་ནི་སངས་རྒྱས་ཀྱི་ཞིང་ཁམས་ཤིག་ནས་ས་མཐོན་པོའི་བྱང་ཆུབ་སེམས་དཔའ་རྣམ་དཔྱོད་སྙིང་སྟོབས་ཅན་ཞིག་མགོན་བོ་ཀླུ་སྒྲུབ་ཀྱི་ཟབ་མོ་ལྟ་བའི་བསྟན་བ་སྤེལ་ཕྱིར་བསམ་བཞིན་བྱོན་པ་ཡིན་བས་དེའི་གཞུང་ལ་ནོར་འཁྲུལ་མེད་ཚུལ་གསུངས་བ་བཞིན།',\n",
       " 'Target': 'Mañjushrı told him that Chandrakırti was a courageous and discerning bodhisattva of high degree who had come here from a buddhafield with the intention of spreading Protector Nagarjuna’s teachings on the profound view, and that his writings were error free.',\n",
       " 'File_Name': 'TM0718',\n",
       " 'Machine Aligned': False,\n",
       " '__index_level_0__': 1102159,\n",
       " 'Tag': {'Buddhist': True,\n",
       "  'LH labels': ['Nyingma Mönlam'],\n",
       "  'Topic': 'Mahamudra'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer, Model, and Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90dadfe696be41cdbe3d9a4fd13a21c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b720f4bafa4b29bd5a5dfb8c3590c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337534bd81c84282a144e07f57b52734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e0cc67836a42148db8df1224c342df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2ffdc6ff214d0a9aca2bf2682402b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec40dd80191944469c89243a86e16981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a2b33f0aed47dd96b41daac16572e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint = \"google/switch-base-8\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, device_map=\"cuda:0\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32355, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a list of all Tibetan Unicode characters (U+0F00 to U+0FFF)\n",
    "tibetan_chars = [chr(codepoint) for codepoint in range(0x0F00, 0x0FFF)]\n",
    "\n",
    "# Add the Tibetan characters to the tokenizer's vocabulary\n",
    "new_tokens = [char for char in tibetan_chars if char not in tokenizer.get_vocab()]\n",
    "\n",
    "# Add new tokens to the tokenizer\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Resize model embeddings to accommodate the new vocabulary size\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (659 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'སྒོམ།སྤྱོད།འབྲས་ཀྱི་རྣམ་གཞག་ཅུང་ཟད་བརྗོད་ན།འཇུག་སྒོ་ནི།ཕྱི་ནང་སྒྲུབ་གསང་ཆུ་བོ་བཞི་རྫོགས་ཀྱི་དབང་སུམ་ཅུ་རྩ་དྲུག་གིས་རྒྱུད་སྨིན་ཅིང་།དམ་ཚིག་གཞུང་བཞིན་སྲུང་བའོ།།གཞི་ལྟ་བ་གཏན་ལ་འབེབས་ཚུལ་ནི།ཆོས་ཐམས་ཅད་ཀྱི་གཤིས་ལུགས་དཀྱིལ་འཁོར་གསུམ་དུ་གནས་པ་ཉིད་ཀྱི་ཆོས་ཅན་ཤེས་བྱའི་དོན་ལ་ཤེས་བྱེད་རིགས་པའི་གཏན་ཚིགས་རྣམས་ཀྱི་ཇི་ལྟ་བར་གཏན་ལ་ཕབ་སྟེ་རྟོགས་པའོ།།ལམ་སྒོམ་པས་ཉམས་སུ་ལེན་ཚུལ་ནི།ཆོས་ཉིད་སྙིང་པོ་ཇི་བཞིན་པའི་ངང་ལ་མཉམ་པར་འཇོག་པ་མཚན་མེད་རྣམ་པར་མི་རྟོག་པའི་ཏིང་ངེ་འཛིན་དང་།བསྐྱེད་སྔགས་བརྗོད་པ་ཙམ་གྱིས་རྟེན་དང་བརྟེན་པའི་འཁོར་ལོར་བསྒོམ་པ་མཚན་བཅས་ལྷའི་ཏིང་འཛིན་བསྒོམ་པ་གྲོལ་ལམ་དང་།ཐབས་ལམ་སྟེང་འོག་གི་སྒོ་ལ་བརྟེན་ནས་བདེ་སྟོང་གི་ཡེ་ཤེས་བསྐྱེད་པ་སྟེ་ལམ་གཉིས་ཀྱི་རྣལ་འབྱོར་བསྒོམ་པའོ།།</s>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = tokenizer.encode(dataset['train'][0]['Source'])\n",
    "dec = tokenizer.decode(enc)\n",
    "dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "The dataset can now be tokenized for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = 'Source'\n",
    "target_lang = 'Target'\n",
    "\n",
    "def preprocess_function(examples):\n",
    "\n",
    "    inputs = [example for example in examples[source_lang]]\n",
    "    targets = [example for example in examples[target_lang]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fe8ce338fc4889ba2e1d1af7e69e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc54ad69a164604a4232da4cde38850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load BLEU and CHRF metrics\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Postprocess text\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu_score = bleu_result[\"score\"]\n",
    "\n",
    "    # Compute CHRF score\n",
    "    chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    chrf_score = chrf_result[\"score\"]\n",
    "\n",
    "    # Compute generation length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    avg_gen_len = np.mean(prediction_lens)\n",
    "\n",
    "    # Return rounded results\n",
    "    return {\n",
    "        \"bleu\": round(bleu_score, 4),\n",
    "        \"chrf\": round(chrf_score, 4),\n",
    "        \"gen_len\": round(avg_gen_len, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, Adafactor\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(), \n",
    "    scale_parameter=True, \n",
    "    relative_step=False, \n",
    "    warmup_init=False, \n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "model, optimizer = accelerator.prepare(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbillingsmoore\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT='moe-translation-experiments'\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_PROJECT='moe-translation-experiments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/j/Documents/MLotsawa/Notebooks/Models/MixtureOfExperts/RetrievalAugmentedTranslation/wandb/run-20250205_172011-d3h1yk7f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/billingsmoore/%27moe-translation-experiments%27/runs/d3h1yk7f' target=\"_blank\">rat-no-context</a></strong> to <a href='https://wandb.ai/billingsmoore/%27moe-translation-experiments%27' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/billingsmoore/%27moe-translation-experiments%27' target=\"_blank\">https://wandb.ai/billingsmoore/%27moe-translation-experiments%27</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/billingsmoore/%27moe-translation-experiments%27/runs/d3h1yk7f' target=\"_blank\">https://wandb.ai/billingsmoore/%27moe-translation-experiments%27/runs/d3h1yk7f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299ee7fd64f74bc39923afee11f19156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16875 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a14f3c29ca43a1858f123d4f18245c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9d86f7281740bcb99d24f381a54a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.524, 'grad_norm': 30.51161003112793, 'learning_rate': 0.0002977777777777777, 'epoch': 0.02}\n",
      "{'loss': 1.2751, 'grad_norm': 3.5249032974243164, 'learning_rate': 0.0002955555555555555, 'epoch': 0.04}\n",
      "{'loss': 1.158, 'grad_norm': 1.2728544473648071, 'learning_rate': 0.00029333333333333327, 'epoch': 0.07}\n",
      "{'loss': 1.107, 'grad_norm': 0.9643834829330444, 'learning_rate': 0.0002911111111111111, 'epoch': 0.09}\n",
      "{'loss': 1.0446, 'grad_norm': 0.9874529242515564, 'learning_rate': 0.0002888888888888888, 'epoch': 0.11}\n",
      "{'loss': 1.0157, 'grad_norm': 2.494997024536133, 'learning_rate': 0.0002866666666666667, 'epoch': 0.13}\n",
      "{'loss': 0.9763, 'grad_norm': 3.061110496520996, 'learning_rate': 0.0002844444444444444, 'epoch': 0.16}\n",
      "{'loss': 1.0786, 'grad_norm': 1.2082693576812744, 'learning_rate': 0.00028222222222222223, 'epoch': 0.18}\n",
      "{'loss': 0.984, 'grad_norm': 3.3752059936523438, 'learning_rate': 0.00028, 'epoch': 0.2}\n",
      "{'loss': 0.9789, 'grad_norm': 0.8587245941162109, 'learning_rate': 0.0002777777777777778, 'epoch': 0.22}\n",
      "{'loss': 1.1373, 'grad_norm': 2.765749454498291, 'learning_rate': 0.0002755555555555555, 'epoch': 0.24}\n",
      "{'loss': 0.9778, 'grad_norm': 1.688852071762085, 'learning_rate': 0.00027333333333333333, 'epoch': 0.27}\n",
      "{'loss': 1.045, 'grad_norm': 0.7730001211166382, 'learning_rate': 0.0002711111111111111, 'epoch': 0.29}\n",
      "{'loss': 0.9637, 'grad_norm': 3.961329698562622, 'learning_rate': 0.0002688888888888889, 'epoch': 0.31}\n",
      "{'loss': 0.9717, 'grad_norm': 0.9961081147193909, 'learning_rate': 0.0002666666666666666, 'epoch': 0.33}\n",
      "{'loss': 1.0364, 'grad_norm': 2.359276294708252, 'learning_rate': 0.00026444444444444443, 'epoch': 0.36}\n",
      "{'loss': 1.0138, 'grad_norm': 4.542107582092285, 'learning_rate': 0.00026222222222222223, 'epoch': 0.38}\n",
      "{'loss': 0.9375, 'grad_norm': 1.779083013534546, 'learning_rate': 0.00026, 'epoch': 0.4}\n",
      "{'loss': 1.007, 'grad_norm': 1.1213067770004272, 'learning_rate': 0.0002577777777777778, 'epoch': 0.42}\n",
      "{'loss': 0.9791, 'grad_norm': 1.245171308517456, 'learning_rate': 0.00025555555555555553, 'epoch': 0.44}\n",
      "{'loss': 1.0071, 'grad_norm': 1.5343340635299683, 'learning_rate': 0.00025333333333333333, 'epoch': 0.47}\n",
      "{'loss': 0.935, 'grad_norm': 1.0057899951934814, 'learning_rate': 0.0002511111111111111, 'epoch': 0.49}\n",
      "{'loss': 0.8934, 'grad_norm': 1.4889651536941528, 'learning_rate': 0.0002488888888888889, 'epoch': 0.51}\n",
      "{'loss': 0.923, 'grad_norm': 2.727799654006958, 'learning_rate': 0.0002466666666666666, 'epoch': 0.53}\n",
      "{'loss': 0.9672, 'grad_norm': 1.5814086198806763, 'learning_rate': 0.00024444444444444443, 'epoch': 0.56}\n",
      "{'loss': 0.9964, 'grad_norm': 1.9900953769683838, 'learning_rate': 0.0002422222222222222, 'epoch': 0.58}\n",
      "{'loss': 0.9743, 'grad_norm': 1.5989022254943848, 'learning_rate': 0.00023999999999999998, 'epoch': 0.6}\n",
      "{'loss': 0.939, 'grad_norm': 1.135394811630249, 'learning_rate': 0.00023777777777777775, 'epoch': 0.62}\n",
      "{'loss': 0.9381, 'grad_norm': 3.37518572807312, 'learning_rate': 0.00023555555555555553, 'epoch': 0.64}\n",
      "{'loss': 0.9811, 'grad_norm': 1.3799805641174316, 'learning_rate': 0.0002333333333333333, 'epoch': 0.67}\n",
      "{'loss': 0.913, 'grad_norm': 2.7728734016418457, 'learning_rate': 0.00023111111111111108, 'epoch': 0.69}\n",
      "{'loss': 0.9302, 'grad_norm': 1.483661413192749, 'learning_rate': 0.00022888888888888885, 'epoch': 0.71}\n",
      "{'loss': 0.9207, 'grad_norm': 1.4484739303588867, 'learning_rate': 0.00022666666666666663, 'epoch': 0.73}\n",
      "{'loss': 0.956, 'grad_norm': 2.83856201171875, 'learning_rate': 0.0002244444444444444, 'epoch': 0.76}\n",
      "{'loss': 0.8722, 'grad_norm': 1.3994487524032593, 'learning_rate': 0.00022222222222222218, 'epoch': 0.78}\n",
      "{'loss': 0.9055, 'grad_norm': 1.477027416229248, 'learning_rate': 0.00021999999999999995, 'epoch': 0.8}\n",
      "{'loss': 0.9562, 'grad_norm': 1.5493665933609009, 'learning_rate': 0.00021777777777777778, 'epoch': 0.82}\n",
      "{'loss': 0.9583, 'grad_norm': 1.587107539176941, 'learning_rate': 0.00021555555555555556, 'epoch': 0.84}\n",
      "{'loss': 0.8767, 'grad_norm': 0.9176985621452332, 'learning_rate': 0.00021333333333333333, 'epoch': 0.87}\n",
      "{'loss': 0.9724, 'grad_norm': 1.5293132066726685, 'learning_rate': 0.0002111111111111111, 'epoch': 0.89}\n",
      "{'loss': 0.9406, 'grad_norm': 1.6758965253829956, 'learning_rate': 0.00020888888888888888, 'epoch': 0.91}\n",
      "{'loss': 0.8847, 'grad_norm': 0.5235921740531921, 'learning_rate': 0.00020666666666666666, 'epoch': 0.93}\n",
      "{'loss': 0.9093, 'grad_norm': 3.2301971912384033, 'learning_rate': 0.00020444444444444443, 'epoch': 0.96}\n",
      "{'loss': 0.873, 'grad_norm': 0.9974763989448547, 'learning_rate': 0.0002022222222222222, 'epoch': 0.98}\n",
      "{'loss': 0.9249, 'grad_norm': 1.6123685836791992, 'learning_rate': 0.00019999999999999998, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3defe4bfd0c84fd4a483ee5b75baf67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8416443467140198, 'eval_bleu': 0.0219, 'eval_chrf': 5.5659, 'eval_gen_len': 18.4374, 'eval_runtime': 342.4293, 'eval_samples_per_second': 14.602, 'eval_steps_per_second': 1.825, 'epoch': 1.0}\n",
      "{'loss': 0.9025, 'grad_norm': 1.220794677734375, 'learning_rate': 0.00019777777777777776, 'epoch': 1.02}\n",
      "{'loss': 0.8844, 'grad_norm': 2.7910773754119873, 'learning_rate': 0.00019555555555555556, 'epoch': 1.04}\n",
      "{'loss': 0.9384, 'grad_norm': 1.1053590774536133, 'learning_rate': 0.00019333333333333333, 'epoch': 1.07}\n",
      "{'loss': 0.8465, 'grad_norm': 1.0290740728378296, 'learning_rate': 0.0001911111111111111, 'epoch': 1.09}\n",
      "{'loss': 0.8153, 'grad_norm': 0.5474309325218201, 'learning_rate': 0.00018888888888888888, 'epoch': 1.11}\n",
      "{'loss': 0.8554, 'grad_norm': 2.537532329559326, 'learning_rate': 0.00018666666666666666, 'epoch': 1.13}\n",
      "{'loss': 0.8617, 'grad_norm': 2.9197356700897217, 'learning_rate': 0.00018444444444444443, 'epoch': 1.16}\n",
      "{'loss': 0.9155, 'grad_norm': 1.172302484512329, 'learning_rate': 0.0001822222222222222, 'epoch': 1.18}\n",
      "{'loss': 0.8307, 'grad_norm': 1.5790420770645142, 'learning_rate': 0.00017999999999999998, 'epoch': 1.2}\n",
      "{'loss': 0.8742, 'grad_norm': 2.7658803462982178, 'learning_rate': 0.00017777777777777776, 'epoch': 1.22}\n",
      "{'loss': 0.8595, 'grad_norm': 1.1260719299316406, 'learning_rate': 0.00017555555555555553, 'epoch': 1.24}\n",
      "{'loss': 0.8872, 'grad_norm': 3.533447742462158, 'learning_rate': 0.0001733333333333333, 'epoch': 1.27}\n",
      "{'loss': 0.8852, 'grad_norm': 1.2004669904708862, 'learning_rate': 0.0001711111111111111, 'epoch': 1.29}\n",
      "{'loss': 0.8651, 'grad_norm': 0.6438434720039368, 'learning_rate': 0.00016888888888888889, 'epoch': 1.31}\n",
      "{'loss': 0.8855, 'grad_norm': 0.8911612629890442, 'learning_rate': 0.00016666666666666666, 'epoch': 1.33}\n",
      "{'loss': 0.8619, 'grad_norm': 0.9329342842102051, 'learning_rate': 0.00016444444444444444, 'epoch': 1.36}\n",
      "{'loss': 0.8785, 'grad_norm': 1.4167791604995728, 'learning_rate': 0.0001622222222222222, 'epoch': 1.38}\n",
      "{'loss': 0.8053, 'grad_norm': 1.0776822566986084, 'learning_rate': 0.00015999999999999999, 'epoch': 1.4}\n",
      "{'loss': 0.8822, 'grad_norm': 3.6012110710144043, 'learning_rate': 0.00015777777777777776, 'epoch': 1.42}\n",
      "{'loss': 0.9105, 'grad_norm': 0.6970692276954651, 'learning_rate': 0.00015555555555555554, 'epoch': 1.44}\n",
      "{'loss': 0.834, 'grad_norm': 1.6041443347930908, 'learning_rate': 0.0001533333333333333, 'epoch': 1.47}\n",
      "{'loss': 0.8773, 'grad_norm': 1.4817160367965698, 'learning_rate': 0.00015111111111111109, 'epoch': 1.49}\n",
      "{'loss': 0.9077, 'grad_norm': 1.672896146774292, 'learning_rate': 0.00014888888888888886, 'epoch': 1.51}\n",
      "{'loss': 0.8436, 'grad_norm': 1.2761646509170532, 'learning_rate': 0.00014666666666666664, 'epoch': 1.53}\n",
      "{'loss': 0.9257, 'grad_norm': 0.7780904769897461, 'learning_rate': 0.0001444444444444444, 'epoch': 1.56}\n",
      "{'loss': 0.9036, 'grad_norm': 1.0162137746810913, 'learning_rate': 0.0001422222222222222, 'epoch': 1.58}\n",
      "{'loss': 0.8734, 'grad_norm': 1.1291720867156982, 'learning_rate': 0.00014, 'epoch': 1.6}\n",
      "{'loss': 0.8942, 'grad_norm': 0.7396151423454285, 'learning_rate': 0.00013777777777777776, 'epoch': 1.62}\n",
      "{'loss': 0.8004, 'grad_norm': 4.5133490562438965, 'learning_rate': 0.00013555555555555554, 'epoch': 1.64}\n",
      "{'loss': 0.841, 'grad_norm': 0.7123967409133911, 'learning_rate': 0.0001333333333333333, 'epoch': 1.67}\n",
      "{'loss': 0.877, 'grad_norm': 0.9095564484596252, 'learning_rate': 0.00013111111111111111, 'epoch': 1.69}\n",
      "{'loss': 0.8621, 'grad_norm': 1.8071300983428955, 'learning_rate': 0.0001288888888888889, 'epoch': 1.71}\n",
      "{'loss': 0.8483, 'grad_norm': 0.7809709310531616, 'learning_rate': 0.00012666666666666666, 'epoch': 1.73}\n",
      "{'loss': 0.8766, 'grad_norm': 1.0310146808624268, 'learning_rate': 0.00012444444444444444, 'epoch': 1.76}\n",
      "{'loss': 0.8069, 'grad_norm': 2.143836498260498, 'learning_rate': 0.00012222222222222221, 'epoch': 1.78}\n",
      "{'loss': 0.8419, 'grad_norm': 3.2706472873687744, 'learning_rate': 0.00011999999999999999, 'epoch': 1.8}\n",
      "{'loss': 0.9015, 'grad_norm': 3.162073850631714, 'learning_rate': 0.00011777777777777776, 'epoch': 1.82}\n",
      "{'loss': 0.8834, 'grad_norm': 3.2016472816467285, 'learning_rate': 0.00011555555555555554, 'epoch': 1.84}\n",
      "{'loss': 0.8627, 'grad_norm': 1.8244891166687012, 'learning_rate': 0.00011333333333333331, 'epoch': 1.87}\n",
      "{'loss': 0.8713, 'grad_norm': 1.2015984058380127, 'learning_rate': 0.00011111111111111109, 'epoch': 1.89}\n",
      "{'loss': 0.8682, 'grad_norm': 0.9424881339073181, 'learning_rate': 0.00010888888888888889, 'epoch': 1.91}\n",
      "{'loss': 0.8957, 'grad_norm': 2.319692373275757, 'learning_rate': 0.00010666666666666667, 'epoch': 1.93}\n",
      "{'loss': 0.8706, 'grad_norm': 0.8238505125045776, 'learning_rate': 0.00010444444444444444, 'epoch': 1.96}\n",
      "{'loss': 0.8566, 'grad_norm': 0.5217728614807129, 'learning_rate': 0.00010222222222222222, 'epoch': 1.98}\n",
      "{'loss': 0.839, 'grad_norm': 1.7032339572906494, 'learning_rate': 9.999999999999999e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc666a85aa842cd93fad6718a6c06a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8123542070388794, 'eval_bleu': 0.0369, 'eval_chrf': 5.8324, 'eval_gen_len': 18.587, 'eval_runtime': 343.0927, 'eval_samples_per_second': 14.573, 'eval_steps_per_second': 1.822, 'epoch': 2.0}\n",
      "{'loss': 0.7794, 'grad_norm': 4.373380184173584, 'learning_rate': 9.777777777777778e-05, 'epoch': 2.02}\n",
      "{'loss': 0.7727, 'grad_norm': 1.3309237957000732, 'learning_rate': 9.555555555555555e-05, 'epoch': 2.04}\n",
      "{'loss': 0.872, 'grad_norm': 1.7452807426452637, 'learning_rate': 9.333333333333333e-05, 'epoch': 2.07}\n",
      "{'loss': 0.819, 'grad_norm': 0.9290546178817749, 'learning_rate': 9.11111111111111e-05, 'epoch': 2.09}\n",
      "{'loss': 0.8792, 'grad_norm': 3.984588623046875, 'learning_rate': 8.888888888888888e-05, 'epoch': 2.11}\n",
      "{'loss': 0.8982, 'grad_norm': 2.0007970333099365, 'learning_rate': 8.666666666666665e-05, 'epoch': 2.13}\n",
      "{'loss': 0.838, 'grad_norm': 1.7778632640838623, 'learning_rate': 8.444444444444444e-05, 'epoch': 2.16}\n",
      "{'loss': 0.7963, 'grad_norm': 2.819756507873535, 'learning_rate': 8.222222222222222e-05, 'epoch': 2.18}\n",
      "{'loss': 0.8152, 'grad_norm': 4.511503219604492, 'learning_rate': 7.999999999999999e-05, 'epoch': 2.2}\n",
      "{'loss': 0.806, 'grad_norm': 1.6935982704162598, 'learning_rate': 7.777777777777777e-05, 'epoch': 2.22}\n",
      "{'loss': 0.7799, 'grad_norm': 0.9467908143997192, 'learning_rate': 7.555555555555554e-05, 'epoch': 2.24}\n",
      "{'loss': 0.7956, 'grad_norm': 2.5627307891845703, 'learning_rate': 7.333333333333332e-05, 'epoch': 2.27}\n",
      "{'loss': 0.8594, 'grad_norm': 1.3098576068878174, 'learning_rate': 7.11111111111111e-05, 'epoch': 2.29}\n",
      "{'loss': 0.8708, 'grad_norm': 1.289193868637085, 'learning_rate': 6.888888888888888e-05, 'epoch': 2.31}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m Seq2SeqTrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrat-no-context\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     auto_find_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     15\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     22\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/accelerate/utils/memory.py:153\u001b[0m, in \u001b[0;36mfind_executable_batch_size.<locals>.decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo executable batch size found, reached zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2285\u001b[0m ):\n\u001b[1;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/transformers/trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/accelerate/accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MLotsawa/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"rat-no-context\",\n",
    "    auto_find_batch_size=True,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, #check this\n",
    "    push_to_hub=False,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    num_train_epochs=3\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
